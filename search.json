[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Code for ‘A Practical Guide . . .’",
    "section": "",
    "text": "Preface\nCode provided here is for the May 2024 text:\n\n“A Practical Guide to Data Analysis Using R – An Example-Based Approach”, by John H Maindonald, W John Braun, and Jeffrey L Andrews.\n\nCode that is shown in the text is filled out to include all code for graphs. In chapter 2 and later, the text includes code only for those graphs that are specifically targeted at the methodology under discussion.\nThe text has been available in digital format since early May 2024. It has been available in print in the UK from May 30, is due to be available in print in the USA in July, and in Australia and New Zealand in August 2024.\nFor details for the UK, see: A Practical Guide to Data Analysis Using R – Cambridge University Press UK",
    "crumbs": [
      "Book",
      "Preface"
    ]
  },
  {
    "objectID": "ch1.html",
    "href": "ch1.html",
    "title": "1  Chapter 1: Learning from data",
    "section": "",
    "text": "library(knitr)\nopts_chunk[['set']](fig.width=6, fig.height=6, comment=\" \",\n                    out.width=\"80%\", fig.align=\"center\", fig.show='hold',\n                    size=\"small\", ps=10, strip.white = TRUE, \n                    tidy.opts = list(replace.assign=FALSE))\n\n\n## xtras=TRUE    ## Set to TRUE to execute code 'extras'\nxtras &lt;- FALSE\nlibrary(knitr)\n## opts_chunk[['set']](results=\"asis\")\n## opts_chunk[['set']](eval=FALSE)   ## Set to TRUE to execute main part of code\nopts_chunk[['set']](eval=FALSE) \n\n\nPackages required (plus any dependencies)\nlatticeExtra (lattice is a dependency); DAAG; car; MASS; AICcmodavg; BayesFactor; boot; MPV; ggplot2; tidyr\nAdditionally, knitr and Hmisc are required in order to process the Rmd source file. The prettydoc package is by default used to format the html output.\n\n\nChapter summary\n\n\nA note on terminology — variables, factors, and more!\n\n\nSection 1.1: Questions, and data that may point to answers\n\nSubsection 1.1.1: A sample is a window into the wider population\n\n## For the sequence below, precede with set.seed(3676)\nset.seed(3696)\nsample(1:9384, 12, replace=FALSE)  # NB: `replace=FALSE` is the default\n\n\nchosen1200 &lt;- sample(1:19384, 1200, replace=FALSE)  \n\n\n## For the sequence below, precede with set.seed(366)\nset.seed(366)\nsplit(sample(seq(1:10)), rep(c(\"Control\",\"Treatment\"), 5))\n# sample(1:10) gives a random re-arrangement (permutation) of 1, 2, ..., 10\n\n\nCluster sampling\n\n\n*A note on with-replacement samples\n\nsample(1:10, replace=TRUE)\n## sample(1:10, replace=FALSE) returns a random permutation of 1,2,...10\n\n\n\n\nSubsection 1.1.2: Formulating the scientific question\n\nExample: a question about cuckoo eggs\n\nlibrary(latticeExtra)   # Lattice package will be loaded and attached also\ncuckoos &lt;- DAAG::cuckoos\n## Panel A: Dotplot without species means added\ndotplot(species ~ length, data=cuckoos)   ## `species ~ length` is a 'formula'\n## Panel B: Box and whisker plot\nbwplot(species ~ length, data=cuckoos)\n## The following shows Panel A, including species means & other tweaks\nav &lt;- with(cuckoos, aggregate(length, list(species=species), FUN=mean))\ndotplot(species ~ length, data=cuckoos, alpha=0.4, xlab=\"Length of egg (mm)\") +\n  as.layer(dotplot(species ~ x, pch=3, cex=1.4, col=\"black\", data=av))\n  # Use `+` to indicate that more (another 'layer') is to be added.\n  # With `alpha=0.4`, 40% is the point color with 60% background color\n  # `pch=3`: Plot character 3 is '+'; `cex=1.4`: Default char size X 1.4\n\n\n## Code\nsuppressPackageStartupMessages(library(latticeExtra, quietly=TRUE))\ncuckoos &lt;- DAAG::cuckoos\n## For tidier labels replace \".\", in several of the names, by a space\nspecnam &lt;- with(cuckoos, sub(pattern=\".\", replacement=\" \", \n                             levels(species), fixed=TRUE))\n# fixed=TRUE: \"interpret \".\" as \".\", not as a 'any single character'\"\ncuckoos &lt;- within(cuckoos, levels(species) &lt;- specnam)\n## Panel A: Dotplot: data frame cuckoos (DAAG)\nav &lt;- with(cuckoos, aggregate(length, list(species=species), FUN=mean))\ngphA &lt;- dotplot(species ~ length, data=cuckoos, alpha=0.4) +\n  as.layer(dotplot(species ~ x, pch=3, cex=1.4, col=\"black\", data=av))\n# alpha sets opacity. With alpha=0.4, 60% of the background shows through\n# Enter `print(plt1)` or `plot(plt1)` or simply `plt1` to display the graph\n## Panel B: Box plot\ngphB &lt;- bwplot(species ~ length, data=cuckoos)\nupdate(c(\"A: Dotplot\"=gphA, \"B: Boxplot\"=gphB), between=list(x=0.4),\n       xlab=\"Length of egg (mm)\") \n## latticeExtra::c() joins compatible plots together. \n##   See `?latticeExtra::c`\n\n\n\n\nSubsection 1.1.3: Planning for a statistical analysis\n\nUnderstand the data\n\n\nCausal inference\n\n\nWhat was measured? Is it the relevant measure?\n\n\nUse relevant prior information in the planning stages\n\n\nSubject area knowledge and judgments\n\n\nThe importance of clear communication\n\n\nData-based selection of comparisons\n\n\nModels must be fit for their intended use\n\n\n\nSubsection 1.1.4: Results that withstand thorough and informed challenge\n\n\nSubsection 1.1.5: Using graphs to make sense of data\n\nGraphical comparisons\n\n\n\nSubsection 1.1.6: Formal model-based comparison\n\noptions(width=70)\ncuckoos &lt;- DAAG::cuckoos\nav &lt;- with(cuckoos, aggregate(length, list(species=species), FUN=mean))\nsetNames(round(av[[\"x\"]],2), abbreviate(av[[\"species\"]],10))\n\n\nwith(cuckoos, scale(length[species==\"wren\"], scale=FALSE))[,1] \n\n\n\n\nSection 1.2: Graphical tools for data exploration\n\nSubsection 1.2.1: Displays of a single variable\n\nlibrary(latticeExtra, quietly=TRUE)\nfossum &lt;- subset(DAAG::possum, sex==\"f\")\nfemlen &lt;- DAAG::bounce(fossum[[\"totlngth\"]], d=0.1)\n## Panel A\nyaxpos &lt;- c(0,5,10,15,20)/(5*nrow(fossum))\nz &lt;- boxplot(list(val = femlen), plot = FALSE)\ngph1 &lt;- bwplot(~femlen, ylim=c(0.55,2.75), xlim=c(70,100), \n               scales=list(y=list(draw=FALSE)))+\n        latticeExtra::layer(panel.rug(x,pch=\"|\"))\nlegstat &lt;- data.frame(x=c(z$out,z$stats), y=c(1.08, rep(1.3,5)),\n  tx=c(\"Outlier?\", \"Smallest value\", \"lower quartile\", \"median\", \n       \"upper quartile\",  \"Largest value\"), \n  tx2= c(\"\", \"(outliers excepted)\",rep(\"\",3), \"(no outliers)\"))\ngphA &lt;- gph1+latticeExtra::layer(data=legstat,\n  panel.text(x=x,y=y,labels=tx,adj=c(0,0.4),srt=90, cex=0.85),\n  panel.text(x=x[c(2,6)]+0.75,y=c(1.125,1.38),labels=tx2[c(2,6)],\n             adj=c(0,0.4),srt=90, cex=0.85))\n## Panel B\ngph2 &lt;- densityplot(~femlen, ylim=c(0,0.108), xlim=c(70,100), \n          plot.points=TRUE, pch=\"|\",cex=1.75, ylab=c(\"\",\"      Density\"))\ngph3 &lt;- histogram(~femlen, ylim=c(0,0.108), type=\"density\", \n  scales=list(y=list(at=yaxpos, labels=c(0,5,10,15,20), col=\"gray40\")), \n  alpha=0.5, ylab=\"\", breaks=c(75,80,85,90,95,100), \n  col='transparent',border='gray40')\ngph4 &lt;- doubleYScale(gph2, gph3, use.style=FALSE, add.ylab2=FALSE)\ngphB &lt;- update(gph4, par.settings=list(fontsize = list(text=10, points=5)),\n  scales=list(tck=c(0.5,0.5)))\nupdate(c(\"B: Density curve, with histogram overlaid\"=gphB, \n         \"A: Boxplot, with annotation added\"=gphA, layout=c(1,2), y.same=F), \n       as.table=TRUE, between=list(y=1.4), \n       xlab=\"Total length of female possums (cm)\")\n\n\nfossum &lt;- subset(DAAG::possum, sex==\"f\")\ndensityplot(~totlngth, plot.points=TRUE, pch=\"|\", data=fossum) +\n  layer_(panel.histogram(x, type=\"density\", breaks=c(75,80,85,90,95,100)))\n\n\nComparing univariate displays across factor levels\n\nlibrary(latticeExtra, quietly=TRUE)\nfossum &lt;- subset(DAAG::possum, sex==\"f\")\nfemlen &lt;- DAAG::bounce(fossum[[\"totlngth\"]], d=0.1)\n## Panel A\nyaxpos &lt;- c(0,5,10,15,20)/(5*nrow(fossum))\nz &lt;- boxplot(list(val = femlen), plot = FALSE)\ngph1 &lt;- bwplot(~femlen, ylim=c(0.55,2.75), xlim=c(70,100), \n               scales=list(y=list(draw=FALSE)))+\n        latticeExtra::layer(panel.rug(x,pch=\"|\"))\nlegstat &lt;- data.frame(x=c(z$out,z$stats), y=c(1.08, rep(1.3,5)),\n  tx=c(\"Outlier?\", \"Smallest value\", \"lower quartile\", \"median\", \n       \"upper quartile\",  \"Largest value\"), \n  tx2= c(\"\", \"(outliers excepted)\",rep(\"\",3), \"(no outliers)\"))\ngphA &lt;- gph1+latticeExtra::layer(data=legstat,\n  panel.text(x=x,y=y,labels=tx,adj=c(0,0.4),srt=90, cex=0.85),\n  panel.text(x=x[c(2,6)]+0.75,y=c(1.125,1.38),labels=tx2[c(2,6)],\n             adj=c(0,0.4),srt=90, cex=0.85))\n## Panel B\ngph2 &lt;- densityplot(~femlen, ylim=c(0,0.108), xlim=c(70,100), \n          plot.points=TRUE, pch=\"|\",cex=1.75, ylab=c(\"\",\"      Density\"))\ngph3 &lt;- histogram(~femlen, ylim=c(0,0.108), type=\"density\", \n  scales=list(y=list(at=yaxpos, labels=c(0,5,10,15,20), col=\"gray40\")), \n  alpha=0.5, ylab=\"\", breaks=c(75,80,85,90,95,100), \n  col='transparent',border='gray40')\ngph4 &lt;- doubleYScale(gph2, gph3, use.style=FALSE, add.ylab2=FALSE)\ngphB &lt;- update(gph4, par.settings=list(fontsize = list(text=10, points=5)),\n  scales=list(tck=c(0.5,0.5)))\nupdate(c(\"B: Density curve, with histogram overlaid\"=gphB, \n         \"A: Boxplot, with annotation added\"=gphA, layout=c(1,2), y.same=F), \n       as.table=TRUE, between=list(y=1.4), \n       xlab=\"Total length of female possums (cm)\")\n\n\n## Create boxplot graph object --- Simplified code\ngph &lt;- bwplot(Pop~totlngth | sex, data=possum) \n## plot graph, with dotplot distribution of points below boxplots\ngph + latticeExtra::layer(panel.dotplot(x, unclass(y)-0.4)) \n\n\n\n\nSubsection 1.2.2: Patterns in univariate time series\n\nlayout(matrix(c(1,2)), heights=c(2.6,1.75))\nmeasles &lt;- DAAG::measles\n## Panel A:\npar(mgp=c(2.0,0.5,0))\nplot(log10(measles), xlab=\"\", ylim=log10 (c(1,5000*540)),\n     ylab=\" Deaths\", yaxt=\"n\", fg=\"gray\", adj=0.16)\nlondonpop &lt;-ts(c(1088, 1258, 1504, 1778, 2073, 2491, 2921, 3336, 3881,\n  4266, 4563, 4541, 4498, 4408), start=1801, end=1931, deltat=10)\npoints(log10(londonpop*500), pch=16, cex=.5)\nytiks1 &lt;- c(1, 10, 100, 1000)\naxis(2, at=log10(ytiks1), labels=paste(ytiks1), lwd=0, lwd.ticks=1)\nabline(h=log10(ytiks1), col = \"lightgray\", lwd=2)\npar(mgp=c(-2,-0.5,0))\nytiks2 &lt;- c(1000000, 5000000)  ## London population in thousands\nabline(h=log10(ytiks2*0.5), col = \"lightgray\", lwd=1.5)\nabline(v=seq(from=1650,to=1950,by=50), col = \"lightgray\", lwd = 1.5)\nmtext(side=2, line=0.5, \"Population\", adj=1, cex=1.15, las=3)\naxis(2, at=log10(ytiks2*0.6), labels=paste(ytiks2), tcl=0.3,\n     hadj=0, lwd=0, lwd.ticks=1)\nmtext(side=3, line=0.3, \"A (1629-1939)\", adj=0, cex=1.15)\n##\n## Panel B: window from 1840 to 1882\npar(mgp=c(2.0,0.5,0))\nplot(window(measles, start=1840, end=1882), xlab=\"\",\nylab=\"Deaths    Pop (1000s)\", ylim=c(0, 4200), fg=\"gray\")\npoints(window(londonpop, start=1840, end=1882), pch=16, cex=0.5)\nmtext(side=3, line=0.5, \"B (1841-1881)\", adj=0, cex=1.15)\n\n\n\nSubsection 1.2.3: Visualizing relationships between pairs of variables\n\n\nSubsection 1.2.4: Response lines (and/or curves)\n\npar(pty=\"s\")\nplot(distance.traveled ~ starting.point, data=DAAG::modelcars, fg=\"gray\",\nxlim=c(0,12.5), xaxs=\"i\", xlab = \"Distance up ramp (cm)\",\nylab=\"Distance traveled (cm)\")\n\n\n\nSubsection 1.2.5: Multiple variables and times\n\n## Apply function range to columns of data frame jobs (DAAG)\nsapply(DAAG::jobs, range)  ## NB: `BC` = British Columbia\n\n\n## Panel A: Basic plot; all series in a single panel; use log y-scale\nformRegions &lt;- Ontario+Quebec+BC+Alberta+Prairies+Atlantic ~ Date\nbasicGphA &lt;-\n  xyplot(formRegions, outer=FALSE, data=DAAG::jobs, type=\"l\", xlab=\"\", \n         ylab=\"Number of workers\", scales=list(y=list(log=\"e\")),\n         auto.key=list(space=\"right\", lines=TRUE, points=FALSE))\n  ## `outer=FALSE`: plot all columns in one panel\n## Panel B: Separate panels (`outer=TRUE`); sliced log scale\nbasicGphB &lt;-\n  xyplot(formRegions, data=DAAG::jobs, outer=TRUE, type=\"l\", layout=c(3,2), \n         xlab=\"\", ylab=\"Number of workers\",\n         scales=list(y=list(relation=\"sliced\", log=TRUE)))\n# Provinces are in order of number of workers in Dec96\n## Create improved x- and y-axis tick labels; will update to use\ndatelabpos &lt;- seq(from=95, by=0.5, length=5)\ndatelabs &lt;- format(seq(from=as.Date(\"1Jan1995\", format=\"%d%b%Y\"),\n                   by=\"6 month\", length=5), \"%b%y\")\n## Now create $y$-labels that have numbers, with log values underneath\nylabposA &lt;- exp(pretty(log(unlist(DAAG::jobs[,-7])), 5))\nylabelsA &lt;- paste(round(ylabposA),\"\\n(\", log(ylabposA), \")\", sep=\"\")\n## Repeat, now with 100 ticks, to cover all 6 slices of the scale\nylabposB &lt;- exp(pretty(log(unlist(DAAG::jobs[,-7])), 100))\nylabelsB &lt;- paste(round(ylabposB),\"\\n(\", log(ylabposB), \")\", sep=\"\")\ngphA &lt;- update(basicGphA, scales=list(x=list(at=datelabpos, labels=datelabs),\n              y=list(at=ylabposA, labels=ylabelsA)))\ngphB &lt;- update(basicGphB, xlab=\"\", between=list(x=0.25, y=0.25),\n               scales=list(x=list(at=datelabpos, labels=datelabs),\n               y=list(at=ylabposB, labels=ylabelsB)))\nlayout.list &lt;- list(layout.heights=list(top.padding=0,\n                    bottom.padding=0, sub=0, xlab=0), \n                    fontsize=list(text=8, points=5))\njobstheme &lt;- modifyList(ggplot2like(pch=1, lty=c(4:6,1:3),\n                                    col.line='black', cex=0.75),layout.list)\nprint(update(gphA, par.settings=jobstheme, axis=axis.grid,\n      main=list(\"A: Same vertical log scale\",y=0)),\n      position=c(0.1,0.615,0.9,1), newpage=TRUE)\nprint(update(gphB, par.settings=jobstheme, axis=axis.grid,\n      main=list(\"B: Sliced vertical log scale\",y=0)),\n      position=c(0,0,1,0.625), newpage=FALSE)\n\n\nplot(c(1230,1860), c(0, 10.5), axes=FALSE, bty=\"n\",\n     xlab=\"\", ylab=\"\", type=\"n\", log=\"x\")\nxpoints &lt;- c(1366, 1436, 1752, 1840)\naxis(1, at=xpoints, labels=FALSE, tck=0.01, lty=1, lwd=0, lwd.ticks=1)\nfor(i in 1:4){\n  axis(1, at=xpoints[i],\n       labels=substitute(italic(a), list(a=paste(xpoints[i]))),\n  line=-2.25, lty=0, cex=0.8, lwd=0, lwd.ticks=1)\n  lines(rep(xpoints[i],2), c(0, 0.15*par()[[\"cxy\"]][2]), lty=1)\n}\naxpos &lt;- 1250*cumprod(c(1, rep(1.2,2)))\naxis(1, at=c(axpos,1840), labels=F, lwd.ticks=0)\nlab &lt;- round(axpos)\naxis(1, at=axpos, labels=lab)\nlab2 &lt;- lapply(round(log2(xpoints),3), function(x)substitute(2^a, list(a=x)))\naxis(1, at=xpoints, labels=as.expression(lab2), line=-3.5, lwd=0)\nlabe &lt;- lapply(format(round(log(xpoints),3)), function(x)substitute(e^a, list(a=x)))\naxis(1, at=xpoints, labels=as.expression(labe), line=-5, lwd=0)\nlab10 &lt;- lapply(round(log10(xpoints),3), function(x)substitute(10^a, list(a=x)))\naxis(1, at=xpoints, labels=as.expression(lab10), line=-6.5, lwd=0)\npar(family=\"mono\", xpd=TRUE)\naxis(1, at=1220, labels=\"log=2\", line=-3.5, hadj=0, lwd=0)\naxis(1, at=1220, labels='log=\"e\"', line=-5, hadj=0, lwd=0)\naxis(1, at=1220, labels=\"log=10\", line=-6.5, hadj=0, lwd=0)  \nwid2 &lt;- strwidth(\"log=2\")\npar(family=\"sans\")\n\n\n\nSubsection 1.2.6: *Labeling technicalities\n\n\nSubsection 1.2.7: Graphical displays for categorical data\n\nstones &lt;- array(c(81,6,234,36,192,71,55,25), dim=c(2,2,2),\n                dimnames=list(Success=c(\"yes\",\"no\"),\n                Method=c(\"open\",\"ultrasound\"), Size=c(\"&lt;2cm\", \"&gt;=2cm\")))\nmargin12 &lt;- margin.table(stones, margin=1:2)\n\n\nbyMethod &lt;- 100*prop.table(margin12, margin=2)\npcGood &lt;- 100*prop.table(stones, margin=2:3)[\"yes\", , ]\ndimnam &lt;- dimnames(stones)\nnumOps &lt;- margin.table(stones, margin=2:3)\nopStats &lt;- data.frame(Good=c(pcGood[1,],pcGood[2,]),\n                      numOps=c(numOps[1,], numOps[2,]),\n                      opType=factor(rep(dimnam[[\"Method\"]],c(2,2))),\n                      Size=rep(dimnam[[\"Size\"]],2))\nxlim &lt;- range(opStats$Good)*c(0.65,1.015)\nylim &lt;- c(0, max(opStats$numOps)*1.15)\nplot(numOps~Good, data=opStats, type=\"h\", lwd=4, xlim=xlim, ylim=ylim,\n     fg=\"gray\",col=rep(c(\"blue\",\"red\"),rep(2,2)),\n     xlab=\"Success rate (%)\", ylab=\"Number of operations\")\n# with(opStats, text(numOps~Good, labels=Size,\n#                    col=rep(c('blue','red'),rep(2,2)),\n#                    offset=0.25,pos=3, cex=0.75))\nlabpos &lt;- lapply(split(opStats, opStats$Size), \n  function(x)apply(x[,1:2],2,function(z)c(z[1],mean(z),z[2])))\nsizeNam &lt;- names(labpos)\nlapply(labpos, function(x)lines(x[,'Good'],x[,'numOps']+c(0,35,0),\n  type=\"c\",col=\"gray\"))\ntxtmid &lt;- sapply(labpos, function(x)c(x[2,'Good'],x[2,'numOps']+35))\ntext(txtmid[1,]+c(-1.4,0.85),txtmid[2,],labels=sizeNam,col=\"gray40\",\n     pos=c(4,2), offset=0)\npar(xpd=TRUE)\ntext(byMethod[1,1:2],rep(par()$usr[4],2)+0.5*strheight(\"^\"), labels=c(\"^\",\"^\"),\n     col=c(\"blue\",\"red\"),cex=1.2,srt=180)\ntext(byMethod[1,], par()$usr[4]+1.4*strheight(\"A\"),\n     labels=paste(round(byMethod[1,],1)),cex=0.85)\ntext(byMethod[1,1:2]+c(3.5,-3.5), rep(par()$usr[4],2)+2.65*strheight(\"A\"),\nlabels=c(\"All open\",\"All ultrasound\"), pos=c(2,4))\npar(xpd=FALSE)\nabline(h=100*(0:2),col=\"lightgray\",lwd=0.5)\nabline(v=10*(5:9),col=\"lightgray\",lwd=0.5)\nlegend(\"topleft\", col=c('blue','red'),lty=c(1,1), lwd=1, cex=0.9,\n       y.intersp=0.75, legend=c(\"Open\",\"Ultrasound\"),bty=\"n\",\n       inset=c(-0.01,-0.01))\n\n\n\nSubsection 1.2.8: What to look for in plots\n\nOutliers\n\n\nAsymmetry of the distribution\n\n\nChanges in variability\n\n\nClustering\n\n\nNonlinearity\n\n\nTime trends in the data\n\n\n\n\nSection 1.3: Data Summary\n\nSubsection 1.3.1: Counts\n\n## Table of counts example: data frame nswpsid1 (DAAG)\n## Specify `useNA=\"ifany\"` to ensure that any NAs are tabulated\ntab &lt;- with(DAAG::nswpsid1, table(trt, nodeg, useNA=\"ifany\"))\ndimnames(tab) &lt;- list(trt=c(\"none\", \"training\"), educ = c(\"completed\", \"dropout\"))\ntab\n\n\nTabulation that accounts for frequencies or weights – the xtabs() function\n\ngph &lt;- lattice::bwplot(log(nassCDS$weight+1), xlab=\"Inverse sampling weights\",\n  scales=list(x=list(at=c(0,log(c(10^(0:5)+1))), labels=c(0,10^(0:5)))))\nupdate(gph, par.settings=DAAG::DAAGtheme(color=F, col.points='gray50'))\n\n\nsampNum &lt;- table(nassCDS$dead)\npopNum &lt;- as.vector(xtabs(weight ~ dead, data=nassCDS))\nrbind(Sample=sampNum, \"Total number\"=round(popNum,1))\n\n\nnassCDS &lt;- DAAG::nassCDS\nAtab &lt;- xtabs(weight ~ airbag + dead, data=nassCDS)/1000\n## Define a function that calculates Deaths per 1000\nDeadPer1000 &lt;- function(x)1000*x[2]/sum(x)\nAtabm &lt;- ftable(addmargins(Atab, margin=2, FUN=DeadPer1000))\nprint(Atabm, digits=2, method=\"compact\", big.mark=\",\")\n\n\nSAtab &lt;- xtabs(weight ~ seatbelt + airbag + dead, data=nassCDS)\n## SAtab &lt;- addmargins(SAtab, margin=3, FUN=list(Total=sum))  ## Gdet Totals\nSAtabf &lt;- ftable(addmargins(SAtab, margin=3, FUN=DeadPer1000), col.vars=3)\nprint(SAtabf, digits=2, method=\"compact\", big.mark=\",\")\n\n\nFSAtab &lt;- xtabs(weight ~ dvcat + seatbelt + airbag + dead, data=nassCDS)\nFSAtabf &lt;- ftable(addmargins(FSAtab, margin=4, FUN=DeadPer1000), col.vars=3:4)\nprint(FSAtabf, digits=1)\n\n\n\n\nSubsection 1.3.2: Summaries of information from data frames\n\n## Individual vine yields, with means by block and treatment overlaid\nkiwishade &lt;- DAAG::kiwishade\nkiwishade$block &lt;- factor(kiwishade$block, levels=c(\"west\",\"north\",\"east\"))\nkeyset &lt;- list(space=\"top\", columns=2,\ntext=list(c(\"Individual vine yields\", \"Plot means (4 vines)\")),\npoints=list(pch=c(1,3), cex=c(1,1.35), col=c(\"gray40\",\"black\")))\npanelfun &lt;- function(x,y,...){panel.dotplot(x,y, pch=1, ...)\nav &lt;- sapply(split(x,y),mean); ypos &lt;- unique(y)\nlpoints(ypos~av, pch=3, col=\"black\")}\ndotplot(shade~yield | block, data=kiwishade, col=\"gray40\", aspect=0.65,\n        panel=panelfun, key=keyset, layout=c(3,1))\n## Note that parameter settings were given both in the calls\n## to the panel functions and in the list supplied to key.\n\n\n## mean yield by block by shade: data frame kiwishade (DAAG)\nkiwimeans &lt;- with(DAAG::kiwishade, \n                  aggregate(yield, by=list(block, shade), mean))\nnames(kiwimeans) &lt;- c(\"block\",\"shade\",\"meanyield\")\nhead(kiwimeans, 4)  # First 4 rows\n\n\nThe benefits of data summary – dengue status example\n\n\n\nSubsection 1.3.3: Measures of variation\n\nCuckoo eggs example\n\noptions(width=72)\n## SD of length, by species: data frame cuckoos (DAAG)\nz &lt;- with(cuckoos, sapply(split(length,species), function(x)c(sd(x),length(x))))\nprint(setNames(paste0(round(z[1,],2),\" (\",z[2,],\")\"),\n               abbreviate(colnames(z),11)), quote=FALSE)\n\n\n\nDegrees of freedom\n\n\n\nSubsection 1.3.4: Inter-quartile range (IQR) and median absolute deviation (MAD)\n\n\nSubsection 1.3.5: A pooled standard deviation estimate\n\nElastic bands example\n&lt;&gt;= sapply(DAAG::two65, function(x) c(Mean=mean(x), sd=sd(x))) |&gt; round(2) @\n\n\n\nSubsection 1.3.6: Effect size\n\nsetNames(diff(c(ambient=244.1, heated=253.5))/c(sd=10.91), \"Effect size\")\n\nData are available in the data frame DAAG::two65.\n\nvignette('effectsize', package='effectsize')\n\n\n\nSubsection 1.3.7: Correlation\n\nset.seed(17)\nx1 &lt;- x2 &lt;- x3 &lt;- (11:30)/5\ny1 &lt;- x1 + rnorm(20, sd=0.5)\ny2 &lt;- 2 - 0.05 * x1 + 0.1 * ((x1 - 1.75))^4 + rnorm(20, sd=1.5)\ny3 &lt;- (x1 - 3.85)^2 + 0.015 + rnorm(20)\ntheta &lt;- ((2 * pi) * (1:20))/20\nx4 &lt;- 10 + 4 * cos(theta)\ny4 &lt;- 10 + 4 * sin(theta) + rnorm(20, sd=0.6)\nxy &lt;- data.frame(x = c(rep(x1, 3), x4), y = c(y1, y2, y3, y4),\n                gp = factor(rep(1:4, rep(20, 4))))\nxysplit &lt;- split(xy, xy$gp)\nrho &lt;- sapply(xysplit, function(z)with(z,cor(x,y, method=c(\"pearson\"))))\nrhoS &lt;- sapply(xysplit, function(z)with(z,cor(x,y, method=c(\"spearman\"))))\nrnam &lt;- as.list(setNames(round(c(rho,rhoS),2), paste0(\"r\",1:8)))\nstriplabs &lt;- bquote(expression(paste(r==.(r1), \"    \",r[s]==.(r5)), \n                               paste(r==.(r2), \"    \",r[s]==.(r6)),\n                               paste(r==.(r3), \"    \",r[s]==.(r7)), \n                               paste(r==.(r4), \"    \",r[s]==.(r8))), rnam)\nxyplot(y ~ x | gp, data=xy, layout=c(4,1), xlab=\"\", ylab=\"\", \n  strip=strip.custom(factor.levels=striplabs), aspect=1,\n  scales=list(relation='free', draw=FALSE), between=list(x=0.5,y=0)\n)\n\n\n\n\nSection 1.4: Distributions: quantifying uncertainty\n\nSubsection 1.4.1: Discrete distributions\n\n## dbinom(0:10, size=10, prob=0.15)\nsetNames(round(dbinom(0:10, size=10, prob=0.15), 3), 0:10)\n\n\npbinom(q=4, size=10, prob=0.15)\n\n\nqbinom(p = 0.70, size = 10, prob = 0.15)\n## Check that this lies between the two cumulative probabilities:\n## pbinom(q = 1:2, size=10, prob=0.15)\n\n\nrbinom(15, size=4, p=0.5)\n\n\n## dpois(x = 0:8, lambda = 3)\nsetNames(round(dpois(x = 0:8, lambda = 3),4), 0:8)\n## Probability of &gt; 8 raisins\n## 1-ppois(q = 8, lambda = 3)     ## Or, ppois(q=8, lambda=3, lower.tail=FALSE)\n\n\n1 - ppois(q = 8, lambda = 3)\nppois(q=8, lambda=3, lower.tail=FALSE)  ## Alternative\n1-sum(dpois(x = 0:8, lambda = 3))       ## Another alternative\n\n\nraisins &lt;- rpois(20, 3)\nraisins\n\n\nInitializing the random number generator\n\nset.seed(23286)  # Use to reproduce the sample below\nrbinom(15, size=1, p=0.5)\n\n\n\nMeans and variances\n\n\n\nSubsection 1.4.2: Continuous distributions\n\nz &lt;- seq(-3,3,length=101)\nplot(z, dnorm(z), type=\"l\", ylab=\"Normal density\",\n     yaxs=\"i\", bty=\"L\",  tcl=-0.3, fg=\"gray\",\n    xlab=\"Distance, in SDs, from mean\", cex.lab=0.9)\npolygon(c(z[z &lt;= 1.0],1.0),c(dnorm(z[z &lt;= 1.0]), dnorm(-3)), col=\"grey\")\nchh &lt;- par()$cxy[2]\narrows(-1.8, 0.32, -0.25, 0.2, length=0.07, xpd=T)\ncump &lt;- round(pnorm(1), 3)\ntext(-1.8, 0.32+0.75*chh, paste(\"pnorm(1)\\n\", \"=\", cump), xpd=T, cex=0.8)\n\n\npnormExs &lt;- c('pnorm(0)', 'pnorm(1)', 'pnorm(-1.96)', 'pnorm(1.96)',\n'pnorm(1.96, mean=2)', 'pnorm(1.96, sd=2)')\nProb &lt;- sapply(pnormExs, function(x)eval(parse(text=x)))\ndf &lt;- as.data.frame(Prob)\ndf$Prob &lt;- round(df$Prob,3)\nprint(df)\n\n\n## Plot the normal density, in the range -3 to 3\nz &lt;- pretty(c(-3,3), 30)   # Find ~30 equally spaced points\nht &lt;- dnorm(z)             # Equivalent to dnorm(z, mean=0, sd=1)\nplot(z, ht, type=\"l\", xlab=\"Normal variate\", ylab=\"Density\", yaxs=\"i\")\n# yaxs=\"i\" locates the axes at the limits of the data\n\n\nqnorm(.9)          # 90th percentile; mean=0 and SD=1\n\n\n## Additional examples:\nsetNames(qnorm(c(.5,.841,.975)), nm=c(.5,.841,.975))\nqnorm(c(.1,.2,.3))   # -1.282 -0.842 -0.524  (10th, 20th and 30th percentiles)\nqnorm(.1, mean=100, sd=10)  # 87.2 (10th percentile, mean=100, SD=10)\n\n\nDifferent ways to represent distributions\n\n\nGenerating simulated samples from the normal and other continuous distributions\n\noptions(digits=2)  # Suggest number of digits to display\nrnorm(10)          # 10 random values from the normal distribution\n\n\nmu &lt;- 10\nsigma &lt;- 1\nn &lt;- 1\nm &lt;- 50\nfour &lt;- 4\nnrep &lt;- 5\nseed &lt;- 21\ntotrows &lt;- 1\nif(is.null(totrows))\ntotrows &lt;- floor(sqrt(nrep))\ntotcols &lt;- ceiling(nrep/totrows)\nz &lt;- range(pretty(mu + (c(-3.4, 3.4) * sigma), 50))\nxy &lt;- data.frame(x=rep(0,nrep),y=rep(0,nrep),n=rep(n,nrep),\n                 mm=rep(m,nrep),four=rep(four,nrep))\nfac &lt;- factor(paste(\"Simulation\", 1:nrep),\n              lev &lt;- paste(\"Simulation\", 1:nrep))\nxlim&lt;-z\n## ylim&lt;-c(0,dnorm(0)*sqrt(n))\nylim &lt;- c(0,1)\nxy &lt;- split(xy,fac)\nxy&lt;-lapply(1:length(xy),function(i){c(as.list(xy[[i]]), list(xlim=xlim,\n           ylim=ylim))})\npanel.mean &lt;- function(data, mu = 10, sigma = 1, n2 = 1,\n                       mm = 100, nrows, ncols, ...)\n{\n  vline &lt;- function(x, y, lty = 1, col = 1)\n  lines(c(x, x), c(0, y), lty = lty, col = col)\n  n2&lt;-data$n[1]\n  mm&lt;-data$mm[1]\n  our&lt;-data$four[1]  ## Four characters in each unit interval of x\n  nmid &lt;- round(four * 4)\n  nn &lt;- array(0, 2 * nmid + 1)\n  #########################################\n  z &lt;- mu+seq(from=-3.4*sigma, to=3.4*sigma, length=mm)\n  atx&lt;-pretty(z)\n  qz &lt;- pnorm((z - mu)/sigma)\n  dz &lt;- dnorm((z - mu)/sigma)\n  chw &lt;- sigma/four\n  chh &lt;- strheight(\"O\")*0.75\n  htfac &lt;- (mm * chh)/four\n  if(nrows==1&&ncols==1)\n  lines(z, dz * htfac)\n  if(nrows==1)axis(1,at=atx, lwd=0, lwd.ticks=1)\n  y &lt;- rnorm(mm, mu, sigma/sqrt(n2))\n  pos &lt;- round((y - mu)/sigma * four)\n  for(i in 1:mm) {\n    nn[nmid + pos[i]] &lt;- nn[nmid + pos[i]] + 1\n    xpos &lt;- chw * pos[i]\n    text(mu + xpos, nn[nmid + pos[i]] * chh - chh/4, \"x\")\n  }\n}\nDAAG::panelplot(xy,panel=panel.mean,totrows=totrows,totcols=totcols,\n  oma=c(1.5, 0, rep(0.5,2)), fg='gray')\n\n\n## The following gives a conventional histogram representations:\nset.seed (21)        # Use to reproduce the data in the figure\ndf &lt;- data.frame(x=rnorm(250), gp=rep(1:5, rep(50,5)))\nlattice::histogram(~x|gp, data=df, layout=c(5,1))\n\n\nrunif(n = 20, min=0, max=1) # 20 numbers, uniform distn on (0, 1)\nrexp(n=10, rate=3)          # 10 numbers, exponential, mean 1/3.\n\n\n\n\nSubsection 1.4.3: Graphical checks for normality\n\ntab &lt;- t(as.matrix(DAAG::pair65))\nrbind(tab,\"heated-ambient\"=tab[1,]-tab[2,])\n\n\n## Normal quantile-quantile plot for heated-ambient differences,\n## compared with plots for random normal samples of the same size\nplt &lt;- with(DAAG::pair65, DAAG::qreference(heated-ambient, nrep=10, nrows=2))\nupdate(plt, scales=list(tck=0.4), xlab=\"\")\n\n\n\nSubsection 1.4.4: Population parameters and sample statistics\n\nThe sampling distribution of the mean\n\nlibrary(lattice)\n## Generate n sample values; skew population\nsampfun = function(n) exp(rnorm(n, mean = 0.5, sd = 0.3))\ngph &lt;- DAAG::sampdist(sampsize = c(3, 9, 30), seed = 23, nsamp = 1000,\n  FUN = mean, sampvals=sampfun, plot.type = \"density\")\nsamptheme &lt;- DAAG::DAAGtheme(color=FALSE)\nprint(update(gph, scales=list(tck=0.4),  layout = c(3,1),\n             par.settings=samptheme, main=list(\"A: Density curves\", cex=1.25)),\n      position=c(0,0.5,1,1), more=TRUE)\nsampfun = function(n) exp(rnorm(n, mean = 0.5, sd = 0.3))\ngph &lt;- DAAG::sampdist(sampsize = c(3, 9, 30), seed = 23, nsamp = 1000, \n                      FUN = mean, sampvals=sampfun, plot.type = \"qq\")\nprint(update(gph, scales=list(tck=0.4), layout = c(3,1),\n             par.settings=samptheme, \n             main=list(\"B: Normal quantile-quantile plots\", cex=1.25)),\n      position=c(0,0,1,0.5))\n\n\n\nThe standard error of the median\n\n\nSimulation in learning and research\n\n\n\nSubsection 1.4.5: The \\(t\\)-distribution\n\nx &lt;- seq(from=-4.2, to = 4.2, length.out = 50)\nylim &lt;- c(0, dnorm(0))\nylim[2] &lt;- ylim[2]+0.1*diff(ylim)\nh1 &lt;- dnorm(x)\nh3 &lt;- dt(x, 3)\nh8 &lt;- dt(x,8)\nplot(x, h1, type=\"l\", xlab = \"\", xaxs=\"i\", ylab = \"\", yaxs=\"i\",\nbty=\"L\", ylim=ylim, fg=\"gray\")\nmtext(side=3,line=0.5, \"A: Normal (t8 overlaid)\", adj=-0.2)\nlines(x, h8, col=\"grey60\")\nmtext(side=1, line=1.75, \"No. of SEMs from mean\")\nmtext(side=2, line=2.0, \"Probability density\")\nchh &lt;- par()$cxy[2]\ntopleft &lt;- par()$usr[c(1,4)] + c(0, 0.6*chh)\nlegend(topleft[1], topleft[2], col=c(\"black\",\"grey60\"),\nlty=c(1,1), legend=c(\"Normal\",\"t (8 d.f.)\"), bty=\"n\", cex=0.8)\nplot(x, h1, type=\"l\", xlab = \"\", xaxs=\"i\",\nylab = \"\", yaxs=\"i\", bty=\"L\", ylim=ylim, fg=\"gray\")\nmtext(side=3,line=0.5, \"B: Normal (t3 overlaid)\", adj=-0.2)\nlines(x, h3, col=\"grey60\")\nmtext(side=1, line=1.75, \"No. of SEMs from mean\")\n## mtext(side=2, line=2.0, \"Probability density\")\nlegend(topleft[1], topleft[2], col=c(\"black\",\"grey60\"),\nlty=c(1,1), legend=c(\"Normal\",\"t (3 d.f.)\"), bty=\"n\", cex=0.8)\n## Panels C and D\ncump &lt;- 0.975\nx &lt;- seq(from=-3.9, to = 3.9, length.out = 50)\nylim &lt;- c(0, dnorm(0))\nplotfun &lt;- function(cump, dfun = dnorm, qfun=qnorm,\nytxt = \"Probability density\",\ntxt1=\"qnorm\", txt2=\"\", ...)\n{\nh &lt;- dfun(x)\nplot(x, h, type=\"l\", xlab = \"\", xaxs=\"i\", xaxt=\"n\",\nylab = ytxt, yaxs=\"i\", bty=\"L\", ylim=ylim, fg=\"gray\",\n...)\naxis(1, at=c(-2, 0), cex=0.8, lwd=0, lwd.ticks=1)\naxis(1, at=c((-3):3), labels=F, lwd=0, lwd.ticks=1)\ntailp &lt;- 1-cump\nz &lt;- qfun(cump)\nztail &lt;- pretty(c(z,4),20)\nhtail &lt;- dfun(ztail)\npolygon(c(z,z,ztail,max(ztail)), c(0,dfun(z),htail,0), col=\"gray\")\ntext(0, 0.5*dfun(z)+0.08*dfun(0),\npaste(round(tailp, 3), \" + \", round(1-2*tailp,3),\n\"\\n= \", round(cump, 3), sep=\"\"), cex=0.8)\nlines(rep(z, 2), c(0, dfun(z)))\nlines(rep(-z, 2), c(0, dfun(z)), col=\"gray60\")\nchh &lt;- par()$cxy[2]\narrows(z, -1.5*chh,z,-0.1*chh, length=0.1, xpd=T)\ntext(z, -2.5*chh, paste(txt1, \"(\", cump, txt2, \")\", \"\\n= \",\nround(z,2), sep=\"\"), xpd=T)\nx1 &lt;- z + .3\ny1 &lt;- dfun(x1)*0.35\ny0 &lt;- dfun(0)*0.2\narrows(-2.75, y0, -x1, y1, length=0.1, col=\"gray60\")\narrows(2.75, y0, x1, y1, length=0.1)\ntext(-2.75, y0+0.5*chh, tailp, col=\"gray60\")\ntext(2.75, y0+0.5*chh, tailp)\n}\n## ytxt &lt;- \"t probability density (8 d.f.)\"\nplotfun(cump=cump, cex.lab=1.05)\nmtext(side=3, line=1.25, \"C: Normal distribution\", adj=-0.2)\nytxt &lt;- \"t probability density (8 d.f.)\"\nplotfun(cump=cump, dfun=function(x)dt(x, 8),\n        qfun=function(x)qt(x, 8),\n        ytxt=\"\", txt1=\"qt\", txt2=\", 8\", cex.lab=1.05)\nmtext(side=3, line=1.25, \"D: t distribution (8 df)\", adj=-0.2)\n\n\nqnorm(c(0.975,0.995), mean=0)    # normal distribution\nqt(c(0.975, 0.995), df=8)        # t-distribution with 8 d.f.\n\n\n\nSubsection 1.4.6: The likelihood, and maximum likelihood estimation\n\n\n\nSection 1.5: Simple forms of regression model\n\nSubsection 1.5.1: Line or curve?\n\nroller &lt;- DAAG::roller\nt(cbind(roller, \"depression/weight ratio\"=round(roller[,2]/roller[,1],2)))\n\n\nUsing models to predict\n\ny &lt;- DAAG::roller$depression\nx &lt;- DAAG::roller$weight\npretext &lt;- c(reg = \"A\", lo = \"B\")\nfor(curve in c(\"reg\", \"lo\")) {\n  plot(x, y, xlab = \"Roller weight (t)\", xlim=c(0,12.75), fg=\"gray\",\n       ylab = \"Depression in lawn (mm)\", type=\"n\")\n  points(x, y, cex=0.8, pch = 4)\n  mtext(side = 3, line = 0.25, pretext[curve], adj = 0)\n  topleft &lt;- par()$usr[c(1, 4)]\n  chw &lt;- strwidth(\"O\"); chh &lt;- strheight(\"O\")\n  points(topleft[1]+rep(0.75,2)*chw,topleft[2]-c(0.75,1.8)*chh,\n         pch=c(4,1), col=c(\"black\",\"gray40\"), cex=0.8)\n  text(topleft[1]+rep(1.2,2)*chw, topleft[2]-c(0.75,1.8)*chh,\n       c(\"Data values\", \"Fitted values\"),adj=0, cex=0.8)\n  if(curve==\"lo\")\n    text(topleft[1]+1.2*chw, topleft[2]-2.85*chh,\"(smooth)\", adj=0, cex=0.8)\n  if(curve[1] == \"reg\") {\n    u &lt;- lm(y ~ -1 + x)\n  abline(0, u$coef[1])\n  yhat &lt;- predict(u)\n}\nelse {\n  lawn.lm&lt;-lm(y~x+I(x^2))\n  yhat&lt;-predict(lawn.lm)\n  xnew&lt;-pretty(x,20)\n  b&lt;-lawn.lm$coef\n  ynew&lt;-b[1]+b[2]*xnew+b[3]*xnew^2\n  lines(xnew,ynew)\n}\nhere &lt;- y &lt; yhat\nyyhat &lt;- as.vector(rbind(y[here], yhat[here], rep(NA, sum(here))))\nxx &lt;- as.vector(rbind(x[here], x[here], rep(NA, sum(here))))\nlines(xx, yyhat, lty = 2, col=\"gray\")\nhere &lt;- y &gt; yhat\nyyhat &lt;- as.vector(rbind(y[here], yhat[here], rep(NA, sum(here))))\nxx &lt;- as.vector(rbind(x[here], x[here], rep(NA, sum(here))))\nlines(xx, yyhat, lty = 1, col=\"gray\")\nn &lt;- length(y)\nns &lt;- min((1:n)[y - yhat &gt;= 0.75*max(y - yhat)])\nypos &lt;- 0.5 * (y[ns] + yhat[ns])\nchw &lt;- par()$cxy[1]\ntext(x[ns] - 0.25*chw, ypos, \"+ve residual\", adj = 1,cex=0.75, col=\"gray30\")\npoints(x, yhat, pch = 1, col=\"gray40\")\nns &lt;- (1:n)[y - yhat == min(y - yhat)][1]\nypos &lt;- 0.5 * (y[ns] + yhat[ns])\ntext(x[ns] + 0.4*chw, ypos, \"-ve residual\", adj = 0,cex=0.75,col=\"gray30\")\n}\n\n\n\nWhich model is best — line or curve?\n\n\n\nSubsection 1.5.2: Fitting models – the model formula\n\n## Fit line - by default, this fits intercept & slope.\nroller.lm &lt;- lm(depression ~ weight, data=DAAG::roller)\n## Compare with the code used to plot the data\nplot(depression ~ weight, data=DAAG::roller)\n## Add the fitted line to the plot\nabline(roller.lm)\n\n\n## For a model that omits the intercept term, specify\nlm(depression ~ 0 + weight, data=roller)  # Or, if preferred, replace `0` by `-1`\n\n\nModel objects\n\nroller.lm &lt;- lm(depression ~ weight, data=DAAG::roller)\nnames(roller.lm)     # Get names of list elements\n\n\ncoef(roller.lm)           # Extract coefficients\nsummary(roller.lm)        # Extract model summary information\ncoef(summary(roller.lm))  # Extract coefficients and SEs\nfitted(roller.lm)         # Extract fitted values\npredict(roller.lm)        # Predictions for existing or new data, with SE\n                          # or confidence interval information if required.\nresid(roller.lm)          # Extract residuals\n\n\nroller.lm$coef            # An alternative is roller.lm[[\"coef\"]]\n\n\nprint(summary(roller.lm), digits=3)\n\n\n\nResidual plots\n\n## Normal quantile-quantile plot, plus 7 reference plots\nDAAG::qreference(residuals(roller.lm), nrep=8, nrows=2, xlab=\"\")\n\n\n\nSimulation of regression data\n\nroller.lm &lt;- lm(depression ~ weight, data=DAAG::roller)\nroller.sim &lt;- simulate(roller.lm, nsim=20)  # 20 simulations\n\n\nwith(DAAG::roller, matplot(weight, roller.sim, pch=1, ylim=range(depression)))\npoints(DAAG::roller, pch=16)\n\n\n\n\nSubsection 1.5.3: The model matrix in regression\n\nmodel.matrix(roller.lm)\n## Specify coef(roller.lm) to obtain the column multipliers.\n\n\nFrom straight line regression to multiple regression\n\nmouse.lm &lt;- lm(brainwt ~ lsize+bodywt, data=DAAG::litters)\ncoef(summary(mouse.lm))\n\n\n\n\n\nSection 1.6: Data-based judgments – frequentist, in a Bayesian world\n\nSubsection 1.6.1: Inference with known prior probabilities\n\n## `before` is the `prevalence` or `prior`. \nafter &lt;- function(prevalence, sens, spec){\n  prPos &lt;- sens*prevalence + (1-spec)*(1-prevalence)\n  sens*prevalence/prPos}\n## Compare posterior for a prior of 0.002 with those for 0.02 and 0.2\nsetNames(round(after(prevalence=c(0.002, 0.02, 0.2), sens=.8, spec=.95), 3),\n         c(\"Prevalence=0.002\", \"Prevalence=0.02\", \"Prevalence=0.2\"))\n\n\nRelating ‘incriminating’ evidence to the probability of guilt\n\n\n\nSubsection 1.6.2: Treatment differences that are on a continuous scale\n\n## Use pipe syntax, introduced in R 4.1.0\nsleep &lt;- with(datasets::sleep, extra[group==2] - extra[group==1])\nsleep |&gt; (function(x)c(mean = mean(x), SD = sd(x), n=length(x)))() |&gt; \n     (function(x)c(x, SEM=x['SD']/sqrt(x['n'])))() |&gt;\n     setNames(c(\"mean\",\"SD\",\"n\",\"SEM\")) -&gt; stats\n     print(stats, digits=3)\n\n\n## Sum of tail probabilities\n2*pt(1.580/0.389, 9, lower.tail=FALSE)  \n\n\n## 95% CI for mean of heated-ambient: data frame DAAG::pair65\nt.test(sleep, conf.level=0.95)\n\n\nAn hypothesis test\n\npt(4.06, 9, lower.tail=F)\n\n\n\nThe \\(p\\)-value probability relates to a decision process\n\n\n\nSubsection 1.6.3: Use of simulation with \\(p\\)-values\n\neff2stat &lt;- function(eff=c(.2,.4,.8,1.2), n=c(10,40), numreps=100,\n                     FUN=function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1, \n                                         lower.tail=FALSE)){\n  simStat &lt;- function(eff=c(.2,.4,.8,1.2), N=10, nrep=100, FUN){\n    num &lt;- N*nrep*length(eff)\n    array(rnorm(num, mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N) \n  }\n  mat &lt;- matrix(nrow=numreps*length(eff),ncol=length(n))\n  for(j in 1:length(n)) mat[,j] &lt;- \n    as.vector(simStat(eff, N=n[j], numreps, FUN=FUN))  ## length(eff)*numep\n  data.frame(effsize=rep(rep(eff, each=numreps), length(n)),\n             N=rep(n, each=numreps*length(eff)), stat=as.vector(mat))\n}\n\n\nset.seed(31)\ndf200 &lt;- eff2stat(eff=c(.2,.4,.8,1.2), n=c(10, 40), numreps=200)\nlabx &lt;- c(0.001,0.01,0.05,0.2,0.4,0.8)\ngph &lt;- bwplot(factor(effsize) ~ I(stat^0.25) | factor(N), data=df200, \n              layout=c(2,1), xlab=\"P-value\", ylab=\"Effect size\", \n              scales=list(x=list(at=labx^0.25, labels =labx)))\nupdate(gph+latticeExtra::layer(panel.abline(v=labx[1:3]^0.25, col='lightgray')),\n       strip=strip.custom(factor.levels=paste0(\"n=\",c(10,40))),\n       par.settings=DAAG::DAAGtheme(color=F, col.points=\"gray50\"))\n\n\neff10 &lt;- with(subset(df200, N==10&effsize==0.2), c(gt5pc=sum(stat&gt;0.05), lohi=fivenum(stat)[c(2,4)]))\neff40 &lt;- with(subset(df200, N==40&effsize==0.2), c(gt5pc=sum(stat&gt;0.05), lohi=fivenum(stat)[c(2,4)]))\n\n\n\nSubsection 1.6.4: Power — minimizing the chance of false positives\n\ntf1 &lt;- rbind('R=0.2'=c(0.8*50, 0.05*250),\n'R=1'=c(0.8*150, 0.05*150),\n'R=5'=c(0.8*200, 0.05*50))\ntf2 &lt;- rbind(c('0.8 x50', '0.05x250'),\nc('0.8x150', '0.05x150'),\nc('0.8x250', '0.05 x50'))\ntf &lt;- cbind(\"True positives\"=paste(tf2[,2],tf1[,2],sep=\"=\"),\n\"False positives\"=paste(tf2[,1],tf1[,1],sep=\"=\"))\nrownames(tf) &lt;- rownames(tf1)\nprint(tf, quote=FALSE)\n\n\nPower calculations – examples\n\npower.t.test(d=0.5, sig.level=0.05, type=\"one.sample\", power=0.8)\npwr1 &lt;- power.t.test(d=0.5, sig.level=0.005, type=\"one.sample\", power=0.8)\npwr2 &lt;- power.t.test(d=0.5, sig.level=0.005, type=\"two.sample\", power=0.8)\n## d=0.5, sig.level=0.005, One- and two-sample numbers \nc(\"One sample\"=pwr1$n, \"Two sample\"=pwr2$n)\n\n\neffsize &lt;- c(.05,.2,.4,.8,1.2); npairs &lt;- c(10,20,40)\npwr0.05 &lt;- matrix(nrow=length(effsize), ncol=length(npairs),\n               dimnames=list(paste0('ES=',effsize), paste0('n=',npairs)))\npwr0.005 &lt;- matrix(nrow=length(effsize), ncol=length(npairs),\n               dimnames=list(paste0(effsize), paste0('n=',npairs)))\nfor(i in 1:length(effsize)) for(j in 1:length(npairs)){\n    pwr0.05[i,j] &lt;- power.t.test(n=npairs[j],d=effsize[i],sig.level=0.05,\n                                 type='one.sample')$power\n    pwr0.005[i,j] &lt;- power.t.test(n=npairs[j],d=effsize[i],sig.level=0.005,\n                                  type='one.sample')$power}\ntab &lt;- cbind(round(pwr0.05,4), round(pwr0.005,4))\ntab[1:3,] &lt;- round(tab[1:3,],3)\ntab[5,3] &lt;- '~1.0000'\ntab[5,6] &lt;- '~1.0000'\n\n\nprint(tab[,1:3], quote=F)\n\n\nprint(tab[,4:6], quote=F)\n\n\neffsize &lt;- c(.05,.2,.4,.8,1.2); npairs &lt;- c(10,20,40)\npwr0.05 &lt;- matrix(nrow=length(effsize), ncol=length(npairs),\n               dimnames=list(paste0('ES=',effsize), paste0('n=',npairs)))\npwr0.005 &lt;- matrix(nrow=length(effsize), ncol=length(npairs),\n               dimnames=list(paste0(effsize), paste0('n=',npairs)))\nfor(i in 1:length(effsize)) for(j in 1:length(npairs)){\n    pwr0.05[i,j] &lt;- power.t.test(n=npairs[j],d=effsize[i],sig.level=0.05,\n                                 type='one.sample')$power\n    pwr0.005[i,j] &lt;- power.t.test(n=npairs[j],d=effsize[i],sig.level=0.005,\n                                  type='one.sample')$power}\ntab &lt;- cbind(round(pwr0.05,4), round(pwr0.005,4))\ntab[1:3,] &lt;- round(tab[1:3,],3)\ntab[5,3] &lt;- '~1.0000'\ntab[5,6] &lt;- '~1.0000'\n\n\n\nPositive Predictive Values\n\nR &lt;- pretty(0:3, 40)\npostOdds &lt;- outer(R/0.05,c(.8,.3,.08))\nPPV &lt;- as.data.frame(cbind(R,postOdds/(1+postOdds)))\nnames(PPV) &lt;- c(\"R\",\"p80\",\"p30\",\"p8\")\nkey &lt;- list(text = list(text=c(\"80% power\",\"30% power\", \"8% power\"), cex = 1.0),\n            x = .6, y = .25, color=F)\ngph &lt;- lattice::xyplot(p80+p30+p8~R, data=PPV, lwd=2, type=c(\"l\",\"g\"), \n  xlab=\"Pre-study odds R\", ylab=\"Post-study probability (PPV)\")\nupdate(gph, scales=list(tck=0.5), key=key) \n\n\n\n\nSubsection 1.6.5: The future for \\(p\\)-values\n\nHow small a \\(p\\)-value is needed?\n\n\n\nSubsection 1.6.6: Reporting results\n\nIs there an alternative that is more likely?\n\n\n\n\nSection 1.7: Information statistics and Bayesian methods with Bayes Factors\n\nSubsection 1.7.1: Information statistics – using likelihoods for model choice\n\n## Calculations using mouse brain weight data\nmouse.lm &lt;- lm(brainwt ~ lsize+bodywt, data=DAAG::litters)\nn &lt;- nrow(DAAG::litters)\nRSSlogLik &lt;- with(mouse.lm, n*(log(sum(residuals^2)/n)+1+log(2*pi)))\np &lt;- length(coef(mouse.lm))+1  # NB: p=4 (3 coefficients + 1 scale parameter)\nk &lt;- 2*n/(n-p-1)\nc(\"AICc\" = AICcmodavg::AICc(mouse.lm), fromlogL=k*p-2*logLik(mouse.lm)[1], \n  fromFit=k*p + RSSlogLik) |&gt; print(digits=4)\n\n\nThe sampling properties of the difference in AIC statistics\n\nsim0vs1 &lt;- function(mu=0, n=15, ntimes=200){\na0 &lt;- a1 &lt;- numeric(ntimes)\nfor(i in 1:ntimes){\n  y &lt;- rnorm(n, mean=mu, sd=1)\n  m0 &lt;- lm(y ~ 0); m1 &lt;- lm(y ~ 1) \n  a0[i] &lt;- AIC(m0); a1[i] &lt;- AIC(m1)\n}\ndata.frame(a0=a0, a1=a1, diff01=a0-a1, mu=rep(paste0(\"mu=\",mu)))\n}\nlibrary(latticeExtra)\nsim0 &lt;- sim0vs1(mu=0)\nsim0.5 &lt;- sim0vs1(mu=0.5)\nsimboth &lt;- rbind(sim0, sim0.5)\ncdiff &lt;- with(list(n=15, p=2), 2*(p+1)*p/(n-(p+1)-1))\nxyplot(diff01 ~ a0 | mu, data=simboth, xlab=\"AIC(m0)\", ylab=\"AIC(m0) - AIC(m1)\") + \n  latticeExtra::layer({panel.abline(h=0, col='red'); \n         panel.abline(h=cdiff, lwd=1.5, lty=3, col='red', alpha=0.5);\n         panel.abline(h=-2, lty=2, col='red')})\n\n\ntab &lt;- rbind(c(with(sim0, sum(diff01&gt;0))/200, with(sim0.5, sum(diff01&gt;0))/200),\n  c(with(sim0,sum(diff01&gt;-cdiff))/200, with(sim0.5, sum(diff01&gt;-cdiff))/200))\ndimnames(tab) &lt;- list(c(\"AIC: Proportion choosing m1\",\n                        \"AICc: Proportion choosing m1\"),\n                      c(\"True model is m0\", \"True model is m1\"))\ntab\n\n\n\n\nSubsection 1.7.2: Bayesian methods with Bayes Factors\n\n## Setting `scale=1/sqrt(2)` gives a mildly narrower distribution\nprint(c(\"pcauchy(1, scale=1)\"=pcauchy(1, scale=1), \n        \"   pcauchy(1, scale=1/sqrt(2))\"=pcauchy(1, scale=1/sqrt(2))),\n      quote=FALSE)\n\n\nThe Cauchy prior with different choices of scale parameter\n\nx &lt;- seq(from=-4.5, to=4.5, by=0.1)\ndensMed &lt;- dcauchy(x,scale=sqrt(2)/2)\ndensUltra &lt;- dcauchy(x, scale=sqrt(2))\ndenn &lt;- dnorm(x, sd=1)\nplot(x,densMed, type='l', mgp=c(2,0.5,0), xlab=\"\",\n     ylab=\"Prior density\", col=\"red\", fg='gray')\nmtext(side=1, line=2, expression(\"Effect size \"==phantom(0)*delta), cex=1.1)\nlines(x, denn, col=\"blue\", lty=2)\nlines(x, densUltra,col=2, lty=2)\nlegend(\"topleft\", title=\"Normal prior\",\n       y.intersp=0.8, lty=2, col=\"blue\", bty='n', cex=0.8,\n       legend=expression(bold('sd=1')))\nlegend(\"topright\", title=\"Cauchy priors\", y.intersp=0.8,\n       col=c('red', 'red'),lty=c(1,2), cex=0.8,\n         legend=c(expression(bold('medium')),\n         expression(bold('ultrawide'))),bty=\"n\")\nmtext(side=3, line=0.25, adj=0, cex=1.15,\n      expression(\"A: Alternative priors for \"*delta==frac(mu,sigma)))\n## Panel B\npairedDiffs &lt;- with(datasets::sleep, extra[group==2] - extra[group==1])\nttBF0 &lt;- BayesFactor::ttestBF(pairedDiffs)\nsimpost &lt;- BayesFactor::posterior(ttBF0, iterations=10000)\nplot(density(simpost[,'mu']), main=\"\", xlab=\"\", col=\"red\",\n     mgp=c(2,0.5,0), ylab=\"Posterior density\", fg='gray')\nmtext(side=1, line=2, expression(mu), cex=1.1)\nabline(v=mean(pairedDiffs), col=\"gray\")\nmtext(side=3, line=0.5, expression(\"B: Posterior density for \"*mu), adj=0, cex=1.15)\n\n\n## Calculate and plot density for default prior - Selected lines of code\nx &lt;- seq(from=-4.5, to=4.5, by=0.1)\ndensMed &lt;- dcauchy(x, scale=sqrt(2)/2)\nplot(x, densMed, type='l')\n## Panel B\npairedDiffs &lt;- with(datasets::sleep, extra[group==2] - extra[group==1])\nttBF0 &lt;- BayesFactor::ttestBF(pairedDiffs)\n## Sample from posterior, and show density plot for mu\nsimpost &lt;- BayesFactor::posterior(ttBF0, iterations=10000)\nplot(density(simpost[,'mu']))\n\n\n\nA thought experiment\n\ntval &lt;- setNames(qt(1-c(.05,.01,.005)/2, df=19), paste(c(.05,.01,.005)))\nbf01 &lt;- setNames(numeric(3), paste(c(.05,.01,.005)))\nfor(i in 1:3)bf01[i] &lt;- BayesFactor::ttest.tstat(tval[i],n1=20, simple=T)\n\n\npairedDiffs &lt;- with(datasets::sleep, extra[group==2] - extra[group==1])\nttBF0 &lt;- BayesFactor::ttestBF(pairedDiffs)\nttBFwide &lt;- BayesFactor::ttestBF(pairedDiffs, rscale=1)\nttBFultra &lt;- BayesFactor::ttestBF(pairedDiffs, rscale=sqrt(2))\nrscales &lt;- c(\"medium\"=sqrt(2)/2, \"wide\"=1, ultrawide=sqrt(2))\nBF3 &lt;- c(as.data.frame(ttBF0)[['bf']], as.data.frame(ttBFwide)[['bf']],\n         as.data.frame(ttBFultra)[['bf']])\nsetNames(round(BF3,2), c(\"medium\", \"wide\", \"ultrawide\"))\n\n\npval &lt;- t.test(pairedDiffs)[['p.value']]\n1/(-exp(1)*pval*log(pval))\n\n\n\nA null interval may make better sense\n\nmin45 &lt;- round(0.75/sd(pairedDiffs),2)   ## Use standardized units\nttBFint &lt;- BayesFactor::ttestBF(pairedDiffs, nullInterval=c(-min45,min45))\nround(as.data.frame(ttBFint)['bf'],3)\n\n\nbf01 &lt;- as.data.frame(ttBFint)[['bf']]\n\n\n\nThe effect of changing sample size\n\nt2bfInterval &lt;- function(t, n=10, rscale=\"medium\", mu=c(-.1,.1)){\n     null0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu,\n                                       rscale=rscale,simple=TRUE)\nalt0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu, rscale=rscale, \n                                 complement=TRUE, simple=TRUE)\nalt0/null0\n}\n##\n## Calculate Bayes factors\npval &lt;- c(0.05,0.01,0.001); nval &lt;- c(4,6,10,20,40,80,160)\nbfDF &lt;- expand.grid(p=pval, n=nval)\npcol &lt;- 1; ncol &lt;- 2; tcol &lt;- 3\nbfDF[,'t'] &lt;- apply(bfDF,1,function(x){qt(x[pcol]/2, df=x[ncol]-1,                                  lower.tail=FALSE)})\nother &lt;- apply(bfDF,1,function(x)\n    c(BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"medium\",\n                               simple=TRUE),\n## Now specify a null interval\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"medium\")\n  ))\nbfDF &lt;- setNames(cbind(bfDF, t(other)),\n    c('p','n','t','bf','bfInterval'))\n\n\nplabpos &lt;- with(subset(bfDF, n==max(bfDF$n)), log((bf+bfInterval)/2))\ngphA1 &lt;- lattice::xyplot(log(bf)~log(n), groups=factor(p), data=bfDF,\n                        panel=function(x,y,...){\n                        lattice::panel.xyplot(x,y,type='b',...)})\nylabA &lt;- 10^((-3):6/2)\nscalesA &lt;- list(x=list(at=log(nval), labels=nval),\n                y=list(at=log(ylabA), labels=signif(ylabA,2)))\nkeyA &lt;- list(corner=c(0.99,0.98), lines=list(col=c(1,1), lty=1:2),\n             text=list(c('Point null at 0', \"null=(-0.1,0.1)\")))\nylim2 &lt;- log(c(min(bfDF[['bfInterval']])-0.05,150)) \ngphA2 &lt;- lattice::xyplot(log(bfInterval)~log(n), groups=factor(p), lty=2,\n  xlim=c(log(3.5),log(max(nval)*3.25)), ylim=ylim2, data=bfDF,\n  panel=function(x,y,...){\n    panel.xyplot(x,y,type='b',...)\n    panel.grid(h=-1,v=-1)\n    panel.text(rep(log(max(nval*0.975)),3), plabpos, \n      labels=c('p=0.05','0.01','0.001'), pos=4)\n  },\n  par.settings=DAAG::DAAGtheme(color=T),\n  main=\"A: Bayes factor vs sample size\", \n  xlab=\"Sample size\", ylab=\"Bayes factor\", scales=scalesA, key=keyA)\n## Panel B\nbfDF[['eff']] = bfDF[[\"t\"]]/sqrt(bfDF[['n']])\nylabB &lt;- 10^((-3):2/3)\nscalesB= list(x=list(at=log(nval), labels=nval),\n              y=list(at=log(ylabB), labels=signif(ylabB,2)))\nkeyB &lt;- list(corner=c(0.98,0.975), lines=list(lty=1:3), \n             points=list(pch=1:3), text=list(c('p=0.001','p=0.01','p=0.05')))\ngphB &lt;- xyplot(log(eff)~log(n), groups=log(p), data=bfDF, pch=1:3, lty=1:3, \n               type='b', xlab=\"Sample size\", ylab=\"Effect  size\",\n               par.settings=DAAG::DAAGtheme(color=T),\n  main=\"B: Effect size vs sample size\", key=keyB, scales=scalesB) +\n  latticeExtra::layer(panel.grid(h=-1,v=-1))\nplot(gphA2+latticeExtra::as.layer(gphA1), position=c(0, 0, 0.525, 1), more=T)\nplot(gphB, position=c(0.52, 0, 1, 1), par.settings=DAAG::DAAGtheme(color=T))\n\n\n\nDifferent statistics give different perspectives\n\nn1 &lt;- BayesFactor::ttest.tstat(qt(0.00001, df=40), n1=40, simple=T)\nn2 &lt;- BayesFactor::ttest.tstat(qt(0.000001, df=40), n1=40, simple=T)\n\n\nbf1 &lt;- BayesFactor::ttest.tstat(qt(0.00001, df=40), n1=40, simple=T)\nbf2 &lt;- BayesFactor::ttest.tstat(qt(0.000001, df=40), n1=40, simple=T)\nrbind(\"Bayes Factors\"=setNames(c(bf1,bf2), c(\"p=0.00001\",\"p=0.000001\")),\n  \"t-statistics\"=c(qt(0.00001, df=40), qt(0.000001, df=40))) \n\n\nknitr::kable(matrix(c(\"A bare mention\",\"Positive\",\"Strong\",\"Very strong\"), nrow=1),\n       col.names=c(\"1 -- 3\", \"3 -- 20\", \"20 -- 150\", \"&gt;150\"), align='c',\n      midrule='', vline=\"\")\n\n\n\n*Technical details of the family of priors used in the BayesFactor package\n\n\n\n\nSection 1.8: Resampling methods for SEs, tests and confidence intervals\n\nSubsection 1.8.1: The one-sample permutation test\n\ntab &lt;- t(as.matrix(DAAG::pair65))\nrbind(tab,\"heated-ambient\"=tab[1,]-tab[2,])\n\n\n\nSubsection 1.8.2: The two-sample permutation test\n\n## First of 3 curves; permutation distribution of difference in means\ntwo65 &lt;- DAAG::two65\nset.seed(47)        # Repeat curves shown here\nnsim &lt;- 2000; dsims &lt;- numeric(nsim)\nx &lt;- with(two65, c(ambient, heated))\nn &lt;- length(x); n2 &lt;- length(two65$heated)\ndbar &lt;- with(two65, mean(heated)-mean(ambient))\nfor(i in 1:nsim){\n  mn &lt;- sample(n,n2,replace=FALSE); dsims[i] &lt;- mean(x[mn]) - mean(x[-mn]) }\nplot(density(dsims), xlab=\"\", main=\"\", lwd=0.5, yaxs=\"i\", ylim=c(0,0.08), bty=\"n\")\nabline(v=c(dbar, -dbar), lty=3)\npval1 &lt;- (sum(dsims &gt;= abs(dbar)) + sum (dsims &lt;= -abs(dbar)))/nsim\nmtext(side=3,line=0.25,\n  text=expression(bar(italic(x))[2]-bar(italic(x))[1]), at=dbar)\nmtext(side=3,line=0.25,\n  text=expression(-(bar(italic(x))[2] - bar(italic(x))[1])), at=-dbar)\n## Second permutation density\nfor(i in 1:nsim){\nmn &lt;- sample(n,n2,replace=FALSE)\ndsims[i] &lt;- mean(x[mn]) - mean(x[-mn])\n}\npval2 &lt;- (sum(dsims &gt;= abs(dbar)) + sum (dsims &lt;= -abs(dbar)))/nsim\nlines(density(dsims),lty=2,lwd=1)\n## Third permutation density\nfor(i in 1:nsim){\nmn &lt;- sample(n,n2,replace=FALSE)\ndsims[i] &lt;- mean(x[mn]) - mean(x[-mn])\n}\npval3 &lt;- (sum(dsims &gt;= abs(dbar)) + sum (dsims &lt;= -abs(dbar)))/nsim\nlines(density(dsims),lty=3,lwd=1.25)\nbox(col=\"gray\")\nleg3 &lt;- paste(c(pval1,pval2,pval3))\nlegend(x=20, y=0.078, title=\"P-values are\", cex=1, xpd=TRUE,\n  bty=\"n\", lty=c(1,2,3), lwd=c(1,1,1,1.25), legend=leg3, y.intersp=0.8)\n\n\n\nSubsection 1.8.3: Estimating the standard error of the median: bootstrapping\n\n## Bootstrap estimate of median of wren length: data frame cuckoos\nwren &lt;- subset(DAAG::cuckoos, species==\"wren\")[, \"length\"]\nlibrary(boot)\n## First define median.fun(), with two required arguments:\n##         data specifies the data vector,\n##         indices selects vector elements for each resample\nmedian.fun &lt;- function(data, indices){median(data[indices])}\n## Call boot(), with statistic=median.fun, R = # of resamples\nset.seed(23)\n(wren.boot &lt;- boot(data = wren, statistic = median.fun, R = 4999))\n\n\n\nSubsection 1.8.4: Bootstrap estimates of confidence intervals\n\nBootstrap 95% confidence intervals for the median\n\n## Call the function boot.ci() , with boot.out=wren.boot\nboot.ci(boot.out=wren.boot, type=c(\"perc\",\"bca\"))\n\n\n\nThe correlation coefficient\n\n## Bootstrap estimate of 95% CI for `cor(chest, belly)`: `DAAG::possum`\ncorr.fun &lt;- function(data, indices) \n  with(data, cor(belly[indices], chest[indices]))\nset.seed(29)\ncorr.boot &lt;- boot(DAAG::possum, corr.fun, R=9999)\n\n\nlibrary(boot)\nboot.ci(boot.out = corr.boot, type = c(\"perc\", \"bca\"))\n\n\n\nThe bootstrap – parting comments\n\n\n\n\nSection 1.9: Organizing and managing work, and tools that can assist\n\nThe RStudio Integrated Development Environment\n\n\nSubsection 1.9.1: Reproducible reporting — the knitr package\n\n\n\nSection 1.10: The changing environment for data analysis\n\nSubsection 1.10.1: Models and machine learning\n\nThe limits of current machine learning systems\n\n\nTraps in big data analysis\n\n\nOf mice and machine learning — missing data values\n\n\nHumans are not good intuitive statisticians\n\n\n\nSubsection 1.10.2: Replicability is the definitive check\n\nTo what extent is published work replicable? What is the evidence?\n\n\nSome major replication studies\n\n\nReplicability in pre-clinical cancer research\n\n\nStudies where there may be strong social influences\n\n\nThe scientific study of scientific processes\n\n\nWould lowering the \\(p\\)-value threshold help?\n\n\nPeer review at the study planning stage\n\n\n\n\nSection 1.11: Further, or supplementary, reading\n\n\nExercises (1_12)\n1.4\n\nAnimals &lt;- MASS::Animals\nmanyMals &lt;- rbind(Animals, sqrt(Animals), Animals^0.1, log(Animals))\nmanyMals$transgp &lt;- rep(c(\"Untransformed\", \"Square root transform\",\n  \"Power transform, lambda=0.1\", \"log transform\"),\nrep(nrow(Animals),4))\nmanyMals$transgp &lt;- with(manyMals, factor(transgp, levels=unique(transgp)))\nlattice::xyplot(brain~body|transgp, data=manyMals,\n  scales=list(relation='free'), layout=c(2,2))\n\n1.5\n\nwith(Animals, c(cor(brain,body), cor(brain,body, method=\"spearman\")))\nwith(Animals, c(cor(log(brain),log(body)),\n  cor(log(brain),log(body), method=\"spearman\")))\n\n1.9\n\nusableDF &lt;- DAAG::cuckoohosts[c(1:6,8),]\nnr &lt;- nrow(usableDF)\nwith(usableDF, {\n  plot(c(clength, hlength), c(cbreadth, hbreadth), col=rep(1:2,c(nr,nr)))\n  for(i in 1:nr)lines(c(clength[i], hlength[i]), c(cbreadth[i], hbreadth[i]))\n  text(hlength, hbreadth, abbreviate(rownames(usableDF),8), pos=c(2,4,2,1,2,4,2))\n})\n\n1.10\n\n## Take a random sample of 100 values from the normal distribution\nx &lt;- rnorm(100, mean=3, sd=5)\n(xbar &lt;- mean(x))\n## Plot, against `xbar`, the sum of squared deviations from `xbar`\nlsfun &lt;- function(xbar) apply(outer(x, xbar, \"-\")^2, 2, sum)\ncurve(lsfun, from=xbar-0.01, to=xbar+0.01)\n\n\nboxplot(avs, meds, horizontal=T)\n\n1.15\n\nx &lt;- rpois(7, 78.3)\nmean(x); var(x)\n\n1.16\n\nnvals100 &lt;- rnorm(100)\nheavytail &lt;- rt(100, df = 4)\nveryheavytail &lt;- rt(100, df = 2)\nboxplot(nvals100, heavytail, veryheavytail, horizontal=TRUE)\n\n1.19\n\nboxdists &lt;- function(n=1000, times=10){\n  df &lt;- data.frame(normal=rnorm(n*times), t=rt(n*times, 7),\n  sampnum &lt;- rep(1:times, rep(n,times)))\n  lattice::bwplot(sampnum ~ normal+t, data=df, outer=TRUE, xlab=\"\", \n                  horizontal=T)\n}\n\n1.20\n\na &lt;- 1\nform &lt;- ~rchisq(1000,1)^a+rchisq(1000,25)^a+rchisq(1000,500)^a\nlattice::qqmath(form, scales=list(relation=\"free\"), outer=TRUE)\n\n1.21\n\ny &lt;- rnorm(51)\nydep &lt;- y1[-1] + y1[-51]\nacf(y)      # acf plots `autocorrelation function'(see Chapter 6)\nacf(ydep)\n\n1.24\n\nptFun &lt;- function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1, lower.tail=FALSE)\nsimStat &lt;- function(eff=.4, N=10, nrep=200, FUN)\n    array(rnorm(n=N*nrep*length(eff), mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N) \npval &lt;- simStat(eff=.4, N=10, nrep=200, FUN=ptFun)\n# Suggest a power transform that makes the distribution more symmetric\ncar::powerTransform(pval)   # See Subsection 2.5.6\nlabx &lt;- c(0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.25)\nbwplot(~I(pval^0.2), scales=list(x=list(at=labx^0.2, labels=paste(labx))),\n       xlab=expression(\"P-value (scale is \"*p^{0.2}*\")\") )\n\n1.24a\n\npvalDF &lt;- subset(df200, effsize==0.4 & N==10)$stat\nplot(sort(pval^0.2), sort(pvalDF^0.2))\nabline(0,1)\n\n1.24c\n\n## Estimated effect sizes: Set `FUN=effFun` in the call to `eff2stat()`\neffFun &lt;- function(x,N)mean(x)/sd(x)   \n  # Try: `labx &lt;- ((-1):6)/2`; `at = log(labx)`; `v = log(labx)  \n## NB also, Bayes Factors: Set `FUN=BFfun` in the call to `eff2stat()`\nBFfun &lt;- function(x,N)BayesFactor::ttest.tstat(sqrt(N)*mean(x)/sd(x),\n                                               n1=N, simple=T)\n  # A few very large Bayes Factors are likely to dominate the plots\n\n1.27\n\n(degC &lt;- setNames(c(21,30,38,46),paste('rep',1:4)) ) \n\n1.27a\n\nradonC &lt;- tidyr::pivot_longer(MPV::radon, names_to='key', \n                              cols=names(degC), values_to='percent')\nradonC$temp &lt;- degC[radonC$key]\nlattice::xyplot(percent ~ temp|factor(diameter), data = radonC)\n\n\nmatplot(scale(t(MPV::radon[,-1])), type=\"l\", ylab=\"scaled residuals\")  \n\n1.27d\n\nradon.res &lt;- aggregate(percent ~ diameter, data = radonC, FUN = scale, \n    scale = FALSE)\n\n1.30\n\ndiamonds &lt;- ggplot2::diamonds\nwith(diamonds, plot(carat, price, pch=16, cex=0.25))\nwith(diamonds, smoothScatter(carat, price))\n\n\nt2bfInterval &lt;- function(t, n=10, rscale=\"medium\", mu=c(-.1,.1)){\n     null0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu,\n                                       rscale=rscale,simple=TRUE)\nalt0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu, rscale=rscale, \n                                 complement=TRUE, simple=TRUE)\nalt0/null0\n}\n\n\npval &lt;- c(0.05,0.01,0.001); nval &lt;- c(10,40,160)\nbfDF &lt;- expand.grid(p=pval, n=nval)\npcol &lt;- 1; ncol &lt;- 2; tcol &lt;- 3\nbfDF[,'t'] &lt;- apply(bfDF,1,function(x){qt(x[pcol]/2, df=x[ncol]-1,                                  lower.tail=FALSE)})\nother &lt;- apply(bfDF,1,function(x)\n    c(BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"medium\",\n                               simple=TRUE),\n      BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"wide\",\n                               simple=TRUE),\n## Now specify a null interval\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"medium\"),\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"wide\")\n  ))\nbfDF &lt;- setNames(cbind(bfDF, t(other)),\n    c('p','n','t','bf','bfInterval'))\n\n\ndf &lt;- data.frame(d = with(datasets::sleep, extra[group==2] - extra[group==1]))\nlibrary(statsr)\nBayesFactor::ttestBF(df$d, rscale=1/sqrt(2))   # Or, `rscale=\"medium\"`\n  # `rscale=\"medium\"` is the default\nbayes_inference(d, type='ht', data=df, statistic='mean', method='t', rscale=1/sqrt(2),\n                alternative='twosided', null=0, prior_family = \"JZS\")\n  # Set `rscale=1/sqrt(2)` (default is 1.0) \n  # as for BayesFactor; gives same BF\n# Compare with `prior_family = \"JUI\"` (`\"JZS\"` is the default), \n# with (if not supplied) default settings\nbayes_inference(d, type='ht', data=df, statistic='mean', method='t',\n                alternative='twosided', null=0, prior_family = \"JUI\")\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch1.R\")\n}",
    "crumbs": [
      "Book",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1: Learning from data</span>"
    ]
  },
  {
    "objectID": "ch2.html",
    "href": "ch2.html",
    "title": "2  Chapter 2: Generalizing from models",
    "section": "",
    "text": "Packages required (plus any dependencies)\nDAAG MASS qra investr HistData BHH2 xtable BayesFactor boot zoo boot MCMCpack,\nAdditionally, knitr and Hmisc are required in order to process the Rmd source file.\n\n\nSection 2.1 Model assumptions\n\nSubsection 2.1.1: Inferences are never assumption free\n\n\nSubsection 2.1.2: Has account been taken of all relevant effects?\n\n## Tabulate by Admit and Gender\nbyGender &lt;- 100*prop.table(margin.table(UCBAdmissions, margin=1:2), margin=2)\nround(byGender,1)\n\n\n## Admission rates, by department\npcAdmit &lt;- 100*prop.table(UCBAdmissions, margin=2:3)[\"Admitted\", , ]\nround(pcAdmit,1)\n\n\napplied &lt;- margin.table(UCBAdmissions, margin=2:3)\npcAdmit &lt;- 100*prop.table(UCBAdmissions, margin=2:3)[\"Admitted\", , ]\n      byGender &lt;- 100*prop.table(margin.table(UCBAdmissions,\n      margin=1:2), margin=2)\ndimnam &lt;- dimnames(UCBAdmissions)\nmfStats &lt;- data.frame(Admit=c(pcAdmit[1,],pcAdmit[2,]),\n  Applicants=c(applied[1,], applied[2,]),\n  mf=factor(rep(dimnam[['Gender']],c(6,6)),\n  levels=dimnam[['Gender']]), Department=rep(dimnam[[\"Dept\"]],2))\nxlim &lt;- c(0, max(mfStats$Admit)*1.025)\nylim &lt;- c(0, max(mfStats$Applicants)*1.075)\nplot(Applicants~Admit, data=mfStats, type=\"h\",lwd=2, xlim=xlim, ylim=ylim,\n     fg=\"gray\", cex.lab=1.2, col=rep(c(\"blue\",\"red\"),rep(6,2)),\n     xlab=\"UCB Admission rates (%), 1973\", ylab=\"Number of applicants\")\npcA &lt;- rbind(pcAdmit[1,], apply(pcAdmit,2, mean)+2, pcAdmit[2,], rep(NA,6))\npcA[2,3] &lt;- pcA[2,3]+1\nappA &lt;- rbind(applied[1,], apply(applied,2, mean)+80,\n              applied[2,], rep(NA,6))\ndeptNam &lt;- dimnam[[3]]\nfor(j in 1: ncol(appA)) lines(pcA[,j], appA[,j], col=\"gray\", lwd=0.8)\npoints(pcA[2,],appA[2,], pch=16, cex=1.1, col=\"white\")\ntext(pcA[2,],appA[2,],deptNam, cex=0.85)\n##\npar(xpd=TRUE)\ntext(byGender[1,1:2], rep(par()$usr[4],2)+0.5*strheight(\"^\"),\n     labels=c(\"^\",\"^\"), col=c(\"blue\",\"red\"),cex=1.2,srt=180)\ntext(byGender[1,], par()$usr[4]+1.4*strheight(\"A\"),\n     labels=paste(round(byGender[1,],1)),cex=0.85)\ntext(byGender[1,1:2]+c(-3.5,3.5), rep(par()$usr[4],2)+2.65*strheight(\"A\"),\n     labels=c(\"All males\",\"All females\"), pos=c(4,2), cex=1.2)\npar(xpd=FALSE)\nabline(h=200*(0:4),col=\"lightgray\",lty=\"dotted\")\nabline(v=20*(0:4),col=\"lightgray\",lty=\"dotted\")\nlegend(\"topleft\", col=c('blue','red'),lty=c(1,1), lwd=0.75, cex=0.9,\n       y.intersp=0.65, legend=c(\"Males\",\"Females\"),bty=\"n\")\n\n\n## Calculate totals, by department, of males & females applying\nmargin.table(UCBAdmissions, margin=2:3)\n\n\n\nSubsection 2.1.3: The limitations of models\n\n\nSubsection 2.1.4: Use the methodology that best suits the task in hand?\n\n\n\nSection 2.2: t-statistics, binomial proportions, and correlations\n\nSubsection 2.2.1: One- and two-sample t-tests\n\n\nSubsection 2.2.2: A two-sample comparison\n\nstats2 &lt;- sapply(DAAG::two65,\n                 function(x) c(av=mean(x), sd=sd(x), n=length(x)))\npooledsd &lt;- sqrt( sum(stats2['n',]*stats2['sd',]^2)/sum(stats2['n',]-1) )\nstats2 &lt;- setNames(c(as.vector(stats2), pooledsd),\n                   c('av1','sd1','n1','av2','sd2','n2','pooledsd'))\nprint(stats2, digits=4)\n\n\nwith(DAAG::two65, t.test(heated, ambient, var.equal=TRUE))\n\n\nWhen is pairing helpful?\n\ntitl &lt;- paste(\"Second versus first member, for each pair.  The first\",\n\"\\npanel is for the elastic band data. The second (from\",\n\"\\nDarwin) is for plants of the species Reseda lutea\")\noldpar &lt;- par(pty=\"s\")\non.exit(par(oldpar))\nDAAG::onesamp(dset = DAAG::pair65, x = \"ambient\", y = \"heated\",\n  xlab = \"Amount of stretch (ambient)\",\n  ylab = \"Amount of stretch (heated)\", fg='gray')\n## Data set mignonette holds the Darwin (1877) data on Reseda lutea.\n## Data were in 5 pots, holding 5,5,5,5,4 pairs of plants respectively.\nDAAG::onesamp(dset = DAAG::mignonette, x = \"self\", y = \"cross\",\n  xlab = \"Height of self-fertilised plant\", ylab =\n  \"Height of cross-fertilised plant\", dubious = 0, cex=0.7, fg='gray')\n\n\n\nWhat if the standard deviations are unequal?\n\n\n\nSubsection 2.2.3: The normal approximation to the binomial\n\n\nSubsection 2.2.4: The Pearson or product–moment correlation\n\n## Pearson correlation between `body` and `brain`: Animals\nAnimals &lt;- MASS::Animals\nrho &lt;- with(Animals, cor(body, brain))\n## Pearson correlation, after log transformation\nrhoLogged &lt;- with(log(Animals), cor(body, brain))\n## Spearman rank correlation\nrhoSpearman &lt;- with(Animals, cor(body, brain, method=\"spearman\"))\nc(Pearson=round(rho,2), \" Pearson:log values\"=round(rhoLogged,2),\n  Spearman=round(rhoSpearman,2))\n\n\n\n\nSection 2.3 Extra-binomial and extra-Poisson variation\n\nmaleDF &lt;- data.frame(number=0:12, freq=unname(qra::malesINfirst12[[\"freq\"]]))\nN &lt;- sum(maleDF$freq)\npihat &lt;- with(maleDF, weighted.mean(number, freq))/12\nprobBin &lt;- dbinom(0:12, size=12, prob=pihat)\nrbind(Frequency=setNames(maleDF$freq, nm=0:12),\n      binomialFit=setNames(probBin*N, nm=0:12),\n      rawResiduals = maleDF$freq-probBin*N,\n      SDbinomial=sqrt(probBin*(1-probBin)*N)) |&gt;\n  formatC(digits=2, format=\"fg\") |&gt; print(digits=2, quote=F, right=T)\n\n\nset.seed(29)\nrqres.plot(doBI, plot.type='all', type=\"QQ\", main=\"\"); box(col='white')\nmtext(side=3, line=0.5, \"A: Binomial model, Q-Q\", adj=0, cex=1.25)\nrqres.plot(doBI, plot.type='all', type=\"wp\", main=\"\"); box(col='white')\n## Plots C, D, E, F: Set object name; set`type=\"wp\" (C, E, F), or`\"QQ\"` (D)\nmtext(side=3, line=0.5, \"B: Binomial, worm plot 1\", adj=-0.05, cex=1.25)\nrqres.plot(doBI, plot.type='all', type=\"wp\", main=\"\"); box(col='white')\nmtext(side=3, line=0.5, \"C: Binomial, worm plot 2\", adj=-0.05, cex=1.25)\nrqres.plot(doBB, plot.type='all', type=\"QQ\", main=\"\", ylab=''); box(col='white')\nmtext(side=3, line=0.5, \"D: BB model, Q-Q\", adj=0, cex=1.25)\nrqres.plot(doBB, plot.type='all', type=\"wp\", main=\"\", ylab=''); box(col='white')\nmtext(side=3, line=0.5, \"E: BB, worm plot 1\", adj=0, cex=1.25)\nrqres.plot(doBB, plot.type='all', type=\"wp\", main=\"\", ylab=''); box(col='white')\nmtext(side=3, line=0.5, \"F: BB, worm plot 2\", adj=0, cex=1.25)\n\n\naicStat &lt;- AIC(doBI, doBB)\nrownames(aicStat) &lt;-\n  c(doBI=\"Binomial\", doBB=\"Betabinomial\")[rownames(aicStat)]\naicStat$dAIC &lt;- with(aicStat, round(AIC-AIC[1],1))\naicStat\n\n\n## Numbers of accidents in three months, with Poisson fit\nmachinists &lt;- data.frame(number=0:8, freq=c(296, 74, 26, 8, 4, 4, 1, 0, 1))\nN &lt;- sum(machinists[['freq']])\nlambda &lt;- with(machinists, weighted.mean(number, freq))\nfitPoisson &lt;- dpois(0:8, lambda)*sum(machinists[['freq']])\nrbind(Frequency=with(machinists, setNames(freq, number)),\n      poissonFit=fitPoisson) |&gt;\n  formatC(digits=2, format=\"fg\") |&gt; print(quote=F, digits=2, right=T)\n\n\nset.seed(23)\nrqres.plot(doPO, plot.type='all', type=\"QQ\", main=\"\"); box(col='white')\n## Repeat, changing the argument, for remaining plots\nmtext(side=3, line=0.5, \"A: Poisson, Q-Q plot\", adj=0, cex=1.25)\nrqres.plot(doPO, plot.type='all', type=\"wp\", main=\"\", ylab=''); box(col='white')\nmtext(side=3, line=0.5, \"B: Poisson, worm plot\", adj=0, cex=1.25)\nrqres.plot(doNBI, plot.type='all', type=\"wp\", main=\"\", ylab='')\nmtext(side=3, line=0.5, \"C: NBI, worm plot\", adj=0, cex=1.25); box(col='white')\n\n\n\nSubsection 2.3.2: *Technical details – extra-binomial or extra-Poisson variation\n\nsigma &lt;- exp(coef(doBB, \"sigma\"))\ncat(\"Phi =\", (1+12*sigma)/(1+sigma))\n\n\nmu &lt;- exp(coef(doNBI, \"mu\"))\nsigma &lt;- exp(coef(doNBI, \"sigma\"))\ncat(\"Phi =\", (1+sigma*mu))\n\n\n\nSection 2.4 Contingency tables\n\n## 'Untreated' rows (no training) from psid3, 'treated' rows from nswdemo\nnswpsid3 &lt;- rbind(DAAG::psid3, subset(DAAG::nswdemo, trt==1))\ndegTAB &lt;- with(nswpsid3, table(trt,nodeg))\n# Code 'Yes' if completed high school; 'No' if dropout\ndimnames(degTAB) &lt;- list(trt=c(\"PSID3_males\",\"NSW_male_trainees\"),\n                         deg =c(\"Yes\",\"No\"))\ndegTAB\n\n\n# To agree with hand calculation below, specify correct=FALSE\nchisq.test(degTAB, correct=FALSE)\n\n\nThe mechanics of the chi-squared test\n\n\nAn example where a chi-squared test may not be valid\n\n## Engine man data\nengineman &lt;- matrix(c(5,3,17,85), 2,2)\nchisq.test(engineman)\n\n\n\nRare and endangered plant species\n\nfisher.test(engineman)\n\n\n## Enter the data thus:\nrareplants &lt;- matrix(c(37,190,94,23, 59,23,10,141, 28,15,58,16), ncol=3,\n  byrow=TRUE, dimnames=list(c(\"CC\",\"CR\",\"RC\",\"RR\"), c(\"D\",\"W\",\"WD\")))\n\n\n(x2 &lt;- chisq.test(rareplants))\n\n\n\nExamination of departures from a consistent overall row pattern\n\n## Expected values\nx2$expected\n\n\noptions(digits=2)\n## Standardized residuals\nresiduals(x2)\n\n\n\nInterpretation issues\n\n\n\nSection 2.5 Issues for Regression with a single explanatory variable\n\nSubsection 2.5.1: Iron slag example — check residuals with care!\n\nleg &lt;- c(\"A: Fitted line\", \"B: Residuals from line\", \"C: Variance check\")\nord &lt;- order(DAAG::ironslag[[\"magnetic\"]])\nironslag &lt;- DAAG::ironslag[ord,]\nslagAlpha.lm &lt;- lm(chemical~magnetic, data=ironslag)\nresval &lt;- residuals(slagAlpha.lm)\nfitchem &lt;- fitted(slagAlpha.lm)\nsqrtabs2 &lt;- sqrt(abs(resval))\nplot(chemical~magnetic, xlab = \"Magnetic\", ylab = \"Chemical\",\n     pch = 1, data=ironslag, fg=\"gray\")\nlines(fitchem~ironslag[[\"magnetic\"]])\nmtext(side = 3, line = 0.25, leg[1], adj=-0.1, cex=0.925)\nscatter.smooth(resval~ironslag[[\"magnetic\"]], lpars=list(col=\"red\"), span=0.8,\n               xlab = \"Magnetic\", ylab = \"Residual\", fg=\"gray\")\nmtext(side = 3, line = 0.25, leg[2], adj = -0.1, cex=0.925)\nscatter.smooth(sqrtabs2 ~ fitchem, lpars=list(col=\"red\"), span=0.8,\nxlab = \"Predicted chemical\", fg=\"gray\",\nylab = expression(sqrt(abs(residual))))\nmtext(side = 3, line = 0.25, leg[3], adj = -0.1, cex=0.8)\n## Diagnostics from fit using loess()\nleg2 &lt;- c(\"D: Smooth, using loess()\",\n          \"E: Residuals from smooth\",\n          \"F: Variance check\")\nslag.loess &lt;- loess(chemical~magnetic, data=ironslag, span=0.8)\nresval2 &lt;- slag.loess[[\"residuals\"]]\nfitchem2 &lt;- slag.loess[[\"fitted\"]]\nsqrtabs2 &lt;- sqrt(abs(resval2))\nplot(chemical~magnetic, xlab = \"Magnetic\", ylab = \"Chemical\",\npch = 1, data=ironslag, fg=\"gray\")\nlines(fitchem2 ~ ironslag[[\"magnetic\"]], col=\"red\")\nmtext(side = 3, line = 0.25, leg2[1], adj=-0.1, cex=0.925)\nscatter.smooth(resval2~ironslag[[\"magnetic\"]], span=0.8,\nlpars=list(col=\"red\"),\nxlab = \"Magnetic\", ylab = \"Residual\", fg=\"gray\")\nmtext(side = 3, line = 0.25, leg2[2], adj = -0.1, cex=0.925)\nscatter.smooth(sqrtabs2 ~ fitchem2, lpars=list(col=\"red\"),\nspan=0.8, xlab = \"Predicted chemical\", fg=\"gray\",\nylab = expression(sqrt(abs(residual))))\nmtext(side = 3, line = 0.25, leg2[3], adj = -0.1, cex=0.925)\n\n\n\nSubsection 2.5.2: The analysis of variance table\n\nroller.lm &lt;- lm(depression ~ weight, data=DAAG::roller)\nanova(roller.lm)\n\n\n\nSubsection 2.5.3: Outliers, influence, and robust regression\n\nsoftbacks &lt;- DAAG::softbacks\nx &lt;- softbacks[,\"volume\"]\ny &lt;- softbacks[,\"weight\"]\nu &lt;- lm(y ~ x)\nyhat &lt;- predict(u)\nres &lt;- resid(u)\nr &lt;- with(softbacks, cor(x, y))\nxlim &lt;- with(softbacks, range(volume))\nxlim[2] &lt;- xlim[2]+diff(xlim)*0.08\nplot(y ~ x, xlab = \"Volume (cc)\", xlim=xlim,\ndata=softbacks, ylab = \"Weight (g)\", pch = 4,\nylim = range(c(y, yhat)), cex.lab=0.9, fg=\"gray\")\nabline(u$coef[1], u$coef[2], lty = 1)\nbottomright &lt;- par()$usr[c(2, 3)]\nchw &lt;- par()$cxy[1]\nchh &lt;- par()$cxy[2]\nz &lt;- summary(u)$coef\nbtxt &lt;- c(paste(\"a =\", format(round(z[1, 1], 1)),\n\"  SE =\", format(round(z[1, 2], 1))),\npaste(\"b =\", format(round(z[2, 1], 2)),\n\"  SE =\", format(round(z[2, 2], 2))))\nlegend(bottomright[1],  bottomright[2],\nlegend=btxt, xjust=1, yjust=0, cex=0.8, bty=\"n\")\n\n\nsoftbacks.lm &lt;- lm(weight ~ volume, data=DAAG::softbacks)\nprint(coef(summary(softbacks.lm)), digits=3)\n\n\nplot(softbacks.lm, fg=\"gray\",\ncaption = c(\"A: Residuals vs Fitted\", \"B: Normal Q-Q\",\n\"C: Scale-Location\", \"\", \"D: Resids vs Leverage\"))\n\n\nRobust regression\n\n\n\nSubsection 2.5.4: Standard errors and confidence intervals\n\nConfidence intervals and tests for the slope\n\nSEb &lt;- coef(summary(roller.lm))[2, 2]\ncoef(roller.lm)[2] + qt(c(0.025,.975), 8)*SEb\n\n\n\nSEs and confidence intervals for predicted values\n\n## Code to obtain fitted values and standard errors (SE, then SE.OBS)\nfit.with.se &lt;- predict(roller.lm, se.fit=TRUE)\nfit.with.se$se.fit                                            # SE\nsqrt(fit.with.se[[\"se.fit\"]]^2+fit.with.se$residual.scale^2)  # SE.OBS\n\n\npredict(roller.lm, interval=\"confidence\", level=0.95)\npredict(roller.lm, interval=\"prediction\", level=0.95)  # CI for a new observation\n\n\n## Depression vs weight, with 95\\% pointwise bounds for both\n## the fitted line and predicted values\ninvestr::plotFit(roller.lm, interval=\"both\", col.conf=\"red\", fg=\"gray\")\nmtext(side=3,line=0.75, \"A: Lawn roller data\", cex=1.2, adj=-0.25)\n## Male child vs father height, Galton's data\ngaltonMales &lt;- subset(HistData::GaltonFamilies, gender==\"male\")\ngalton.lm &lt;- lm(childHeight~father, data=galtonMales)\ninvestr::plotFit(galton.lm, interval=\"both\", col.conf=\"red\", hide=FALSE,\n                 col=adjustcolor('black',alpha=0.5), fg=\"gray\")\nmtext(side=3,line=0.75, \"B: Son vs father heights\", cex=1.2, adj=-0.25)\n\n\n\nImplications for design\n\npanelci&lt;-function(data,...)\n{\nnrows&lt;-list(...)$nrows\nncols&lt;-list(...)$ncols\nif(ncols==1)axis(2, lwd=0, lwd.ticks=1)\nif(ncols==1)axis(1, lwd=0, lwd.ticks=1) else\naxis(3, lwd=0, lwd.ticks=1)\nx&lt;-data$stretch; y&lt;-data$distance\nu &lt;- lm(y ~ x)\nupred &lt;- predict(u, interval=\"confidence\")\nci &lt;- data.frame(fit=upred[,\"fit\"],lower=upred[,\"lwr\"], upper=upred[,\"upr\"])\nord&lt;-order(x)\nlines(x[ord], ci[[\"fit\"]][ord], lty=1, lwd=2)\nlines(lowess(x[ord], ci[[\"upper\"]][ord]), lty=2, lwd=2, col=\"grey\")\nlines(lowess(x[ord], ci[[\"lower\"]][ord]), lty=2, lwd=2, col=\"grey\")\n}\nelastic1 &lt;- DAAG::elastic1\nelastic2 &lt;- DAAG::elastic2\nxy&lt;-rbind(elastic2,elastic1)\nnam &lt;- c(\"Range of stretch 30-65 mm\",\"Range of stretch 42-54 mm\")\ntrial&lt;-rep(nam, c(dim(elastic2)[1],dim(elastic1)[1]))\nxlim&lt;-range(elastic2$stretch)\nylim&lt;-range(elastic2$distance)\nxy&lt;-split(xy,trial)\nxy&lt;-lapply(1:length(xy),function(i){c(as.list(xy[[i]]), list(xlim=xlim,\nylim=ylim))})\nnames(xy) &lt;- nam\nDAAG::panelplot(xy,panel=panelci,totrows=1,totcols=2,\n                par.strip.text=list(cex=.9), oma=c(4,4,2.5,2), fg='gray')\nmtext(side = 2, line = 3.35, \"Distance moved (cm)\", cex=1.1, las=0)\nmtext(side=1,line=3,\"Amount of stretch (mm)\", cex=1.1)\n\n\n\n\nSubsection 2.5.5: There are two regression lines!\n\n## There are two regression lines!\npair65 &lt;- DAAG::pair65\nbothregs &lt;- function(x=pair65[, \"ambient\"], y=pair65[, \"heated\"],\n  xlab=\"Stretch (band at ambient)\", ylab = \"Stretch (heated band)\", pch=16){\n    plot(y ~ x, xlab = xlab, ylab = ylab, pch = pch, fg=\"gray\")\n    topleft &lt;- par()$usr[c(1, 4)] + c(0.5, -0.5) * par()$cxy\n    text(topleft[1], topleft[2], paste(\"r =\", round(cor(x, y), 2)), adj = 0)\n    u1 &lt;- lm(y ~ x)\n    abline(u1$coef[1], u1$coef[2])\n    u2 &lt;- lm(x ~ y)\n    abline( - coef(u2)[1]/coef(u2)[2], 1/coef(u2)[2], lty = 2)\n}\nbothregs()\nmtext(side = 3, line = 0.5, \"A\", adj = 0)\nbothregs(x=trees[, \"Girth\"], y=trees[, \"Height\"],\n         xlab=\"Girth (in)\", ylab &lt;- \"Height (ft)\", pch=16)\nmtext(side = 3, line = 0.5, \"B\", adj = 0)\n\n\nAn alternative to a regression line\n\n\n\nSubsection 2.5.6: Logarithmic and Power Transformations\n\n## Logarithmic and Power Transformations\nDAAG::powerplot(expr=\"sqrt(x)\", xlab=\"\")\nDAAG::powerplot(expr=\"x^0.25\", xlab=\"\", ylab=\"\")\nDAAG::powerplot(expr=\"log(x)\", xlab=\"\", ylab=\"\")\nDAAG::powerplot(expr=\"x^2\")\nDAAG::powerplot(expr=\"x^4\", ylab=\"\")\nDAAG::powerplot(expr=\"exp(x)\", ylab=\"\")\n\n\nGeneral power transformations — Box-Cox and Yeo-Johnson\n\n\n\nSubsection 2.5.7: General forms of nonlinear response\n\n\nSubsection 2.5.8: Size and shape data – allometric growth\n\n## Heart weight versus body weight, for 30 Cape fur seals.\ng2.12 &lt;- function()\n{\ncfseal &lt;- DAAG::cfseal\nx &lt;- log(cfseal[,\"weight\"])\ny &lt;- log(cfseal[, \"heart\"])\nylim &lt;- log(c(82.5,1100))\nxlim &lt;- log(c(17,180))\nylab &lt;- \"Heart weight (g, log scale)\"\nxlab &lt;- \"Body weight (kg, log scale)\"\nxtik &lt;- c(20,40,80,160)\nytik &lt;- c(100,200,400,800)\nplot(x, y, xlab = xlab, ylab = ylab, axes = F, xlim =\nxlim, ylim = ylim, pch = 16, cex=0.85, fg=\"gray\", cex.lab=1.1)\naxis(1, at = log(xtik), labels = paste(xtik), lwd=0, lwd.ticks=1)\naxis(2, at = log(ytik), labels = paste(ytik), lwd=0, lwd.ticks=1)\nbox(col=\"gray\")\nform1 &lt;- formula(y ~ x)\nu &lt;- lm(form1, data = cfseal)\nabline(u$coef[1], u$coef[2])\nusum &lt;- summary(u)$coef\noptions(digits=3)\nprint(usum)\ncwh &lt;- par()$cxy\neqn &lt;- paste(\"log y =\", round(usum[1, 1], 2), \" [\",\nround(usum[1, 2], 2), \"] +\", round(usum[2, 1], 3),\n\" [\", round(usum[2, 2], 3), \"] log x\")\nmtext(side=3, line=1.15, eqn, adj = 0.4, cex = 0.8)\nmtext(side=3, line=0.25, \"(Values in square brackets are SEs)\", adj = 0.4, cex = 0.8)\n}\ng2.12()\n\n\nThe allometric growth equation\n\noptions(scipen=4)\ncfseal.lm &lt;- lm(log(heart) ~ log(weight), data=DAAG::cfseal)\nprint(coef(summary(cfseal.lm)), digits=4)\n\n\n\n\n\nSection 2.6 Empirical assessment of predictive accuracy\n\nSubsection 2.6.1: The training/test approach, and cross-validation\n\nCross-validation – a tutorial example\n\nhouseprices &lt;- DAAG::houseprices\ndf &lt;- DAAG::CVlm(houseprices, form.lm = formula(sale.price ~ area),m=3,printit=F,plotit=FALSE)\npanelfun &lt;- function(x,y,subscripts,groups, ...){\n  lattice::panel.superpose(x,y,subscripts,groups, ...)\n  lattice::panel.superpose(x,df[[\"cvpred\"]],subscripts,groups,type=\"b\", cex=0.5, ...)\n}\ngph &lt;- lattice::xyplot(sale.price ~ area, groups=fold, data=df, pch=1:3, panel=panelfun)\nparset &lt;- DAAG::DAAGtheme(color=T, lty=1:3, pch=1:3, lwd=2)\nkeylist &lt;- list(lines=TRUE, columns=3, between.columns=1.5, between=1, cex=0.85)\nupdate(gph, par.settings=parset, auto.key=keylist)\n\n\nset.seed(29)        # Generate results shown\nrand &lt;- sample(rep(1:3, length=15))\n## sample() randomly permutes the vector of values 1:3\nfor(i in 1:3) cat(paste0(i,\":\"), (1:15)[rand == i],\"\\n\")\n\n\nhouseprices &lt;- DAAG::houseprices\nrow.names(houseprices) &lt;- (1:nrow(houseprices))\nDAAG::CVlm(houseprices, form.lm = formula(sale.price ~ area), plotit=FALSE)\n\n\n## Estimate of sigma^2 from regression output\nhouseprices &lt;- DAAG::houseprices\nhouseprices.lm &lt;- lm(sale.price ~ area, houseprices)\nsummary(houseprices.lm)[[\"sigma\"]]^2\n\n\n\n\nSubsection 2.6.2: Bootstrapping in regression\n\nhouseprices &lt;- DAAG::houseprices\nhouseprices.lm &lt;- lm(sale.price ~ area, houseprices)\nprint(coef(summary(houseprices.lm)),digits=2)\n\n\nhouseprices.fn &lt;-\n  function (houseprices, index,\n            statfun=function(obj)coef(obj)[2]){\n            house.resample &lt;- houseprices[index, ]\n            house.lm &lt;- lm(sale.price ~ area, data=house.resample)\n            statfun(house.lm)    # slope estimate for resampled data\n            }\n\n\nset.seed(1028)     # use to replicate the exact results below\nlibrary(boot)      # ensure that the boot package is loaded\n## requires the data frame houseprices (DAAG)\n(houseprices.boot &lt;- boot(houseprices, R=999, statistic=houseprices.fn))\n\n\nstatfun1200 &lt;- function(obj)predict(obj, newdata=data.frame(area=1200))\nprice1200.boot &lt;- boot(houseprices, R=999, statistic=houseprices.fn,\nstatfun=statfun1200)\nboot.ci(price1200.boot, type=\"perc\") # \"basic\" is an alternative to \"perc\"\n\n\nset.seed(1111)\nlibrary(boot)\npar(las=0)\nhouseprices2.fn&lt;-function (houseprices,index){\nhouse.resample&lt;-houseprices[index,]\nhouse.lm&lt;-lm(sale.price~area,data=house.resample)\nhouseprices$sale.price-predict(house.lm,houseprices)\n# resampled prediction errors\n}\nhouseprices &lt;- DAAG::houseprices\nn&lt;-nrow(houseprices)\nR &lt;- 199    ## Will obtain 199 estimates of prediction error\nhouseprices.lm&lt;-lm(sale.price~area,data=houseprices)\nhouseprices2.boot&lt;-boot(houseprices, R=R, statistic=houseprices2.fn)\nhouse.fac&lt;-factor(rep(1:n,rep(R,n)))\nplot(house.fac,as.vector(houseprices2.boot$t),\n     ylab=\"\", xlab=\"House\", fg=\"gray\")\nmtext(side=2, line=2, \"Prediction Errors\")\nmtext(side = 3, line = 0.5, \"A\", adj = 0)\nboot.se &lt;- apply(houseprices2.boot$t,2,sd)\nmodel.se &lt;- predict.lm(houseprices.lm,se.fit=T)$se.fit\nplot(boot.se/model.se, ylab=\"\", xlab=\"House\",pch=16, fg=\"gray\")\nmtext(side=2, line=2.0, \"Ratio of SEs\\nBootstrap to Model-Based\", cex=0.9)\nmtext(side = 3, line = 0.5, \"B\", adj = 0)\nabline(1,0)\n\n\nCommentary\n\n\n\n\nSection 2.7 One- and two-way comparisons\n\nSubsection 2.7.1: One-way comparisons\n\ntomato &lt;- data.frame(Weight = c(1.5, 1.9, 1.3, 1.5, 2.4, 1.5,   # Water\n                                1.5, 1.2, 1.2, 2.1, 2.9, 1.6,   # Nutrient\n                                1.9, 1.6, 0.8, 1.15, 0.9, 1.6), # Nutrient+24D\n  trt = factor(rep(c(\"Water\", \"Nutrient\", \"Nutrient+24D\"), c(6, 6, 6))))\n## Make `Water` the first level of trt.  In aov or lm calculations, it is\n## then taken as the baseline or reference level.\ntomato$trt &lt;- relevel(tomato$trt, ref=\"Water\")\n\n\n## A: Weights of tomato plants (g)\nlibrary(lattice, quietly=TRUE)\ngph &lt;- stripplot(trt~Weight, aspect=0.35, scale=list(tck=0.6), data=tomato)\nupdate(gph, scales=list(tck=0.4), cex=0.9, col=\"black\", xlab=\"\",\n       main=list('A: Weights of tomato plants (g)', y=0, cex=1.1))\n\n\n## B: Summarize comparison between LSD and Tukey's HSD graphically\ntomato.aov &lt;- aov(Weight ~ trt, data=tomato)\nDAAG::onewayPlot(obj=tomato.aov)\ntitle(main=\"B: LSD, compared with Tukey HSD\", adj=0.1, outer=T,\n      line=-1.0, font.main=1, cex.main=1.25)\n\n\nBHH2::anovaPlot(tomato.aov)\n\n\nThe analysis of variance table\n\n## Do analysis of variance calculations\nanova(tomato.aov)\n\n\n\nOther multiple comparison tests\n\n\n\nSubsection 2.7.2: Regression versus qualitative comparisons – issues of power\n\ngph &lt;- DAAG::simulateLinear(alpha=0.6, seed=17, aspect='iso')\nupdate(gph, par.settings=DAAG::DAAGtheme(color=FALSE, alpha=0.4))\n\n\n\nSubsection 2.7.3: *Severe multiplicity — the false discovery rate\n\n*Microarrays and alternatives — technical note\n\n\nThe false discovery rate (FDR)\n\ncoralPval &lt;- DAAG::coralPval\npcrit &lt;- c(0.05, 0.02, 0.01, 0.001)\nunder &lt;- sapply(pcrit, function(x)sum(coralPval&lt;=x))\n\n\nexpected &lt;- pcrit*length(coralPval)\n\n\nfdrtab &lt;- data.frame(Threshold=pcrit, Expected=expected,\nDiscoveries=under, FDR=round(expected/under, 4))\nprint(xtable::xtable(fdrtab), include.rownames=FALSE, hline.after=FALSE)\n\n\nfdr &lt;- p.adjust(coralPval, method=\"BH\")\n\n\nfdrcrit &lt;- c(0.05, 0.04, 0.02, 0.01)\nunder &lt;- sapply(fdrcrit, function(x)sum(coralPval&lt;=x))\nsetNames(under, paste(fdrcrit))\n\n\n\n\nSubsection 2.7.4: Data with a two-way structure, i.e., two factors\n\npar(fig=c(0.525,1,0,1), mgp=c(1.5,0.4,0))\nlev &lt;- c(\"F10\", \"NH4Cl\", \"NH4NO3\", \"F10 +ANU843\",\n         \"NH4Cl +ANU843\", \"NH4NO3 +ANU843\")\nrice &lt;- within(DAAG::rice, trt &lt;- factor(trt, levels=lev))\nwith(rice, interaction.plot(fert, variety, ShootDryMass, fg=\"gray\",\n     legend = FALSE, xlab=\"Fertiliser\", cex.lab=0.95, mex=0.65))\nxleg &lt;- par()$usr[2]\nyleg &lt;- par()$usr[4] - 0.72 * diff(par()$usr[3:4])\nleginfo &lt;- legend(xleg, yleg, bty = \"n\", legend = levels(rice$variety),\n                  col = 1, lty = 2:1, lwd=1, xjust = 1, cex = 0.8,\n                  y.intersp=0.8)$rect\ntext(leginfo$left + 0.5 * leginfo$w, leginfo$top, \"  variety\",\n      adj = 0.5, cex = 0.8)\nmtext(side=3, line=0.65, cex=0.9, adj=-0.15, \"B\")\ngph &lt;- dotplot(trt ~ ShootDryMass, pch=1, cex=0.9, las=2,\n               xlab=\"Shoot dry mass (g)\", data=rice,\n               panel=function(x,y,...){panel.dotplot(x,y,...)\n                 av &lt;- sapply(split(x,y),mean);\n                 ypos &lt;- unique(y)\n                 lpoints(ypos~av, pch=3, col=\"gray40\", cex=1.25)},\n               main=list(\"A\", cex=0.88, just=\"left\", x=0.1, y=-0.7, font=1))\npars &lt;-  DAAG::DAAGtheme(fontsize=list(text=9, points=6), color=FALSE)\nprint(update(gph, scales=list(tck=0.5), par.settings=pars, aspect=0.9),\n      position=c(-0.065,0.0,0.6,1), newpage=FALSE)\n\n\n\nSubsection 2.7.5: Presentation issues\n\n\n\nSection 2.8 Data with a nested variation structure\n\nSubsection 2.8.1: Degrees of freedom considerations\n\n\nSubsection 2.8.2: General multi-way analysis of variance designs\n\n\n\nSection 2.9 Bayesian estimation – further commentary and approaches\n\nSubsection 2.9.1: Bayesian estimation with normal priors and likelihood\n\n\nSubsection 2.9.2: Further comments on Bayes Factors\n\nThe Sellke calibration upper limit\n\n\nA note on the Bayesian Information Criterion\n\npval &lt;- c(.05,.01,.001); np &lt;- length(pval)\nNval &lt;- c(4,6,10,20,40,80,160); nlen &lt;- length(Nval)\n## Difference in BIC statistics, interpreted as Bayes factor\nt2BFbic &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n                         exp((N*log(1+t^2/(N-1))-log(N))/2)}\nbicVal &lt;- outer(pval, Nval, t2BFbic)\n## Bayes factor, calculated using BayesFactor::ttest.tstat()\nt2BF &lt;- function(p, N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n          BayesFactor::ttest.tstat(t=t, n1=N, simple=TRUE, rscale = \"medium\")}\nBFval &lt;- matrix(nrow=np, ncol=nlen)\nfor(i in 1:np)for(j in 1:nlen) BFval[i,j] &lt;- t2BF(pval[i], Nval[j])\ncfVal &lt;- rbind(BFval, bicVal)[c(1,4,2,5,3,6),]\ndimnames(cfVal) &lt;- list(\n  paste(rep(pval,rep(2,np)), rep(c(\"- from ttest.tstat\", \"- from BIC\"),np)),\n        paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nround(cfVal,1)\n\n\n\n\nSubsection 2.9.3: Bayesian regression estimation using the MCMCpack package\n\nsuppressPackageStartupMessages(library(MCMCpack))\nroller.mcmc &lt;- MCMCregress(depression ~ weight, data=DAAG::roller)\nsummary(roller.mcmc)\n\n\nmat &lt;- matrix(c(1:6), byrow=TRUE, ncol=2)\nlayout(mat, widths=rep(c(2,1.1),3), heights=rep(0.9,8))\n  # NB: widths & heights are relative\nplot(roller.mcmc, auto.layout=FALSE, ask=FALSE, col=\"gray\", fg=\"gray\")\n\n\n\n\nSection 2.10: Recap\n\nRegress \\(y\\) on \\(x\\), or \\(x\\) on~\\(y\\)?\n\n\n\nSection 2.11: Further reading\n\n\nExercises (2.12)\n2.2\n\n## UCBAdmissions is in the datasets package\n## For each combination of margins 1 and 2, calculate the sum\nUCBtotal &lt;- apply(UCBAdmissions, c(1,2), sum)\n\n2.2b\n\napply(UCBAdmissions, 3, function(x)(x[1,1]*x[2,2])/(x[1,2]*x[2,1]))\n\n2.3\n\ntabA &lt;- array(c(30,30,10,10,15,5,30,10), dim=c(2,2,2))\ntabB &lt;- array(c(30,30,20,10,10,5,20,25), dim=c(2,2,2))\n\n2.5\n\nz.transform &lt;- function(r) .5*log((1+r)/(1-r))\nz.inverse &lt;- function(z) (exp(2*z)-1)/(exp(2*z)+1)\n  possum.fun &lt;- function(data, indices) {\n    chest &lt;- data$chest[indices]\n    belly &lt;- data$belly[indices]\n    z.transform(cor(belly, chest))}\npossum.boot &lt;- boot::boot(DAAG::possum, possum.fun, R=999)\nz.inverse(boot.ci(possum.boot, type=\"perc\")$percent[4:5])\n # The 4th and 5th elements of the percent list element\n # hold the interval endpoints. See ?boot.ci\n\n2.11\n\nwith(pressure, MASS::boxcox(pressure ~ I(1/(temperature+273))))\n\n2.14\n\n\"funRel\" &lt;-\nfunction(x=leafshape$logpet, y=leafshape$loglen, scale=c(1,1)){\n  ## Find principal components rotation; see Subsection 9.1.2\n  ## Here (unlike 9.1.2) the interest is in the final component\n  xy.prc &lt;- prcomp(cbind(x,y), scale=scale)\n  b &lt;- xy.prc$rotation[,2]/scale\n  c(bxy = -b[1]/b[2])     # slope of functional equation line\n}\n## Try the following:\nleafshape &lt;- DAAG::leafshape\nfunRel(scale=c(1,1))    # Take x and y errors as equally important\n  # Note that all lines pass through (mean(x), mean(y))\n\n2.15\n\nP &lt;- rbind(\n    c(1 , 0 , 0 , 0 , 0 , 0),\n    c(.5, 0 , .5, 0 , 0 , 0),\n    c(0 , .5, 0 , .5, 0 , 0),\n    c(0 , 0 , .5, 0 , .5, 0),\n    c(0 , 0 , 0 , .5, 0 , .5),\n    c(0 , 0 , 0 , 0 , 0 , 1))\ndimnames(P) &lt;- list(0:5,0:5)\nP\n\n\nMarkov &lt;- function(N=15, initial.value=1, transition=P, stopval=NULL)\n  {X &lt;- numeric(N)\n   X[1] &lt;- initial.value + 1  # States 0:(n-1); subscripts 1:n\n   n &lt;- nrow(transition)\n   for (i in 2:N){\n    X[i] &lt;- sample(1:n, size=1, prob=transition[X[i-1], ])\n    if(length(stopval)&gt;0)if(X[i] %in% (stopval+1)){X &lt;- X[1:i]; break}}\n  X - 1\n}\n # Set `stopval=c(0,5)` to stop when  the player's fortune is $0 or $5\n\n2.16\n\nPb &lt;- rbind(\n  Sun = c(Sun=0.6, Cloud=0.2, Rain=0.2),\n  Cloud= c(0.2, 0.4, 0.4),\n  Rain= c(0.4, 0.3, 0.3))\nPb\n\n2.16b\n\nplotmarkov &lt;-\n  function(n=1000, width=101, start=0, transition=Pb, npanels=5){\n    xc2 &lt;- Markov(n, initial.value=start, transition)\n    mav0 &lt;- zoo::rollmean(as.integer(xc2==0), k=width)\n    mav1 &lt;- zoo::rollmean(as.integer(xc2==1), k=width)\n    npanel &lt;- cut(1:length(mav0), breaks=seq(from=1, to=length(mav0),\n                  length=npanels+1), include.lowest=TRUE)\n    df &lt;- data.frame(av0=mav0, av1=mav1, x=1:length(mav0), gp=npanel)\n    print(xyplot(av0+av1 ~ x | gp, data=df, layout=c(1,npanels), type=\"l\",\n          par.strip.text=list(cex=0.65), auto.key=list(columns=2),\n          scales=list(x=list(relation=\"free\"))))\n}\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch2.R\")\n}",
    "crumbs": [
      "Book",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Generalizing from models</span>"
    ]
  },
  {
    "objectID": "ch3.html",
    "href": "ch3.html",
    "title": "3  Chapter 3: Multiple linear regression",
    "section": "",
    "text": "Packages required (plus any dependencies)\nDAAG car MASS AICcmodavg leaps BayesFactor splines\nAdditionally, Hmisc and knitr are required in order to process the Rmd source file.\n\n\nSection 3.1 Basic ideas: the allbacks book weight data\n\nallbacks &lt;- DAAG::allbacks  # Place the data in the workspace\nallbacks.lm &lt;- lm(weight ~ volume+area, data=allbacks)\nprint(coef(summary(allbacks.lm)), digits=2)\n\n\nxlim &lt;- range(allbacks$volume)\nxlim &lt;- xlim+c(-.075,.075)*diff(xlim)\n## Plot of weight vs volume: data frame allbacks (DAAG)\nplot(weight ~ volume, data=allbacks, pch=c(16,1)[unclass(cover)],\nlwd=1.25, xlim=xlim, fg=\"gray\")\n## unclass(cover) gives the integer codes that identify levels\n## As text() does not accept the parameter data, use with()\n## to specify the data frame.\nwith(allbacks, text(weight ~ volume, labels=paste(1:15), cex=0.75, offset=0.35,\npos=c(2,4)[unclass(cover)]))\nlegend(x='topleft', pch=c(16,1), legend=c(\"hardback  \",\"softback\"),\nhoriz=T, bty=\"n\", xjust=0.5, x.intersp=0.75, )\n\n\n## Correlations between estimates -- model with intercept\nround(summary(allbacks.lm, corr=TRUE)$correlation, 3)\n\n\nout &lt;- capture.output(summary(allbacks.lm,digits=2))\ncat(out[15:17], sep='\\n')\n\n\n## 5% critical value; t-statistic with 12 d.f.\nqt(0.975, 12)\n\n\ncat(out[5:7], sep='\\n')\n\n\nSubsection 3.1.1: A sequential analysis of variance table\n\nanova(allbacks.lm)\n\n\nOmission of the intercept term\n\n## Show rows 1, 7, 8 and 15 only\nmodel.matrix(allbacks.lm)[c(1,7,8,15), ]\n## NB, also, code that returns the data frame used\nmodel.frame(allbacks.lm)\n\n\nallbacks.lm0 &lt;- lm(weight ~ -1+volume+area, data=allbacks)\nprint(coef(summary(allbacks.lm0)), digits=2)\n\n\n## Correlations between estimates -- no intercept\nprint(round(summary(allbacks.lm0, corr=TRUE)$correlation, 3))\n\n\n\n\nSubsection 3.1.2: Diagnostic plots\n\nallbacks.lm0 &lt;- lm(weight ~ -1+volume+area, data=allbacks)\nplot(allbacks.lm0, caption=c('A: Resids vs Fitted', 'B: Normal Q-Q',\n     'C: Scale-Location', '', 'D: Resids vs Leverage'), cex.caption=0.85,\n     fg='gray')\n\n\n## To show all plots in the one row, precede with\npar(mfrow=c(1,4))      # Follow with par(mfrow=c(1,1))\n\n\n## The following has the default captions\nplot(allbacks.lm0)\n\n\nallbacks.lm13 &lt;- lm(weight ~ -1+volume+area, data=allbacks[-13, ])\nprint(coef(summary(allbacks.lm13)), digits=2)\n\n\n\n\nSection 3.2 The interpretation of model coefficients\n\nSubsection 3.2.1: Times for Northern Irish hill races\n\noldpar &lt;- par(fg='gray20',col.axis='gray20',lwd=0.5,col.lab='gray20')\nnihr &lt;- within(DAAG::nihills, {mph &lt;- dist/time; gradient &lt;- climb/dist})\nnihr &lt;- nihr[, c(\"time\", \"dist\", \"climb\", \"gradient\", \"mph\")]\nvarLabs &lt;- c(\"\\ntime\\n(hours)\",\"\\ndist\\n(miles)\",\"\\nclimb\\n(feet)\",\n             \"\\ngradient\\n(ft/mi)\", \"\\nmph\\n(mph)\")\nsmoothPars &lt;- list(col.smooth='red', lty.smooth=2, lwd.smooth=0.5, spread=0)\ncar::spm(nihr, cex.labels=1.2, regLine=FALSE, col='blue',\n         oma=c(1.95,3,4, 3), gap=.25, var.labels=varLabs, smooth=smoothPars)\ntitle(main=\"A: Untransformed scales:\", outer=TRUE,\nadj=0, line=-1.0, cex.main=1, font.main=1)\n## Panel B: Repeat with log(nihills) in place of nihills,\n## and with variable labels suitably modified.\nvarLabs &lt;- c(\"\\ntime\\n(log h)\",\"\\ndist\\n(log miles)\", \"\\nclimb\\n(log feet)\",\n             \"\\ngradient\\n(log ft/mi)\", \"\\nmph\\n(log mph)\")\ncar::spm(log(nihr), regLine=FALSE, col=\"blue\", oma=c(1.95,2.5,4, 2.5),\n         gap=.25, var.labels=varLabs, smooth=smoothPars)\ntitle(\"B: Logarithmic scales\", outer=TRUE,\n      adj=0, line=-1.0, cex.main=1, font.main=1)\npar(oldpar)\n\n\nWhat is special about logarithmic transformations?\n\n\n\nSubsection 3.2.2: An equation that predicts dist/time\n\n##  Hold climb constant at mean on logarithmic scale\nmphClimb.lm &lt;- lm(mph ~ log(dist)+log(climb), data = nihr)\n## Hold `gradient=climb/dist` constant at mean on logarithmic scale\nmphGradient.lm &lt;- lm(mph ~ log(dist)+log(gradient), data = nihr)\navRate &lt;- mean(nihr$mph)\nbClimb &lt;- coef(mphClimb.lm)\nconstCl &lt;- c(bClimb[1]+bClimb[3]*mean(log(nihr$climb)), bClimb[2])\nbGradient &lt;- coef(mphGradient.lm)\nconstSl &lt;- c(bGradient[1]+bGradient[3]*mean((log(nihr$climb/nihr$dist))),\n             bGradient[2])\n# Use `dist` and `climb` as explanatory variables\ncoef(mphClimb.lm)\n# Use `dist` and `gradient` as explanatory variables\ncoef(mphGradient.lm)\n\n\nopar &lt;- par(mfrow=c(1,2), mgp=c(2.25,0.5,0), mar=c(3.6,4.1,2.1,1.6))\nlineCols &lt;- c(\"red\", adjustcolor(\"magenta\",0.4))\nyaxlab&lt;-substitute(paste(\"Minutes per mile (Add \", ym, \")\"), list(ym=round(avRate,2)))\ncar::crPlots(mphClimb.lm, terms = . ~ log(dist), xaxt='n',\n             xlab=\"Distance\", col.lines=lineCols,  ylab=yaxlab)\naxis(2, at=4:7, labels=paste(4:7))\nlabx &lt;- c(4,8,16,32)\naxis(1, at=log(2^(2:5)), labels=paste(2^(2:5)))\nbox(col=\"white\")\nmtext(\"A: Hold climb constant at mean value\", adj=0,\n      line=0.8, at=0.6, cex=1.15)\ncar::crPlots(mphGradient.lm, terms = . ~log(dist), xaxt='n',\n             xlab=\"Distance\", col.lines=lineCols, ylab=yaxlab)\naxis(1, at=log(2^(2:5)), labels=paste(2^(2:5)))\naxis(2, at=4:7, labels=paste(4:7))\nbox(col=\"white\")\nmtext(\"B: Hold log(gradient) constant at mean\", adj=0, line=0.8, at=0.6, cex=1.15)\npar(opar)\n\n\nsummary(mphClimb.lm, corr=T)$correlation[\"log(dist)\", \"log(climb)\"]\nsummary(mphGradient.lm, corr=T)$correlation[\"log(dist)\", \"log(gradient)\"]\n\n\n## Show the plots, with default captions\nplot(mphClimb.lm, fg='gray')\n\n\nplot(mphGradient.lm, caption=c('A: Resids vs Fitted', 'B: Normal Q-Q',\n'C: Scale-Location', '', 'D: Resids vs Leverage'),\ncex.caption=0.85, fg='gray')\n\n\n\nSubsection 3.2.3: Equations that predict log(time)\n\nlognihr &lt;- setNames(log(nihr), paste0(\"log\", names(nihr)))\ntimeClimb.lm &lt;- lm(logtime ~ logdist + logclimb, data = lognihr)\n\n\nprint(coef(summary(timeClimb.lm)), digits=2)\n\n\ntimeGradient.lm &lt;- lm(logtime ~ logdist + loggradient, data=lognihr)\nprint(coef(summary(timeGradient.lm)), digits=3)\n\n\n\nSubsection 3.2.4: Book dimensions — the oddbooks dataset\n\noldpar &lt;- par(fg='gray40',col.axis='gray20',lwd=0.5,col.lab='gray20')\n## Code for Panel A\noddbooks &lt;- DAAG::oddbooks\npairs(log(oddbooks), lower.panel=panel.smooth, upper.panel=panel.smooth,\n      labels=c(\"log(thick)\", \"log(breadth)\", \"log(height)\", \"log(weight)\"),\n      gap=0.25, oma=c(1.95,1.95,4, 1.95), col='blue')\ntitle(main=\"A: Columns from log(oddbooks)\",\n      outer=TRUE, adj=0, line=-1.0, cex.main=1.1, font.main=1)\n## Panel B\noddothers &lt;-\n  with(oddbooks, data.frame(density = weight/(breadth*height*thick),\narea = breadth*height, thick=thick, weight=weight))\npairs(log(oddothers), lower.panel=panel.smooth, upper.panel=panel.smooth,\nlabels=c(\"log(density)\", \"log(area)\", \"log(thick)\", \"log(weight)\"),\ngap=0.5, oma=c(1.95,1.95,4, 1.95), col='blue')\ntitle(\"B: Add density & area; omit breadth & height\",\nouter=TRUE, adj=0, line=-1.0, cex.main=1.1, font.main=1)\npar(oldpar)\n\n\nlob3.lm &lt;- lm(log(weight) ~ log(thick)+log(breadth)+log(height),\n              data=oddbooks)\n# coef(summary(lob3.lm))\n\n\nlob2.lm &lt;- lm(log(weight) ~ log(thick)+log(breadth), data=oddbooks)\ncoef(summary(lob2.lm))\n\n\nlob0.lm &lt;- lm(log(weight) ~ 1, data=oddbooks)\nadd1(lob0.lm, scope=~log(breadth) + log(thick) + log(height))\nlob1.lm &lt;- update(lob0.lm, formula=. ~ .+log(breadth))\n\n\nround(rbind(\"lob1.lm\"=predict(lob1.lm), \"lob2.lm\"=predict(lob2.lm),\n            \"lob3.lm\"=predict(lob3.lm)),2)\n\n\noddbooks &lt;- within(oddbooks, density &lt;- weight/(thick*breadth*height))\nlm(log(weight) ~ log(density), data=oddbooks) |&gt; summary() |&gt; coef() |&gt;\n  round(3)\n\n\n## Code that the reader may care to try\nlm(log(weight) ~ log(thick)+log(breadth)+log(height)+log(density),\n   data=oddbooks) |&gt; summary() |&gt; coef() |&gt; round(3)\n\n\n\nSubsection 3.2.5: Mouse brain weight example\n\noldpar &lt;- par(fg='gray40',col.axis='gray20',lwd=0.5,col.lab='gray20')\nlitters &lt;- DAAG::litters\npairs(litters, labels=c(\"lsize\\n\\n(litter size)\", \"bodywt\\n\\n(Body Weight)\",\n                        \"brainwt\\n\\n(Brain Weight)\"), gap=0.5, fg='gray',\n                        col=\"blue\", oma=rep(1.95,4))\npar(oldpar)\n\n\n## Regression of brainwt on lsize\nsummary(lm(brainwt ~ lsize, data = litters), digits=3)$coef\n## Regression of brainwt on lsize and bodywt\nsummary(lm(brainwt ~ lsize + bodywt, data = litters), digits=3)$coef\n\n\n\nSubsection 3.2.6: Issues for causal interpretation\n\nEffects of lifestyle on health\n\n\nThe studies mostly agree. But what do they say?\n\n\nAdjusting for confounders\n\n\n\n\nSection 3.3 Choosing the model, and checking it out\n\nEffects of lifestyle on health\n\n\nThe studies mostly agree. But what do they say?\n\n\nAdjusting for confounders\n\noddbooks.lm &lt;- lm((weight) ~ log(thick)+log(height)+log(breadth),\ndata=DAAG::oddbooks)\nyterms &lt;- predict(oddbooks.lm, type=\"terms\")\n\n\n\nSubsection 3.3.3: A more formal approach to the choice of transformation\n\n## Use car::powerTransform\nnihr &lt;- within(DAAG::nihills, {mph &lt;- dist/time; gradient &lt;- climb/dist})\nsummary(car::powerTransform(nihr[, c(\"dist\", \"gradient\")]), digits=3)\n\n\nform &lt;- mph ~ log(dist) + log(gradient)\nsummary(car::powerTransform(form, data=nihr))\n\n\nThe use of transformations — further comments\n\n\n\nSubsection 3.3.4: Accuracy estimates, fitted values and new observations\n\nlognihr &lt;- log(DAAG::nihills)\nnames(lognihr) &lt;- paste0(\"log\", names(lognihr))\ntimeClimb.lm &lt;- lm(logtime  ~ logdist + logclimb, data = lognihr)\n## Coverage intervals; use exp() to undo the log transformation\ncitimes &lt;- exp(predict(timeClimb.lm, interval=\"confidence\"))\n## Prediction intervals, i.e., for new observations\npitimes &lt;- exp(predict(timeClimb.lm, newdata=lognihr, interval=\"prediction\"))\n## fit ci:lwr ci:pwr pi:lwr pi:upr\nci_then_pi &lt;- cbind(citimes, pitimes[,2:3])\ncolnames(ci_then_pi) &lt;- paste0(c(\"\", rep(c(\"ci-\",\"pi-\"), c(2,2))),\n                               colnames(ci_then_pi))\n## First 4 rows\nprint(ci_then_pi[1:4,], digits=2)\n\n\ntimeClimb2.lm &lt;- update(timeClimb.lm, formula = . ~ . + I(logdist^2))\ng3.10 &lt;-\nfunction(model1=timeClimb.lm, model2=timeClimb2.lm)\n{\n## Panel A\ncitimes &lt;- predict(model1, interval=\"confidence\")\nord &lt;- order(citimes[,\"fit\"])\ncitimes &lt;- citimes[ord,]\nhat &lt;- citimes[,\"fit\"]\npitimes &lt;- predict(model1, newdata=lognihr, interval=\"prediction\")[ord,]\nlogobs &lt;- log(nihr[ord,\"time\"])\nxtiks &lt;- pretty(exp(hat))\nylim &lt;- range(c(pitimes[,\"lwr\"], pitimes[,\"upr\"], logobs)-rep(hat,3))\nlogytiks &lt;- pretty(ylim,5)\nytiks &lt;- round(exp(logytiks),2)\nxlim &lt;- range(hat)\nplot(hat, citimes[,\"lwr\"]-hat, type=\"n\", xlab = \"Time (fitted)\",\nylab = \"Difference from fit\",\nxlim=xlim, ylim = ylim, xaxt=\"n\", yaxt=\"n\", fg=\"gray\")\nmtext(side=3, line=0.75, adj=0, at=-2.0, \"A: CIs and PIs: Mean, prediction\")\nmtext(side=4, line=1.25, \"exp(Difference from fit)\", las=0)\naxis(1, at=log(xtiks), labels=paste(xtiks), lwd=0, lwd.ticks=1)\naxis(2, at=logytiks, las=1, lwd=0, lwd.ticks=1)\naxis(4, at=logytiks, labels=paste(ytiks), las=0, lwd=0, lwd.ticks=1)\npoints(hat, logobs-hat, pch=16, cex=0.65)\nlines(hat, citimes[,\"lwr\"]-hat, col = \"red\")\nlines(hat, citimes[,\"upr\"]-hat, col = \"red\")\nlines(hat, pitimes[,\"lwr\"]-hat, col = \"black\")\nlines(hat, pitimes[,\"upr\"]-hat, col = \"black\")\n## Panel B\ncitimes2 &lt;- predict(model2, interval=\"confidence\")[ord,]\nplot(hat, citimes[,\"lwr\"]-hat, type=\"n\", xlab = \"Time (fitted)\",\nylab = \"Difference from fit\",\nxlim=xlim, ylim = ylim, xaxt=\"n\", yaxt=\"n\", fg=\"gray\")\nmtext(side=3, line=0.75, adj=0, at=-2.0,\n\"B: CIs for fit, compare two models\")\nmtext(side=4, line=1.25, \"exp(Difference from fit)\", las=0)\naxis(1, at=log(xtiks), labels=paste(xtiks), lwd=0, lwd.ticks=1)\naxis(2, at=logytiks,las=1, lwd=0, lwd.ticks=1)\naxis(4, at=logytiks, labels=paste(ytiks), las=0,, lwd=0, lwd.ticks=1)\npoints(hat, logobs-hat, pch=16, cex=0.65)\nlines(hat, citimes[,\"lwr\"]-hat, col = \"red\")\nlines(hat, citimes[,\"upr\"]-hat, col = \"red\")\nhat2 &lt;- citimes2[,\"fit\"]\nlines(hat, citimes2[,\"lwr\"]-hat2, col = \"blue\", lty=2, lwd=1.5)\nlines(hat, citimes2[,\"upr\"]-hat2, col = \"blue\", lty=2, lwd=1.5)\n}\n\n\ntimeClimb2.lm &lt;- update(timeClimb.lm, formula = . ~ . + I(logdist^2))\n\n\n\nSubsection 3.3.5: Choosing the model — deaths from Atlantic hurricanes\n\noldpar &lt;- par(fg='gray20',col.axis='gray20',lwd=0.5,col.lab='gray20')\nhurric &lt;- DAAG::hurricNamed[,c(\"LF.PressureMB\", \"BaseDam2014\", \"deaths\")]\nthurric &lt;- car::powerTransform(hurric, family=\"yjPower\")\ntransY &lt;- car::yjPower(hurric, coef(thurric, round=TRUE))\nsmoothPars &lt;- list(col.smooth='red', lty.smooth=2, lwd.smooth=1, spread=0)\ncar::spm(transY, lwd=0.5, regLine=FALSE, oma=rep(2.5,4), gap=0.5,\n         col=\"blue\", smooth=smoothPars, cex.labels=1)\npar(oldpar)\n\n\nmodelform &lt;- deaths ~ log(BaseDam2014) + LF.PressureMB\npowerT &lt;- car::powerTransform(modelform, data=as.data.frame(hurric),\n                              family=\"yjPower\")\nsummary(powerT, digits=3)\n\n\ndeathP &lt;- with(hurric, car::yjPower(deaths, lambda=-0.2))\npower.lm &lt;- MASS::rlm(deathP ~ log(BaseDam2014) + LF.PressureMB, data=hurric)\nprint(coef(summary(power.lm)),digits=2)\n\n\n## Use (deaths+1)^(-0.2) as outcome variable\nplot(power.lm, cex.caption=0.85, fg=\"gray\",\n  caption=list('A: Resids vs Fitted', 'B: Normal Q-Q', 'C: Scale-Location', '',\n               'D: Resids vs Leverage'))\n\n\n\nSubsection 3.3.6: Strategies for fitting models — suggested steps\n\nDiagnostic checks\n\n\n\n\nSection 3.4 Robust regression, outliers, and influence\n\nSubsection 3.4.1: Making outliers obvious — robust regression\n\nhills2000 &lt;- DAAG::hills2000[,c(\"dist\", \"climb\", \"time\")]\nvarLabels &lt;- c(\"\\ndist\\n(log miles)\", \"\\nclimb\\n(log feet)\", \"\\ntime\\n(log hours)\")\nsmoothPars &lt;- list(col.smooth='red', lty.smooth=2, lwd.smooth=1, spread=0)\nhills2000 &lt;- DAAG::hills2000[,c(\"dist\", \"climb\", \"time\")]\nvarLabels &lt;- c(\"\\ndist\\n(log miles)\", \"\\nclimb\\n(log feet)\", \"\\ntime\\n(log hours)\")\ncar::spm(log(hills2000), smooth=smoothPars,  regLine=FALSE, cex.labels=1.5,\nvar.labels = varLabels, lwd=0.5, gap=0.5, oma=c(1.95,1.95,1.95,1.95))\n\n\n## Panel A\nlhills2k.lm &lt;- lm(log(time) ~ log(climb) + log(dist), data = hills2000)\nplot(lhills2k.lm, caption=\"\", which=1, fg=\"gray\", col=adjustcolor(\"black\", alpha=0.8))\nmtext(side=3, line=0.75, \"A: Least squares (lm) fit\", adj=0, cex=1.1)\n## Panel B\nlhills2k.lqs &lt;- MASS::lqs(log(time) ~ log(climb) + log(dist), data = hills2000)\nreres &lt;- residuals(lhills2k.lqs)\nrefit &lt;- fitted(lhills2k.lqs)\nbig3 &lt;- which(abs(reres) &gt;= sort(abs(reres), decreasing=TRUE)[3])\nplot(reres ~ refit, xlab=\"Fitted values (resistant fit)\",\nylab=\"Residuals (resistant fit)\", col=adjustcolor(\"black\", alpha=0.8), fg=\"gray\")\nlines(lowess(reres ~ refit), col=2)\ntext(reres[big3] ~ refit[big3], labels=rownames(hills2000)[big3],\npos=4-2*(refit[big3] &gt; mean(refit)), cex=0.8)\nmtext(side=3, line=0.75, \"B: Resistant (lqs) fit\", adj=0, cex=1.1)\n\n\n## Show only the 2nd diognostic plot, i.e., a normal Q-Q plot\n## plot(lhills2k.lm, which=2)\n\n\nOutliers, influential or not, should be taken seriously\n\n\n\nSubsection 3.4.2: Leverage, influence, and Cook’s distance\n\n\\(^*\\) Leverage and the hat matrix — technical details\n\nround(unname(hatvalues(timeClimb.lm)),2)\n\n\n\nInfluential points and Cook’s distance\n\n\nDynamic graphics\n\n## Residuals versus leverages\nnihills &lt;- DAAG::nihills\ntimeClimb.lm &lt;- lm(log(time)  ~ log(dist) + log(climb), data = nihills)\nplot(timeClimb.lm, which=5, add.smooth=FALSE, ps=9, sub.caption=\"\",\n     cex.caption=1.1, fg=\"gray\")\n  ## The points can alternatively be plotted using\n  ## plot(hatvalues(model.matrix(timeClimb.lm)), residuals(timeClimb.lm))\n\n\n## Residuals versus leverages\nplot(timeClimb.lm, which=5, add.smooth=FALSE)\n## The points can alternatively be plotted using\n## plot(hatvalues(model.matrix(timeClimb.lm)), residuals(timeClimb.lm))\n\n\n## This code is designed to be evaluated separately from other chunks\nwith(nihills, scatter3d(x=log(dist), y=log(climb), z=log(time), grid=FALSE,\n                        point.col=\"black\", surface.col=\"gray60\",\n                        surface.alpha=0.2, axis.scales=FALSE))\nwith(nihills, Identify3d(x=log(dist), y=log(climb), z=log(time),\n                labels=row.names(DAAG::nihills), minlength=8), offset=0.05)\n## To rotate display, hold down the left mouse button and move the mouse.\n## To put labels on points, right-click and drag a box around them, perhaps\n## repeatedly.  Create an empty box to exit from point identification mode.\n\n\n\nInfluence on the regression coefficients\n\n## Residuals versus leverages\nnihills &lt;- DAAG::nihills\ntimeClimb.lm &lt;- lm(log(time)  ~ log(dist) + log(climb), data = nihills)\nplot(timeClimb.lm, which=5, add.smooth=FALSE, ps=9, sub.caption=\"\",\n     cex.caption=1.1, fg=\"gray\")\n  ## The points can alternatively be plotted using\n  ## plot(hatvalues(model.matrix(timeClimb.lm)), residuals(timeClimb.lm))\n\n\n\n*Additional diagnostic plots\n\n## As an indication of what is available, try\ncar::influencePlot(allbacks.lm)\n\n\n\n\n\nSection 3.5 Assessment and comparison of regression models\n\nSubsection 3.5.1: *AIC, AICc, BIC, and Bayes Factors for normal theory regression models\n\n## Calculations using mouse brain weight data\nmouse.lm &lt;- lm(brainwt ~ lsize+bodywt, data=DAAG::litters)\nmouse0.lm &lt;- update(mouse.lm, formula = . ~ . - lsize)\n\n\naicc &lt;- sapply(list(mouse0.lm, mouse.lm), AICcmodavg::AICc)\ninfstats &lt;- cbind(AIC(mouse0.lm, mouse.lm), AICc=aicc,\n                  BIC=BIC(mouse0.lm, mouse.lm)[,-1])\nprint(rbind(infstats, \"Difference\"=apply(infstats,2,diff)), digits=3)\n\n\nlibrary(lattice)\ndf &lt;- data.frame(n=5:35, AIC=rep(2,31), BIC=log(5:35))\ncfAICc &lt;- function(n,p,d) 2*(p+d)*n/(n-(p+d)-1) - 2*p*n/(n-p-1)\ndf &lt;- cbind(df, AICc12=cfAICc(5:35,1,1), AICc34=cfAICc(5:35,3,1))\nlabs &lt;- sort(c(2^(0:6),2^(0:6)*1.5))\nxyplot(AICc12+AICc34+AIC+BIC ~ n, data=df, type='l', auto.key=list(columns=4),\n       scales=list(y=list(log=T, at=labs, labels=paste(labs))),\n par.settings=simpleTheme(lty=c(1,1:3), lwd=2, col=rep(c('gray','black'), c(1,3))))\n\n\nThe functions drop1() and add1()\n\n## Obtain AIC or BIC using `drop1()` or `add1()`\nn &lt;- nrow(DAAG::litters)\ndrop1(mouse.lm, scope=~lsize)              # AIC, with/without `lsize`\ndrop1(mouse.lm, scope=~lsize, k=log(n))   # BIC, w/wo `lsize`\nadd1(mouse0.lm, scope=~bodywt+lsize)     # AIC, w/wo `lsize`, alternative\n\n\n\nThe use of Bayesfactor::lmBF to compare the two models\n\nsuppressPackageStartupMessages(library(BayesFactor))\nbf1 &lt;- lmBF(brainwt ~ bodywt, data=DAAG::litters)\nbf2 &lt;- lmBF(brainwt ~ bodywt+lsize, data=DAAG::litters)\nbf2/bf1\n\n\n## Relative support statistics\nsetNames(exp(-apply(infstats[,-1],2,diff)/2), c(\"AIC\",\"AICc\",\"BIC\"))\n\n\n\n\nSubsection 3.5.2: Using anova() to compare models — the ihills data\n\nlognihr &lt;- log(DAAG::nihills)\nlognihr &lt;- setNames(log(nihr), paste0(\"log\", names(nihr)))\ntimeClimb.lm &lt;- lm(logtime ~ logdist + logclimb, data = lognihr)\ntimeClimb2.lm &lt;- update(timeClimb.lm, formula = . ~ . + I(logdist^2))\nprint(anova(timeClimb.lm, timeClimb2.lm, test=\"F\"), digits=4)\n\n\nprint(anova(timeClimb.lm, timeClimb2.lm, test=\"Cp\"), digits=3)\n## Compare with the AICc difference\nsapply(list(timeClimb.lm, timeClimb2.lm), AICcmodavg::AICc)\n\n\nform1 &lt;- update(formula(timeClimb.lm), ~ . + I(logdist^2) + logdist:logclimb)\naddcheck &lt;- add1(timeClimb.lm, scope=form1, test=\"F\")\nprint(addcheck, digits=4)\n\n\n\nSubsection 3.5.3: Training/test approaches, and cross-validation\n\n## Check how well timeClimb.lm model predicts for hills2000 data\ntimeClimb.lm &lt;- lm(logtime  ~ logdist + logclimb, data = lognihr)\nlogscot &lt;- log(subset(DAAG::hills2000,\n               !row.names(DAAG::hills2000)==\"Caerketton\"))\nnames(logscot) &lt;- paste0(\"log\", names(hills2000))\nscotpred &lt;- predict(timeClimb.lm, newdata=logscot, se=TRUE)\ntrainVar &lt;- summary(timeClimb.lm)[[\"sigma\"]]^2\ntrainDF &lt;- summary(timeClimb.lm)[[\"df\"]][2]\nmspe &lt;- mean((logscot[,'logtime']-scotpred[['fit']])^2)\nmspeDF &lt;- nrow(logscot)\n\n\npf(mspe/trainVar, mspeDF, trainDF, lower.tail=FALSE)\n\n\nscot.lm &lt;- lm(logtime ~ logdist+logclimb, data=logscot)\nsignif(summary(scot.lm)[['sigma']]^2, 4)\n\n\n\nSubsection 3.5.4: Further points and issues\n\nPatterns in the diagnostic plots – are they more than hints?\n{r 3_18, eval=F|\n\n\nWhat is the scatter about the fitted response\n\n\nModel selection and tuning risks\n\n\nGeneralization to new contexts requires a random sample of contexts\n\n\nWhat happens if we do not transform the hillrace data?\n\n\nAre “errors in x” an issue?\n\n\n\n\nSection 3.6 Problems with many explanatory variables\n\nSubsection 3.6.1: Variable selection issues\n\nVariable selection – a simulation with random data\n\ny &lt;- rnorm(100)\n## Generate a 100 by 40 matrix of random normal data\nxx &lt;- matrix(rnorm(4000), ncol = 40)\ndimnames(xx)&lt;- list(NULL, paste(\"X\",1:40, sep=\"\"))\n\n\n## ## Find the best fitting model. (The 'leaps' package must be installed.)\nxx.subsets &lt;- leaps::regsubsets(xx, y, method = \"exhaustive\", nvmax = 3, nbest = 1)\nsubvar &lt;- summary(xx.subsets)$which[3,-1]\nbest3.lm &lt;- lm(y ~ -1+xx[, subvar])\nprint(summary(best3.lm, corr = FALSE))\n\n\n## DAAG::bestsetNoise(m=100, n=40)\nbest3 &lt;- capture.output(DAAG::bestsetNoise(m=100, n=40))\ncat(best3[9:14], sep='\\n')\n\n\n\nThe extent of selection effects – a detailed simulation:\n\noldpar &lt;- par(fg='gray20',col.axis='gray20',lwd=0.5,col.lab='gray20')\nset.seed(41)\nlibrary(splines)\nDAAG::bsnVaryNvar(nvmax=3, nvar = 3:35, xlab=\"\")\nmtext(side=1, line=1.75, \"Number selected from\")\n\n\n\nCross-validation that accounts for the variable selection process\n\n\n*Regularization approaches\n\n\n\nSubsection 3.6.2: Multicollinearity\n\nAn example – compositional data\n\ndata(Coxite, package=\"compositions\")  # Places Coxite in the workspace\n  # NB: Proceed thus because `Coxite` is not exported from `compositions`\ncoxite &lt;- as.data.frame(Coxite)\n\n\noldpar &lt;- par(fg='gray20',col.axis='gray20',lwd=0.5,col.lab='gray20', tcl=-0.25)\npanel.cor &lt;- function(x, y, digits = 3, prefix = \"\", cex.cor=0.8, ...)\n{\nold.par &lt;- par(usr = c(0, 1, 0, 1)); on.exit(par(old.par))\nr &lt;- abs(cor(x, y))\ntxt &lt;- format(c(r, 0.123456789), digits = digits)[1]\ntxt &lt;- paste0(prefix, txt)\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * sqrt(r))\n}\npairs(coxite, gap=0.4, col=adjustcolor(\"blue\", alpha=0.9), upper.panel=panel.cor)\npar(oldpar)\n\n\ncoxiteAll.lm &lt;- lm(porosity ~ A+B+C+D+E+depth, data=coxite)\nprint(coef(summary(coxiteAll.lm)), digits=2)\n\n\ncoxiteAll.lm &lt;- lm(porosity ~ A+B+C+D+E+depth, data=coxite)\ncoxite.hat &lt;- predict(coxiteAll.lm, interval=\"confidence\")\nhat &lt;- coxite.hat[,\"fit\"]\nplot(porosity ~ hat, data=coxite, fg=\"gray\", type=\"n\", xlab=\"Fitted values\",\nylab=\"Fitted values, with 95% CIs\\n(Points are observed porosities)\",\ntcl=-0.35)\nwith(coxite, points(porosity ~ hat, cex=0.75, col=\"gray45\"))\nlines(hat, hat, lwd=0.75)\nord &lt;- order(hat)\nsebar &lt;- function(x, y1, y2, eps=0.15, lwd=0.75){\nlines(rep(x,2), c(y1,y2), lwd=lwd)\nlines(c(x-eps,x+eps), rep(y1,2), lwd=lwd)\nlines(c(x-eps,x+eps), rep(y2,2), lwd=lwd)\n}\nq &lt;- ord[round(quantile(1:length(hat), (1:9)/10))]\nfor(i in q)sebar(hat[i], coxite.hat[i,\"lwr\"], coxite.hat[i,\"upr\"])\ncoxiteAll.lm &lt;- lm(porosity ~ A+B+C+D+E+depth, data=coxite)\ncoxite.hat &lt;- predict(coxiteAll.lm, interval=\"confidence\")\nhat &lt;- coxite.hat[,\"fit\"]\n\n\n## Pointwise confidence bounds can be obtained thus:\nhat &lt;- predict(coxiteAll.lm, interval=\"confidence\", level=0.95)\n\n\n\n\nSubsection 3.6.3: The variance inflation factor (VIF)\n\nprint(DAAG::vif(lm(porosity ~ A+B+C+D+depth, data=coxite)), digits=2)\n\n\nb &lt;- leaps::regsubsets(porosity ~ ., data=coxite, nvmax=4, method='exhaustive')\n## The calculation fails for nvmax=5\ninOut &lt;- summary(b)[[\"which\"]]\n## Extract and print the coefficents for the four regressions\ndimnam &lt;- list(rep(\"\",4),c(\"Intercept\", colnames(coxite)[-7]))\ncmat &lt;- matrix(nrow=4, ncol=7, dimnames=dimnam)\nfor(i in 1:4)cmat[i,inOut[i,]] &lt;- signif(coef(b,id=1:4)[[i]],3)\noutMat &lt;- cbind(cmat,\"  \"=rep(NA,4),\nas.matrix(as.data.frame(summary(b)[c(\"adjr2\", \"cp\", \"bic\")])))\nprint(signif(outMat,3),na.print=\"\")\n\n\nBC.lm &lt;- lm(porosity ~ B+C, data=coxite)\nprint(signif(coef(summary(BC.lm)), digits=3))\ncar::vif(BC.lm)\n\n\n## Diagnostic plots can be checked thus:\nplot(BC.lm, eval=xtras)\n\n\nNumbers that do not quite add up\n\ncoxiteR &lt;- coxite\ncoxiteR[, 1:5] &lt;- round(coxiteR[, 1:5])\ncoxiteR.lm &lt;- lm(porosity ~ ., data=coxiteR)\nprint(coef(summary(coxiteR.lm)), digits=2)\nprint(DAAG::vif(lm(porosity ~ .-E, data=coxiteR)), digits=2)\n\n\n\nRemedies for multicollinearity\n\n\n\n\nSection 3.7 Errors in x\n\nMeasurement of dietary intake\n\n\nSimulations of the effect of measurement error\n\ngph &lt;- DAAG::errorsINx(gpdiff=0, plotit=FALSE, timesSDx=(1:4)/2,\n                       layout=c(5,1), print.summary=FALSE)[[\"gph\"]]\nparset &lt;- DAAG::DAAGtheme(color=FALSE, alpha=0.6, lwd=2,\n                          col.points=c(\"gray50\",\"black\"),\n                          col.line=c(\"gray50\",\"black\"), lty=1:2)\nupdate(gph, par.settings=parset)\n\n\n\n*Two explanatory variables\n\n\nTwo explanatory variables, one measured without error – a simulation\n\ngph &lt;- DAAG::errorsINx(gpdiff=1.5, timesSDx=(1:2)*0.8, layout=c(3,1),\nprint.summary=FALSE, plotit=FALSE)[[\"gph\"]]\nparset &lt;- DAAG::DAAGtheme(color=FALSE, alpha=0.6, lwd=2,\n                          col.points=c(\"gray50\",\"black\"),\n                          col.line=c(\"gray50\",\"black\"), lty=1:2)\nupdate(gph, par.settings=parset)\n\n\n\nAn arbitrary number of variables\n\n\n*The classical error model versus the Berkson error model\n\n\nUsing missing value approaches to address measurement error\n\n\n\nSection 3.8 Multiple regression models – additional points\n\ncoef(lm(area ~ volume + weight, data=allbacks))\nb &lt;- as.vector(coef(lm(weight ~ volume + area, data=allbacks)))\nc(\"_Intercept_\"=-b[1]/b[3], volume=-b[2]/b[3], weight=1/b[3])\n\n\nUnintended correlations\n\n\nSubsection 3.8.2: Missing explanatory variables\n\ngaba &lt;- DAAG::gaba\ngabalong &lt;- stack(gaba[\"30\", -match('min', colnames(gaba))])\ngabalong$sex &lt;- factor(rep(c(\"male\", \"female\",\"all\"), rep(2,3)),\nlevels=c(\"female\",\"male\",\"all\"))\ngabalong$treatment &lt;- factor(rep(c(\"Baclofen\",\"No baclofen\"), 3),\nlevels=c(\"No baclofen\",\"Baclofen\"))\ngph &lt;- lattice::stripplot(sex~values, groups=treatment, data=gabalong,\npanel=function(x,y,...){\nlattice::panel.stripplot(x,y,...)\nlattice::ltext(x,y,paste(c(3,9,15,7,22,12)), pos=1, cex=0.8)\n}, auto.key=list(space=\"right\", points=TRUE, cex=0.8))\nbw9 &lt;- list(fontsize=list(text=9, points=5),\ncex=c(1.5,1.5), pch=c(1,16))\nupdate(gph, par.settings=parset,\nxlab=list(\"Average reduction: 30 min vs 0 min\", cex=1.0),\nscales=list(cex=1.0, tck=0.35))\n\n\nStrategies\n\n\n\nSubsection 3.8.3: Added variable plots\n\nyONx.lm &lt;- lm(logtime ~ logclimb, data=lognihr)\ne_yONx &lt;- resid(yONx.lm)\nprint(coef(yONx.lm), digits=4)\n\n\nzONx.lm &lt;- lm(logdist ~ logclimb, data=lognihr)\ne_zONx &lt;- resid(zONx.lm)\nprint(coef(yONx.lm), digits=4)\n\n\ney_xONez_x.lm &lt;- lm(e_yONx ~ 0+e_zONx)\ne_yONxz &lt;- resid(ey_xONez_x.lm)\nprint(coef(ey_xONez_x.lm), digits=4)\n\n\noldpar &lt;- par(fg='gray')\n## Code for added variable plots\nlogtime.lm &lt;- lm(logtime ~ logclimb+logdist, data=lognihr)\ncar::avPlots(logtime.lm, lwd=1, terms=\"logdist\", fg=\"gray\")\nmtext(side=3, line=0.5, \"A: Added var: 'logdist'\", col=\"black\", adj=0, cex=1.15)\ncar::avPlots(logtime.lm, lwd=1, terms=\"logclimb\", fg=\"gray\")\nmtext(side=3, line=0.5, \"B: Added var: 'logclimb'\", col=\"black\", adj=0, cex=1.15)\npar(oldpar)\n\n\n## One call to show both plots\ncar::avPlots(timeClimb.lm, terms=~.)\n\n\n## Alternative code for first plot\nplot(e_yONx ~ e_zONx)\n\n\nplot(yONx.lm, which=1, caption=\"\", fg=\"gray\")\nmtext(side=3, line=0.5, \"A: From 'logtime' on 'logclimb'\", adj=0, cex=0.85)\nplot(zONx.lm, which=1, caption=\"\", fg=\"gray\")\nmtext(side=3, line=0.5, \"B: From 'logdist' on 'logclimb'\", adj=0, cex=0.85)\nplot(ey_xONez_x.lm, which=1, caption=\"\", fg=\"gray\")\nmtext(side=3, line=0.5, \"C: From AVP\", adj=-0, cex=0.85)\n\n\nAlternatives to Added Variable Plots\n\n\n*Algebraic details\n\nab1 &lt;- coef(yONx.lm)\nab2 &lt;- coef(zONx.lm)\nb2 &lt;- coef(ey_xONez_x.lm)\nb1 &lt;- ab1[2] - b2*ab2[2]\na &lt;- ab1[1] - b2*ab2[1]\n\n\ncoef(lm(logtime ~ logclimb + logdist, data=lognihr))\n\n\n\n\nSubsection 3.8.4: Nonlinear methods – an alternative to transformation?\n\nnihr$climb.mi &lt;- nihr$climb/5280\nnihr.nls0 &lt;- nls(time ~ (dist^alpha)*(climb.mi^beta), start =\n                    c(alpha = 0.68, beta = 0.465), data = nihr)\n## plot(residuals(nihr.nls0) ~ log(predict(nihr.nls0)))\n\n\nsignif(coef(summary(nihr.nls0)),3)\n\n\nnihr.nls &lt;- nls(time ~ gamma + delta1*dist^alpha + delta2*climb.mi^beta,\nstart=c(gamma = .045, delta1 = .09, alpha = 1,\ndelta2=.9, beta = 1.65), data=nihr)\n## plot(residuals(nihr.nls) ~ log(predict(nihr.nls)))\n\n\nsignif(coef(summary(nihr.nls)),3)\n\n\n\n\nSection 3.9: Recap\n\n\nSection 3.10: Further reading\n\n\nExercises (3.11)\n3.1\n\n## ## Set up factor that identifies the `have' cities\ncities &lt;- DAAG::cities\ncities$have &lt;- with(cities, factor(REGION %in% c(\"ON\",\"WEST\"),\n                                   labels=c(\"Have-not\",\"Have\")))\n\n\ngphA &lt;- lattice::xyplot(POP1996~POP1992, groups=have, data=cities,\n                auto.key=list(columns=2))\ngphB&lt;-lattice::xyplot(log(POP1996)~log(POP1992), groups=have, data=cities,\n                auto.key=list(columns=2))\nprint(gphA, split=c(1,1,2,1), more=TRUE)\nprint(gphB, split=c(2,1,2,1))\n\n\ncities.lm1 &lt;- lm(POP1996 ~ have+POP1992, data=cities)\ncities.lm2 &lt;- lm(log(POP1996) ~ have+log(POP1992), data=cities)\n\n3.8a\n\nnihills.lm &lt;- lm(time ~ dist+climb, data=DAAG::nihills)\nnihillsX.lm &lt;- lm(time ~ dist+climb+dist:climb, data=DAAG::nihills)\nanova(nihills.lm, nihillsX.lm)   # Use `anova()` to make the comparison\ncoef(summary(nihillsX.lm))       # Check coefficient for interaction term\ndrop1(nihillsX.lm)\n\n3.11\n\nlog(time) ~ log(dist) + log(climb)    ## lm model\ntime ~ alpha*dist + beta*I(climb^2)   ## nls model\n\n3.13\n\nx1 &lt;- runif(10)            # predictor which will be missing\nx2 &lt;- rbinom(10, 1, 1-x1)\n  ## observed predictor, depends on missing predictor\ny &lt;- 5*x1 + x2 + rnorm(10,sd=.1)  # simulated model; coef of x2 is positive\ny.lm &lt;- lm(y ~ factor(x2)) # model fitted to observed data\ncoef(y.lm)\ny.lm2 &lt;- lm(y ~ x1 + factor(x2))   # correct model\ncoef(y.lm2)\n\n3.16\n\nbomData &lt;- DAAG::bomregions2021\nnraw.lqs &lt;- MASS::lqs(northRain ~ SOI + CO2, data=bomData)\nnorth.lqs &lt;- MASS::lqs(I(northRain^(1/3)) ~ SOI + CO2, data=bomData)\nplot(residuals(nraw.lqs) ~ Year, data=bomData)\nplot(residuals(north.lqs) ~ Year, data=bomData)\n\n3.17f\n\nsocpsych &lt;- subset(DAAG::repPsych, Discipline=='Social')\nwith(socpsych, scatter.smooth(T_r.R~T_r.O))\nabline(v=.5)\n\n\nsoc.rlm &lt;- MASS::rlm(T_r.R~T_r.O, data=subset(socpsych, T_r.O&lt;=0.5))\n## Look at summary statistics\ntermplot(soc.rlm, partial.resid=T, se=T)\n\n\nplot(soc.rlm)\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch3.R\")\n}",
    "crumbs": [
      "Book",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "4  Chapter 4: Exploiting the linear model framework",
    "section": "",
    "text": "Packages required (with dependencies)\nDAAG effects mgcv splines scam MASS latticeExtra car WDI AICcmodavg ggplot2 kableExtra qgam patchwork\nAdditionally, Hmisc and knitr are required in order to process the Rmd source file.\nNote the use of the ‘patchwork’ package to make it easy to place two ggplot2 plots side by side.\n\nHmisc::knitrSet(basename=\"exploit\", lang='markdown', fig.path=\"figs/g\", w=7, h=7)\noldopt &lt;- options(digits=4, width=70, scipen=999)\nlibrary(knitr)\n## knitr::render_listings()\nopts_chunk[['set']](cache.path='cache-', out.width=\"80%\", fig.align=\"center\", \n                    fig.show='hold', formatR.arrow=FALSE, ps=10, \n                    strip.white = TRUE, comment=NA, width=70, \n                    tidy.opts = list(replace.assign=FALSE))\n\n\n\nSection 4.1 Levels of a factor – using indicator variables\n\nSubsection 4.1.1: Example – sugar weight\n\nsugar &lt;- DAAG::sugar  # Copy dataset 'sugar' into the workspace\n## Ensure that \"Control\" is the first level\nsugar[[\"trt\"]] &lt;- relevel(sugar[[\"trt\"]], ref=\"Control\")\noptions()[[\"contrasts\"]]  # Check the default factor contrasts\n## If your output does not agree with the above, then enter\n## options(contrasts=c(\"contr.treatment\", \"contr.poly\"))\n\n\nsugar.aov &lt;- aov(weight ~ trt, data=sugar)\n## To display the model matrix, enter: model.matrix(sugar.aov)\n## Note the use of summary.lm(), not summary() or summary.aov()\nround(signif(coef(summary.lm(sugar.aov)), 3), 4)\n\n\nsem &lt;- summary.lm(sugar.aov)$sigma/sqrt(3)  # 3 results/trt\n# Alternatively, sem &lt;- 6.33/sqrt(2)\nqtukey(p=.95, nmeans=4, df=8) * sem\n\n\n\nSubsection 4.1.2: Different choices for the model matrix when there are factors\n\ncontrasts(sugar$trt) &lt;- 'contr.sum'\nsugarSum.aov &lt;- aov(weight ~ trt, data = sugar)\nround(signif(coef(summary.lm(sugarSum.aov)), 3),4)\n\n\ndummy.coef(sugarSum.aov)\n\n\nFactor contrasts – further details\n\ncontrasts(sugar$trt) &lt;- \"contr.sum\"\n\n\nfish &lt;- factor(1:3, labels=c(\"Trout\",\"Cod\",\"Perch\"))\n\n\ncontr.treatment(fish)\n# Base is \"Trout\"\n\n\ncontr.SAS(fish)\n# Base is \"Perch\"\n\n\ncontr.sum(fish)\n# Base is mean of levels\n\n\n\n*Tests for main effects in the presence of interactions?\n\n\n\n\nSection 4.2 Block designs and balanced incomplete block designs\n\nSubsection 4.2.1: Analysis of the rice data, allowing for block effects\n\nrice &lt;- DAAG::rice\nricebl.aov &lt;- aov(ShootDryMass ~ Block + variety * fert, data=rice)\nprint(summary(ricebl.aov), digits=3)\n\n\nround(signif(coef(summary.lm(ricebl.aov)), 3), 5)\nwith(summary.lm(ricebl.aov),\ncat(\"Residual standard error: \", sigma, \"on\", df[2], \"degrees of freedom\"))\n\n## AOV calculations, ignoring block effects\nrice.aov &lt;- aov(ShootDryMass ~ variety * fert, data=rice)\nsummary.lm(rice.aov)$sigma\n\nricebl.aov &lt;- aov(ShootDryMass ~ factor(Block) + variety * fert, data=rice)\n\n\nmodel.tables(ricebl.aov, type=\"means\", se=TRUE, cterms=\"variety:fert\")\n\n\n\nSubsection 4.2.2: A balanced incomplete block design\n\nappletaste &lt;- DAAG::appletaste\nwith(appletaste, table(product, panelist))\n\n\nsapply(appletaste, is.factor)  # panelist & product are factors\nappletaste.aov &lt;- aov(aftertaste ~ product + panelist, data=appletaste)\nsummary(appletaste.aov)\n\n\nas.data.frame(effects::Effect(\"product\", appletaste.aov, confidence.level=0.95))\n\n\n## NB that 'product' was first term in the model formula\n## Thus, the 1st 4 coefficients have the information required\ncoef(summary.lm(appletaste.aov))[1:4, ]\n\n\n\n\nSection 4.3 Fitting multiple lines\n\n## Fit various models to columns of data frame leaftemp (DAAG)\nleaftemp &lt;- DAAG::leaftemp\nleaf.lm1 &lt;- lm(tempDiff ~ 1 , data = leaftemp)\nleaf.lm2 &lt;- lm(tempDiff ~ vapPress, data = leaftemp)\nleaf.lm3 &lt;- lm(tempDiff ~ CO2level + vapPress, data = leaftemp)\nleaf.lm4 &lt;- lm(tempDiff ~ CO2level + vapPress +\n               vapPress:CO2level, data = leaftemp)\n\n\nanova(leaf.lm1, leaf.lm2, leaf.lm3, leaf.lm4)\n\n\nprint(coef(summary(leaf.lm3)), digits=2)\n\n\n\nSection 4.4 Methods for fitting smooth curves\n\nSubsection 4.4.1: Polynomial Regression\n\nseedrates &lt;- DAAG::seedrates\nform2 &lt;- grain ~ rate + I(rate^2)\n# Without the wrapper function I(), rate^2 would be interpreted\n# as the model formula term rate:rate, and hence as rate.\nquad.lm2 &lt;- lm(form2, data = seedrates)\n## Alternative, using gam()\n## quad.gam &lt;- mgcv::gam(form2, data = seedrates)\n\n\nsuppressPackageStartupMessages(library(ggplot2))\n\n\n## Use ggplot2 functions to plot points, line, curve, & 95% CIs\n## library(ggplot2)\ngph &lt;- ggplot(DAAG::seedrates, aes(rate,grain))+\n  geom_point(aes(size=3), color='magenta')+xlim(c(25,185))\ncolors &lt;- c(\"Linear\"=\"blue\", \"Quadratic\"=\"red\")\nggdat &lt;- ggplot_build(gph+geom_smooth(aes(rate,grain,color=\"Linear\"),\n  method=lm, formula=y~poly(x,2),fullrange=TRUE))$data[[2]]\ngph1 &lt;- gph+geom_smooth(aes(color=\"Linear\"), method=lm, formula=y~x, fullrange=TRUE, fill='dodgerblue')\ngph1 +  geom_line(data = ggdat, aes(x = x, y = y, color=\"Quadratic\"),\n                  linewidth=0.75)+\n  geom_ribbon(data=ggdat, aes(x=x,y=y, ymin=ymin, ymax=ymax,\n                              color=\"Quadratic\"), linewidth=0.75,\n  fill=NA, linetype=2, outline.type='both', show.legend=FALSE) +\n  scale_color_manual(values=colors, aesthetics = \"color\")+\n  theme(legend.position=c(.8,.78)) +\n  coord_cartesian(expand=FALSE) + xlab(\"Seeding rate (kg/ha)\") +\n    ylab(\"Grains per head\") + labs(color=\"Model\") +\n  guides(size='none',\n         color = guide_legend(override.aes = list(fill=\"transparent\") ) )\n## detach(\"package:ggplot2\")\n\n\nquad.lm2 &lt;- lm(grain ~ rate + I(rate^2), data = DAAG::seedrates)\nprint(coef(summary(quad.lm2)), digits=2)\ncat(\"\\nCorrelation matrix\\n\")\nprint(summary(quad.lm2, corr=TRUE)$correlation, digits=2)\n\n\n*An alternative formulation using orthogonal polynomials\n\nseedratesP.lm2 &lt;- lm(grain ~ poly(rate,2), data = seedrates)\nprint(coef(summary(seedratesP.lm2)), digits=2)\n\n\n## Alternative, using mgcv::gam()\nseedratesP.gam &lt;- mgcv::gam(grain ~ poly(rate,2), data = seedrates)\n\n\nlogseed.lm &lt;- lm(log(grain) ~ log(rate), data=DAAG::seedrates)\ncoef(summary(logseed.lm))\n\n\n## Use ggplot2 functions to plot points, line, curve, & 95% CIs\n## library(ggplot2)\ngph &lt;- ggplot(DAAG::seedrates, aes(rate,grain)) +\n  geom_point(size=3, color=\"magenta\")+xlim(c(25,185))\ncolors &lt;- c(\"Loglinear\"=\"gray40\", \"Quadratic\"=\"red\")\nggdat &lt;- ggplot_build(gph+geom_smooth(method=lm, formula=y~poly(x,2),\n                                      fullrange=TRUE))$data[[2]]\nggln &lt;- ggplot_build(gph+geom_smooth(method=lm,\n                        formula=log(y)~log(x),fullrange=TRUE))$data[[2]]\n## Assign to gphA rather than (as in text) plotting at this point\ngphA &lt;- gph +  geom_line(data = ggdat, aes(x = x, y = y, color=\"Quadratic\"),\n                 linewidth=0.75) +\ngeom_ribbon(data=ggdat, aes(x=x,y=y, ymin=ymin, ymax=ymax, color=\"Quadratic\"),\n            linewidth=0.75, fill=NA, linetype=2, outline.type='both',\n            show.legend=FALSE) +\ngeom_line(data = ggln, aes(x = x, y = exp(y), color=\"Loglinear\"),\n          linewidth = 0.75) +\ngeom_ribbon(data=ggln, aes(x=x,y=exp(y), ymin=exp(ymin), ymax=exp(ymax),\n            color=\"Loglinear\"), fill=NA, linewidth=0.75, linetype=3,\n            outline.type='both', show.legend=FALSE)+\n  scale_color_manual(values=colors, aesthetics = \"color\")+\n  coord_cartesian(expand=FALSE) +\n  xlab(\"Seeding rate (kg/ha)\") + ylab(\"Grains per head\") +\n  labs(title=\"A: Loglinear fit vs quadratic fit\", color=\"Model\") +\n  guides(size='none',\n         color = guide_legend(override.aes = list(fill=\"transparent\") ) ) +\n  theme(legend.position=c(.8,.78))\ndf &lt;- data.frame(rate=rep(DAAG::seedrates$rate,2), res=c(resid(logseed.lm),\n  log(DAAG::seedrates$grain)-log(fitted(quad.lm2))),\n  Model=rep(c(\"Loglinear\",\"Quadratic\"),rep(nrow(DAAG::seedrates),2)))\n## Assign to gphB rather than (as in text) plotting at this point\ngphB &lt;- ggplot(df, aes(x=rate, y=res, shape=Model,color=Model))+\ngeom_point(size=2.5) + scale_color_manual(values=colors) +\nxlab(\"Seeding rate (kg/ha)\") + ylab(\"Residuals on log scale\") +\nlabs(title=\"B: Residuals\") +\n  guides(size='none',\n         color = guide_legend(override.aes = list(fill=\"transparent\") ) ) +\n  theme(legend.position=c(.8,.78))\n## Now take advantage of the magic of the 'patchwork' package\nlibrary(patchwork)\ngphA+gphB\n## detach(\"package:ggplot2\")\n\n\naic &lt;- AIC(quad.lm2, logseed.lm)\naic[\"logseed.lm\",2] &lt;- aic[\"logseed.lm\",2] + sum(2*log(seedrates$grain))\nround(aic,1)\n\n\nseedrates&lt;-DAAG::seedrates\nquad.lm2 &lt;- lm(grain ~ poly(rate,degree=2), data=seedrates)\nns.lm2 &lt;- lm(grain ~ splines::ns(rate,df=2), data=seedrates)\ntps.gam2 &lt;- mgcv::gam(grain ~ s(rate, k=3, fx=T), data=seedrates)\n\n\nmflist &lt;- lapply(list(quad=quad.lm2, nsplines=ns.lm2, tps=tps.gam2), model.matrix)\nmftab &lt;- with(mflist, cbind(quad, nsplines, tps))\ncolnames(mftab) &lt;- c(\"(Int)\", \"poly2.1\", \"poly2.2\", \"(Int)\", \"ns2.1\", \"ns2.2\", \"(Int)\", \"s3.1\", \"s3.2\")\nlibrary(kableExtra)\nlinesep = c('', '', '', '\\\\addlinespace')\nkbl(mftab, booktabs=TRUE, format='latex', toprule=FALSE,\nformat.args=list(justify=\"right\", width=8)) |&gt;\nkable_styling(latex_options = c(\"scale_down\",latex_options = \"hold_position\"), position='center') |&gt;\nadd_header_above(c('poly(rate,2)' = 3, 'splines::ns(rate,df=2)' = 3, 's(rate, k=3, fx=T)' = 3),\nalign='c', monospace=rep(T,3))|&gt;\nadd_header_above(c('lm: grain~' = 3, 'lm: grain~'=3, 'gam: grain~'=3),\n                 align='c', monospace=rep(T,3), line=F)\n\n\n\nGAM models versus models fitted using lm()\n\n\nAlternative fits – what is the best choice?\n\n## Load required packages\nsuppressPackageStartupMessages(library(splines))\nsuppressPackageStartupMessages(library(mgcv))\n\n\nohms.tp &lt;- gam(kohms~s(juice, bs=\"tp\"), data=fruitohms)\nohms.cs &lt;- gam(kohms~s(juice, bs=\"cs\"), data=fruitohms)\nrange(fitted(ohms.tp)-fitted(ohms.cs))\n\n\nsummary(ohms.tp)\n\n\nsummary(ohms.tpBIC)\n\n\n\n\nSubsection 4.4.3: The contributions of basis curves to the fit\n\n\nSubsection 4.4.4: Checks on the fitted model\n\n## Printed output from `gam.check(ohms.tpBIC)`\ncat(out, sep=\"\\n\")\n\n\n\nSubsection 4.3.5: Monotone curves\n\nohms.scam &lt;- scam::scam(kohms ~ s(juice,bs=\"mpd\"), data=fruitohms)\nsummary(ohms.scam)\n\n\nAIC(ohms.scam, ohms.tp)\n\n\nBIC(ohms.scam, ohms.tp)\n\n\n\nSubsection 4.4.6: Different smooths for different levels of a factor\n\nwhiteside &lt;- MASS::whiteside\ngas.gam &lt;- gam(Gas ~ Insul+s(Temp, by=Insul), data=whiteside)\n\n\nsummary(gas.gam)\n\n\nBox.test(resid(gas.gam)[whiteside$Insul=='Before'], lag=1)\nBox.test(resid(gas.gam)[whiteside$Insul=='After'], lag=1)\n\n\n\nSubsection 4.4.7: The remarkable reach of mgcv and related packages\n\nDepartures from independence assumptions\n\n\n\nSubsection 4.4.8: Multiple spline smoothing terms — dewpoint data\n\n## GAM model -- `dewpoint` data\ndewpoint &lt;- DAAG::dewpoint\nds.gam &lt;- gam(dewpt ~ s(mintemp) + s(maxtemp), data=dewpoint)\nplot(ds.gam, resid=TRUE, pch=\".\", se=2, cex=2, fg=\"gray\")\n\n\nUsing residuals as a check for non-additive effects\n\nlibrary(lattice)\n## Residuals vs maxtemp, for different mintemp ranges\nmintempRange &lt;- equal.count(dewpoint$mintemp, number=3)\nds.xy &lt;- xyplot(residuals(ds.gam) ~ maxtemp|mintempRange, data=dewpoint,\n                layout=c(3,1), scales=list(tck=0.5), aspect=1, cex=0.65,\n                par.strip.text=list(cex=0.75), type=c(\"p\",\"smooth\"),\n                xlab=\"Maximum temperature\", ylab=\"Residual\")\nds.xy\n\n\n\n*A smooth surface\n\n## Fit surface\nds.tp &lt;- gam(dewpt ~ s(mintemp, maxtemp), data=DAAG::dewpoint)\nvis.gam(ds.tp, plot.type=\"contour\")   # gives a contour plot of the\n# fitted regression surface\nvis.gam(ds.gam, plot.type=\"contour\")  # cf, model with 2 smooth terms\n\n\n\n\nSubsection 4.4.9: Atlantic hurricanes that made landfall in the US\n\nhurricNamed &lt;- DAAG::hurricNamed\nhurricS.gam &lt;- gam(car::yjPower(deaths, lambda=-0.2) ~\n  s(log(BaseDam2014)) + s(LF.PressureMB),\n  data=hurricNamed, method=\"ML\")\nanova(hurricS.gam)\n\n\nplot(hurricS.gam, resid=TRUE, pch=16, cex=0.5, select=1, fg=\"gray\")\nmtext(side=3, line=1, \"A: Term in log(BaseDam2014)\", cex=1.0, adj=0, at=-3.75)\nplot(hurricS.gam, resid=TRUE, pch=16, cex=0.5, select=2, fg=\"gray\")\nmtext(side=3, line=1, \"B: Term in LF.PressureMB\", cex=1.0, adj=0, at=878)\nqqnorm(resid(hurricS.gam), main=\"\", fg=\"gray\")\nmtext(side=3, line=1, \"C: Q-Q plot of residuals\", cex=1.0, adj=0, at=-4.25)\n\n\nAn explanatory variable with an overly long-tailed distribution\n\nhurricSlog1.gam &lt;- gam(log(deaths+1) ~ s(log(BaseDam2014)), data=hurricNamed)\nhurricSlog2.gam &lt;- gam(log(deaths+1) ~ s(BaseDam2014), data=hurricNamed)\n\n\nplot(hurricSlog1.gam, resid=TRUE, pch=16, cex=0.5, adj=0, fg=\"gray\")\nmtext(side=3, \"A: Use log(BaseDam2014)\", cex=1.4, adj=0, line=1, at=-3.15)\nplot(hurricSlog2.gam, resid=TRUE, pch=16, cex=0.5, fg=\"gray\")\nmtext(side=3, \"B: Use BaseDam2014\", cex=1.4, adj=0, line=1, at=-28500)\n\n\n\n\nSubsection 4.4.10: Other smoothing methods\n\n\n\nSection 4.5 Quantile regression\n\n## If necessary, install the 'WDI' package & download data\nif(!file.exists(\"wdi.RData\")){\n  if(!is.element(\"WDI\", installed.packages()[,1]) )install.packages(\"WDI\")\ninds &lt;- c('SP.DYN.TFRT.IN','SP.DYN.LE00.IN', 'SP.POP.TOTL')\nindnams &lt;- c(\"FertilityRate\", \"LifeExpectancy\", \"population\")\nwdi2020 &lt;- WDI::WDI(country=\"all\", indicator=inds, start=2020, end=2020,\n                    extra=TRUE)\nwdi2020 &lt;- na.omit(droplevels(subset(wdi2020, !region %in% \"Aggregates\")))\nwdi &lt;- setNames(wdi2020[order(wdi2020[, inds[1]]),inds], indnams)\nsave(wdi, file=\"wdi.RData\")\n}\n\n\n2020 World Bank data on fertility and life expectancy\n\nload(\"wdi.RData\")  # Needs `wdi.RData` in working directory; see footnote\nlibrary(qgam)\nwdi[, \"ppop\"] &lt;- with(wdi, population/sum(population))\nwdi[,\"logFert\"] &lt;- log(wdi[,\"FertilityRate\"])\nform &lt;- LifeExpectancy ~ s(logFert)\n## Panel A model\nfit.qgam &lt;- qgam(form, data=wdi, qu=.5)\n## Panel B: Multiple (10%, 90% quantiles; unweighted, then weighted\nfit19.mqgam &lt;- mqgam(form, data=wdi, qu=c(.1,.9))\nwtd19.mqgam &lt;- mqgam(form, data=wdi, qu=c(.1,.9),\n                      argGam=list(weights=wdi[[\"ppop\"]]))\n\n\nhat50 &lt;- cbind(LifeExpectancy=wdi[, \"LifeExpectancy\"], logFert=wdi[,\"logFert\"],\n                as.data.frame(predict(fit.qgam, se=T)))\nhat50 &lt;- within(hat50, {lo &lt;- fit-2*se.fit; hi &lt;- fit+2*se.fit})\nhat19 &lt;- as.data.frame(matrix(nrow=nrow(wdi), ncol=4))\nfor(i in 1:2){hat19[[i]] &lt;- qdo(fit19.mqgam, c(.1,.9)[i], predict)\n              hat19[[i+2]] &lt;- qdo(wtd19.mqgam, c(.1,.9)[i], predict) }\n  ## NB, can replace `predict` by `plot`, or `summary`\ncolnames(hat19) &lt;- c(paste0(rep(c('q','qwt'),c(2,2)), rep(c('10','90'),2)))\nhat19 &lt;- cbind(hat19, logFert=wdi[,\"logFert\"])\n\n\n## Panel A: Fit with SE limits, 50% quantile\ngphA &lt;- xyplot(lo+fit+hi~logFert, data=hat50, lty=c(2,1,2),lwd=1.5,type='l') +\n  latticeExtra::as.layer(xyplot(LifeExpectancy~logFert,\n                                data=hat50, pch='.', cex=2))\n## Panel B: Multiple quantiles; unweighted and weighted fits\ngph19 &lt;- xyplot(q10+q90+qwt10+qwt90 ~ logFert, type=\"l\",\n                data=hat19, lty=rep(1:2,c(2,2)),lwd=1.5)\ngphB &lt;- xyplot(LifeExpectancy ~ logFert, data=wdi) + as.layer(gph19)\nupdate(c(\"A: 50% curve, 2 SE limits\"=gphA, \"B: 0.1, 0.9 quantiles\"=gphB,\n         x.same=T, y.same=T), between=list(x=0.5),\n       xlab=\"Fertility Rate\", ylab=\"Life Expectancy\",\n       scales=list(x=list(at=log(2^((0:5)/2)), labels=round(2^((0:5)/2),1)),\n                   alternating=F),\n       par.settings=DAAG::DAAGtheme(color=F, col='gray50', cex=2, pch='.'))\n\n\n## Plots for the individual quantiles can be obtained thus:\n## ## Panel A\nplot(fit.qgam, shift=mean(predict(fit.qgam)))\n## Panel B, 10% quantile\nfitm10 &lt;- qdo(fit19.mqgam, qu=0.1)\nplot(fitm10, resid=T, shift=mean(predict(fitm10)),\n     ylim=range(wdi$LifeExpectancy), cex=2)\nwfitm10 &lt;- qdo(wtd19.mqgam, qu=0.1)\nplot(wfitm10, resid=T, shift=mean(predict(wfitm10)),\n     ylim=range(wdi$LifeExpectancy), cex=2)\n\n\n\n\nSection 4.6: Further reading and remarks\n\n\nExercises (4.7)\n4.2\n\nroller.lm &lt;- lm(depression~weight, data=DAAG::roller)\nroller.lm2 &lt;- lm(depression~weight+I(weight^2), data=DAAG::roller)\n\n4.4\n\ntoycars &lt;- DAAG::toycars\nlattice::xyplot(distance ~ angle, groups=factor(car), type=c('p','r'),\n                data=toycars, auto.key=list(columns=3))\n\n4.4a\n\nparLines.lm &lt;- lm(distance ~ 0+factor(car)+angle, data=toycars)\nsepLines.lm &lt;- lm(distance ~ factor(car)/angle, data=toycars)\n\n4.4b\n\nsepPol3.lm &lt;- lm(distance ~ factor(car)/angle+poly(angle,3)[,2:3], data=toycars)\n\n4.4c\n\nsapply(list(parLines.lm, sepLines.lm, sepPol3.lm), AICcmodavg::AICc)\n\n4.4e\n\nsetNames(sapply(list(parLines.lm, sepLines.lm, sepPol3.lm),\n  function(x)summary(x)$adj.r.squared), c(\"parLines\",\"sepLines\",\"sepPol3\"))\n\n4,7\n\nseedrates.lm &lt;- lm(grain ~ rate + I(rate^2), data=seedrates)\nseedrates.pol &lt;- lm(grain ~ poly(rate,2), data=seedrates)\n\n4.10a\n\ngeo.gam &lt;- gam(thickness ~ s(distance), data=DAAG::geophones)\n\n4.11\n\nplot(DAAG::geophones$distance, acf(resid(geo.gam), lag.max=55)$acf)\nBox.test(resid(geo.gam), lag=10)\nBox.test(resid(geo.gam), lag=20)\nBox.test(resid(geo.gam), lag=20, type=\"Ljung\")\n\n4.15\n\nlibrary(mgcv)\nxy &lt;- data.frame(x=1:200, y=arima.sim(list(ar=0.75), n=200))\ndf.gam &lt;- gam(y ~ s(x), data=xy)\nplot(df.gam, residuals=TRUE)\n\n4.16\n\nlibrary(mgcViz)\nohms.tpBIC &lt;- gam(kohms ~ s(juice, bs=\"tp\"), data=fruitohms, \n                  gamma=log(nrow(fruitohms))/2, method=\"REML\")\nohms.gamViz &lt;- mgcViz::getViz(ohms.tpBIC)   # Convert to a `gamViz` object              \ng1 &lt;- plot(sm(ohms.gamViz, 1))  # Graphics object for term 1 (of 1)\ng1 + l_fitLine(colour = \"red\") + l_rug(mapping = aes(x=x, y=y), alpha = 0.4) +\n     l_ciLine(mul = 2, colour = \"blue\", linetype = 2) +  # Multiply SE by `mul`\n     l_points(shape = 19, size = 1, alpha = 0.5)\n\n4.16a\n\nplot(sm(ohms.gamViz, 1), nsim = 20) + l_ciLine() + l_fitLine() + l_simLine()\n\n4.16b\n\ngam(Gas ~ Insul+s(Temp, by=Insul), data=whiteside) |&gt; \n   getViz() -&gt; gas.gamViz\nplot(sm(gas.gamViz,1), nsim = 20) + l_ciLine() + l_fitLine() + l_simLine()\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch4.R\")\n}",
    "crumbs": [
      "Book",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Exploiting the linear model framework</span>"
    ]
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "5  Chapter 5: Generalized linear models and survival analysis",
    "section": "",
    "text": "Packages required (with dependencies)\nDAAG car mgcv colorspace HistData gamlss dplyr tidyr MASS ggplot2 latticeExtra qgam VGAM survival HistData\nAdditionally, knitr is required in order to process the Rmd source file.\n\n\nSection 5.1 Generalized linear models\n\nSubsection 5.1.1: Linking the expected value to the covariate\n\n## Simplified plot showing the logit link function\np &lt;- (1:39)/40\nlogitp &lt;- log(p/(1 - p))\nplot(p, logitp, xlab = \"Proportion\", ylab = \"logit(p)\", type = \"l\", pch = 1)\n\n\npar(las=0)\np &lt;- seq(from=1, to=99, by=1)/100; n&lt;- 150; eps=0.001\ngitp &lt;- log(p/(1 - p))\nplot(p, gitp, xlab = \"\", ylab = \"\", type = \"l\", pch = 1,\nlas=1, xlim=0:1, xaxs=\"i\", fg=\"gray\")\nmtext(side = 1, line = 1.75, expression(\"Proportion \"*pi))\nmtext(side = 2, line = 1.75,\nexpression(\"logit(\"*pi*\") = log(Odds)\"))\nmtext(side = 3, line = 0.5, \"A: Logit link\", adj=0, cex=1.0)\npval &lt;- c(0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.999)\npar(mgp = c(2.5, 0.5, 0))\n##  axis(1, at=c(0,1), lwd=0, labels=c(0,1), xpd=TRUE)\naxis(4, adj=0.075, at = log(pval/(1 - pval)), las=1,\ncol=\"gray\", labels = paste(pval), lwd=0, lwd.ticks=1)\nseP &lt;- sqrt(p*(1-p)/100)\nplot(p, seP, xlab = \"\", ylab = \"\", type = \"l\", pch = 1,\nlas=1, xlim=0:1, xaxs=\"i\", fg=\"gray\")\n##  axis(1, at=c(0,1), lwd=0, labels=c(0,1), xpd=TRUE)\nmtext(side = 1, line = 1.75, expression(\"Proportion \"*pi))\nmtext(side = 2, line = 2.25, expression(\"SD[\"*p*\"], \"*n*\"=100\"))\nmtext(side = 3, line = 0.5,\nexpression(\"B: SD[\"*p*\"], \"*n*\"=100\"), adj=0, cex=1.0)\nseLP &lt;- (p*(1-p)/n)*((p+eps)*(1-p+eps))^-2\nplot(p, seLP, xlab = \"\", ylab = \"\", type = \"l\", pch = 1,\nlas=1, xlim=0:1, xaxs=\"i\", fg=\"gray\")\n##  axis(1, at=c(0,1), lwd=0, labels=c(0,1), xpd=TRUE)\nmtext(side = 1, line = 1.75, expression(\"Proportion \"*pi))\nmtext(side = 2, line = 1.75, expression(\"SD[logit(\"*p*\")], \"*n*\"=100\"))\nmtext(side = 3, line = 0.5, \"C: SD[logit(p)]\", adj=0, cex=1.0)\n\n\n\nSubsection 5.1.2: Noise terms need not be normal\n\n\nSubsection 5.1.3: Variation that is greater than binomial or Poisson\n\nLeast squares versus logistic regression\n\n\n\nSubsection 5.1.4: Log odds in contingency tables\n\n\nSubsection 5.1.5: Logistic regression with a continuous explanatory variable\n\nanestot &lt;- aggregate(DAAG::anesthetic[, c(\"move\",\"nomove\")],\nby=list(conc=DAAG::anesthetic$conc), FUN=sum)\n## The column 'conc', because from the 'by' list, is then a factor.\n## The next line recovers the numeric values\nanestot$conc &lt;- as.numeric(as.character(anestot[[\"conc\"]]))\nanestot$total &lt;- apply(anestot[, c(\"move\",\"nomove\")], 1 , sum)\nanestot$prop &lt;- anestot$nomove/anestot$total\n\n\npar(mgp=c(2.5,.5,0))\nanesthetic &lt;- DAAG::anesthetic\nz &lt;- table(anesthetic$nomove, anesthetic$conc)\ntot &lt;- apply(z, 2, sum)\nprop &lt;- z[2,  ]/(tot)\noprop &lt;- sum(z[2,  ])/sum(tot)\nconc &lt;- as.numeric(dimnames(z)[[2]])\npar(las=0)\nplot(conc, prop, xlab = \"Concentration\", ylab = \"Proportion\",\n     xlim=c(0.5, 2.5), ylim = c(0, 1), pch = 16, axes=F)\naxis(1, cex=0.9, lwd=0, lwd.ticks=1)\naxis(2, at=c(0, 0.5, 1.0), cex=0.9, lwd=0, lwd.ticks=1)\naxis(2, at=c(0.25, 0.75), cex=0.9, lwd=0, lwd.ticks=1)\nbox(col=\"gray\")\nchh &lt;- par()$cxy[2]\nchw &lt;- par()$cxy[1]\ntext(conc - 0.3 * chw, prop-sign(prop-0.5)*chh/4, paste(tot),\nadj = 1, cex=0.65)\nabline(h = oprop, lty = 2)\n\n\n## Fit model directly to the 0/1 data in nomove\nanes.glm &lt;- glm(nomove ~ conc, family=binomial(link=\"logit\"),\n                data=DAAG::anesthetic)\n## Fit model to the proportions; supply total numbers as weights\nanes1.logit &lt;- glm(prop ~ conc, family=binomial(link=\"logit\"),\n                  weights=total, data=anestot)\n\n\nDAAG::sumry(anes.glm, digits=2)\n\n\nA note on model output\n\n## Tp get coefficients, SEs, and associated statistics, specify:\nprint(coef(summary(anes.glm)), digits=2)\n## Get full default output\nsummary(anes.glm, digits=2)\n\n\n\n\n\nSection 5.2 Logistic multiple regression\n\nfrogs &lt;- DAAG::frogs\n\n\n## Presence/absence information: data frame frogs (DAAGS)\nsuppressMessages(library(ggplot2))\np &lt;- ggplot(frogs, aes(easting, northing)) +\n  geom_point(size=3, alpha=0.25) + coord_fixed() +\n  xlab(\"Meters east of reference point\")+ylab(\"Meters north\") +\n  theme(axis.title=element_text(size=11), axis.text=element_text(size=8))\np + geom_point(data=subset(frogs, pres.abs==1),\n               aes(easting, northing), alpha=1, shape=3, col=\"white\", size=1.5)\n\n\nfrogs &lt;- within(frogs, {maxSubmin &lt;- meanmax-meanmin\n                        maxAddmin &lt;- meanmax+meanmin})\n\n\nImplications for the Variance Inflation Factor\n\n\nSubsection 5.2.1: Choose explanatory terms, and fit model\n\n## Find power transformations\nuseCols &lt;- c('distance','NoOfPools','NoOfSites','avrain','maxAddmin','maxSubmin')\ntfrogs &lt;- car::powerTransform(frogs[,useCols], family=\"yjPower\")\n## Create, for later use, a matrix with variables transformed as suggested\ntransY &lt;- car::yjPower(frogs[,useCols], coef(tfrogs, round=TRUE))\nsummary(tfrogs, digits=2)\n\n\nfrogs0.glm &lt;- glm(formula = pres.abs ~ log(distance) + log(NoOfPools)+\n                  sqrt(NoOfSites) + avrain + maxAddmin + maxSubmin,\n                  family = binomial, data = frogs)\nDAAG::sumry(frogs0.glm, digits=1)\n\n\n## Check effect of omitting sqrt(NoOfSites) and avrain from the model\n## ~ . takes the existing formula. Precede terms to be\n## omitted by '-'.  (For additions, precede with '+')\nfrogs.glm &lt;- update(frogs0.glm, ~ . -sqrt(NoOfSites)-avrain)\nfrogsAlt.glm &lt;- update(frogs.glm, ~ . -maxAddmin+altitude)\nAIC(frogs0.glm, frogs.glm,frogsAlt.glm)\n\n\nrbind(\n'frogs0.glm'=coef(frogs0.glm)[c('log(distance)','log(NoOfPools)','maxAddmin','maxSubmin')],\n'frogs.glm'=coef(frogs.glm)[c('log(distance)','log(NoOfPools)','maxAddmin','maxSubmin')]\n)\ncoef(frogsAlt.glm)[c('log(distance)','log(NoOfPools)','altitude','maxSubmin')]\n\n\ncoef(summary(frogs.glm))\n\n\n\nSubsection 5.2.2: Fitted values\n\n## Use of `predict()` and `fitted()` --- examples\nfitted(frogs.glm)    # Fitted values' scale of response\npredict(frogs.glm, type=\"response\")  # Same as fitted(frogs.glm)\npredict(frogs.glm, type=\"link\")      # Scale of linear predictor\n## For approximate SEs, specify\npredict(frogs.glm, type=\"link\", se.fit=TRUE)\n\n\nlibrary(ggplot2)\nfrogs$Prob. &lt;- fitted(frogs.glm)\nfrogs$presAbs &lt;- factor(frogs$pres.abs)\np &lt;- ggplot(frogs, aes(easting, northing, color=Prob.)) +\n  geom_point(size=2, alpha=0.5) + coord_fixed() +\n  xlab(\"Meters east of reference point\")+ylab(\"Meters north\") +\n   theme(axis.title=element_text(size=9), axis.text=element_text(size=6))\np2 &lt;- p+scale_color_gradientn(colours=colorspace::heat_hcl(10,h=c(0,-100),\n                              l=c(75,40), c=c(40,80), power=1)) +\n  guides(fill=guide_legend(title=NULL))\np2 + geom_point(data=subset(frogs, presAbs==1),\n                aes(easting, northing), alpha=1, shape=3, col=\"white\", size=1)\n\n\n\nSubsection 5.2.3: Plots that show the contributions of explanatory variables\n\nopar &lt;- par(mgp=c(2.1,.4,0), mfrow=c(1,3))\nCholera &lt;- HistData::Cholera\nfitP2.glm &lt;- glm(cholera_deaths ~ offset(log(popn)) + water +\n                 log(elevation+3) + poly(poor_rate,2) +I(elevation==350),\n                 data=Cholera, family=quasipoisson)\nCholera[[\"water\"]] &lt;- factor(Cholera[[\"water\"]], labels=c(\"Battersea\",\n                             \"NewRiver\",\"Kew\"))\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\nylabs=rep(\"Partial residual\",3), terms='water', fg=\"gray\")\naxis(1, at=2, labels=\"NewRiver\", lwd=0, line=0.75)\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\n         ylabs=rep(\"Partial residual\",3), terms='log(elevation + 3)', fg=\"gray\")\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\n         ylabs=rep(\"Partial residual\",3), terms='poly(poor_rate, 2)', fg=\"gray\")\npar(opar)\n\n\n\nSubsection 5.2.4: Cross-validation estimates of predictive accuracy\n\nDAAG::CVbinary(frogs.glm)\n\n\nset.seed(19)\nfrogs.acc &lt;- frogs0.acc &lt;- numeric(6)\nfor (j in 1:6){\n  randsam &lt;- sample(1:10, 212, replace=TRUE)\n  ## Sample 212 values (one per pbservation) from 1:10\n  frogs.acc[j] &lt;- DAAG::CVbinary(frogs.glm, rand=randsam,\n                                print.details=FALSE)$acc.cv\n  frogs0.acc[j] &lt;- DAAG::CVbinary(frogs0.glm, rand=randsam,\n  print.details=FALSE)$acc.cv\n}\nprint(rbind(\"frogs (all variables)\" = frogs.acc,\n            \"frogs0 (selected variables)\" = frogs0.acc), digits=3)\n\n\n\nSubsection 5.2.5: Cholera deaths in London — 1849 to 1855\n\nBy air, or by water — the 1849 epidemic\n\nopar &lt;- par(mgp=c(2.1,.4,0), mfrow=c(1,3))\nCholera &lt;- HistData::Cholera\nfitP2.glm &lt;- glm(cholera_deaths ~ offset(log(popn)) + water +\n                 log(elevation+3) + poly(poor_rate,2) +I(elevation==350),\n                 data=Cholera, family=quasipoisson)\nCholera[[\"water\"]] &lt;- factor(Cholera[[\"water\"]], labels=c(\"Battersea\",\n                             \"NewRiver\",\"Kew\"))\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\nylabs=rep(\"Partial residual\",3), terms='water', fg=\"gray\")\naxis(1, at=2, labels=\"NewRiver\", lwd=0, line=0.75)\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\n         ylabs=rep(\"Partial residual\",3), terms='log(elevation + 3)', fg=\"gray\")\ntermplot(fitP2.glm, partial=T, se=TRUE, pch =1,\n         ylabs=rep(\"Partial residual\",3), terms='poly(poor_rate, 2)', fg=\"gray\")\npar(opar)\n\n\n\nThe 1854 epidemic — a natural experiment\n\n\n\n\nSection 5.3 Logistic models for categorical data – an example\n\n## Create data frame from multi-way table UCBAdmissions (datasets)\n## dimnames(UCBAdmissions)  # Check levels of table margins\nUCB &lt;- as.data.frame.table(UCBAdmissions[\"Admitted\", , ], responseName=\"admit\")\nUCB$reject &lt;- as.data.frame.table(UCBAdmissions[\"Rejected\", , ])$Freq\nUCB$Gender &lt;- relevel(UCB$Gender, ref=\"Male\")\n## Add further columns total and p (proportion admitted)\nUCB$total &lt;- UCB$admit + UCB$reject\nUCB$pAdmit &lt;- UCB$admit/UCB$total\n\n\nUCB.glm &lt;- glm(pAdmit ~ Dept*Gender, family=binomial, data=UCB, weights=total)\n## Abbreviated `anova()` output:\nanova(UCB.glm, test=\"Chisq\") |&gt;\n capture.output() |&gt; tail(8) |&gt; (\\(x)x[-c(2,3)])() |&gt; cat(sep='\\n')\n\n\nround(signif(coef(summary(UCB.glm)),4), 3)\n\n\n\nSection 5.4 Models for counts — poisson, quasipoisson, and negative binomial\n\nSubsection 5.4.1: Data on aberrant crypt foci\n\npar(pty=\"s\")\nplot(count ~ endtime, data=DAAG::ACF1, pch=16, fg=\"gray\")\n\n\nACF.glm &lt;- glm(formula = count ~ endtime + I(endtime^2),\n               family = poisson(link=\"identity\"), data = DAAG::ACF1)\nDAAG::sumry(ACF.glm, digits=2)\n\n\nunique(round(predict(ACF.glm),2))\n\n\nsum(resid(ACF.glm, type=\"pearson\")^2)/19\n\n\nACFq.glm &lt;- glm(formula = count ~ endtime + I(endtime^2),\nfamily = quasipoisson, data = DAAG::ACF1)\nprint(coef(summary(ACFq.glm)), digits=2)\n\n\nsapply(split(residuals(ACFq.glm), DAAG::ACF1$endtime), var)\n\n\nfligner.test(resid(ACFq.glm) ~ factor(DAAG::ACF1$endtime))\n\n\n\nSubsection 5.4.2: Moth habitat example\n\n## Number of moths by habitat: data frame DAAG::moths\nmoths &lt;- DAAG::moths\ntab &lt;- rbind(Number=table(moths[, 4]),\n             sapply(split(moths[, -4], moths$habitat), apply, 2, sum))\n\n\n## Number of zero counts, by habitats\nwith(droplevels(subset(moths, A==0)), table(habitat))\n\n\nlibrary(lattice)\ngph &lt;- dotplot(habitat ~ A+P, data=DAAG::moths, xlab=\"Number of moths\", outer=TRUE,\n               strip=strip.custom(factor.levels=paste(\"Number of species\",c(\"A\",\"B\"))),\n               panel=function(x, y, ...){\npanel.dotplot(x,y, pch=1, ...)\nav &lt;- sapply(split(x,y),mean)\nypos &lt;- factor(names(av), levels=names(av))\nlpoints(ypos~av, pch=3, col=\"gray45\", cex=1.25)\n},\nkey=list(text=list(c(\"Individual transects\", \"Mean\")),\npoints=list(pch=c(1,3), cex=c(1,1.25), col=c(\"black\",\"gray45\")),\ncolumns=2), scales=list(tck=0.5, alternating=1))\nbw9 &lt;- list(fontsize=list(text=9, points=5))\nupdate(gph, par.settings=bw9)\n\n\nAstats &lt;- with(DAAG::moths, sapply(split(A, habitat),\nfunction(x)c(Amean=mean(x),Avar=var(x))))\navlength &lt;- with(DAAG::moths, sapply(split(meters, habitat), mean))\nround(rbind(Astats, avlen=avlength),1)\n\n\nA quasipoisson model\n\nA.glm &lt;- glm(A ~ habitat + log(meters), family=quasipoisson,\ndata=DAAG::moths)\nDAAG::sumry(A.glm, digits=1)\n\n\nsubset(DAAG::moths, habitat==\"Bank\")\n\n## Analysis with tighter convergence criterion\nA.glm &lt;- update(A.glm, epsilon=1e-10)\nprint(coef(summary(A.glm)), digits=2)\n\nAfitSE &lt;- predict(A.glm, se=TRUE)$se.fit\ncfSE &lt;- with(DAAG::moths, c(AfitSE[habitat==\"Bank\"],\nrange(AfitSE[habitat!=\"Bank\"])))\nround(setNames(cfSE, c(\"SEbank\", \"SEotherMIN\", \"SEotherMAX\")), digits=2)\n\n\n\nA more satisfactory choice of reference level\n\nmoths &lt;- DAAG::moths\nmoths$habitat &lt;- relevel(moths$habitat, ref=\"Lowerside\")\nAlower.glm &lt;- glm(A ~ habitat + log(meters),\n                  family = quasipoisson, data = moths)\nprint(coef(summary(Alower.glm)), digits=1)\n\n\n\n\nSubsection 5.4.3: Models with negative binomial errors\n\ndframe &lt;- data.frame(sigma1A =(Astats[2,]-Astats[1,])/Astats[1,]^2,\nsigma2A =(Astats[2,]-Astats[1,])/Astats[1,]^1,\nmu = Astats[1,], habitat=colnames(Astats))\nbw9 &lt;- list(fontsize=list(text=9, points=5), pch=1:7)\nxyplot(sigma1A+sigma2A ~ mu, groups=habitat, outer=TRUE,\ndata=subset(dframe,habitat!=\"Bank\"),\npar.settings=bw9, auto.key=list(columns=4),\nstrip=strip.custom(factor.levels=paste(\"Model\",c(\"NBI\",\"NBII\"))),\nxlab=\"Mean number of species A moths\",\nylab=expression(\"Estimate of \"*sigma))\n\n\nlibrary(gamlss, quietly=TRUE)\nnoBank &lt;- subset(moths, habitat!='Bank')\nmothsCon.lss &lt;- gamlss(A ~ log(meters)+habitat, family=NBI(), data=noBank,\n                       trace=F)\nmothsVary.lss &lt;- gamlss(A ~ log(meters)+habitat, family=NBI(),\n                        sigma.formula=~habitat, trace=FALSE, data=noBank)\n\n\nLR.test(mothsCon.lss, mothsVary.lss)\n\n\n## mothsCon.lss &lt;- gamlss(A ~ log(meters)+habitat,family=NBI(),data=noBank)\n## summary(mothsCon.lss, type=\"qr\")   ## Main part of output\n\n\nDiagnostic plots\n\nplot(mothsCon.lss, panel=panel.smooth)\n\n\n\nUse of the square root link function\n\nAsqrt.lss &lt;- gamlss(A ~ habitat + sqrt(meters), trace=FALSE,\n                    family = NBI(mu.link='sqrt'), data = moths)\n\n\n## Asqrt.lss &lt;- gamlss(A ~ habitat + sqrt(meters),\n##                     family = NBI(mu.link='sqrt'), data = moths)\n## summary(Asqrt.lss, type=\"qr\")   ## Main part of output\nout &lt;- capture.output(summary(Asqrt.lss, digits=1))[-(3:10)]\ncat(out, sep=\"\\n\")\n\n\n\n\nSubsection 5.4.4: Negative binomial versus alternatives — hurricane deaths\n\nAside – a quasibinomial binomial fit\n\nordx &lt;- with(DAAG::hurricNamed, order(BaseDam2014))\nhurric &lt;- DAAG::hurricNamed[ordx,]\n# Ordering a/c values of BaseDam2014 simplifies later code\nhurr.glm &lt;- glm(deaths ~ log(BaseDam2014), family=quasipoisson, data=hurric)\nplot(hurr.glm, col=adjustcolor('black', alpha=0.4),\n     cex.caption=0.95, sub.caption=rep(\"\",4), fg=\"gray\")\n\n\n\nNegative binomial versus power transformed scale\n\n\nFit a negative binomial (NBI) model\n\nlibrary(gamlss)\nhurrNB.gamlss &lt;- gamlss::gamlss(deaths ~ log(BaseDam2014), family=NBI(),\n                                data=hurric[-56,])\nmures &lt;- resid(hurrNB.gamlss, what=\"mu\")\nzres &lt;- resid(hurrNB.gamlss, what=\"z-scores\")  ## equivalent normal quantiles\n\n\ntable(sign(mures))\n\n\n\nFit linear model to power transformed response\n\nhurr.lm &lt;- lm(car::yjPower(deaths,-0.2) ~ log(BaseDam2014), data=hurric[-56,])\n## Use the following function to transform from power scale to log scale\npowerTOlog &lt;- function(z, lambda)log(lambda*z+1)/lambda\n## Calculate fitted values, and transform to log(deaths+1) scale\nhatPower &lt;- powerTOlog(predict(hurr.lm), lambda=-0.2)\nresPower &lt;- log(hurric[-56,\"deaths\"]+1) - hatPower\n\n\ntable(sign(resPower))\n\n\n\nCompare NBI and power transform fits with smoothed quantiles\n\nlibrary(qgam, quietly=TRUE)\nhat68.8 &lt;- predict(qgam(log(deaths+1) ~ s(log(BaseDam2014)), qu=.648,\n                        data=hurric[-56,]))\nhat40.9 &lt;- predict(qgam(log(deaths+1) ~ s(log(BaseDam2014)), qu=.409,\n                        data=hurric[-56,]))\n\n\nxvar &lt;- log(hurric$BaseDam2014)[-56]\nplot(log(deaths+1) ~ log(BaseDam2014), data=hurric, xaxt=\"n\", yaxt=\"n\",\n  cex=4, pch=\".\", fg=\"gray\", col=adjustcolor(\"black\",alpha.f=0.65),\n  xlab=\"Damage, millions of US$ in 2014\", ylab=\"Deaths\")\naxis(1, at=log(c(1,10,1000, 100000)),\n  labels=paste(c(1,10,1000, 100000)), lwd=0, lwd.ticks=1)\naxis(2, at=log(c(0,10,100,1000)+1),\n  labels=paste(c(0,10,100,1000)), lwd=0, lwd.ticks=1)\n## Negative binomial regression fitted values\nhatNB &lt;- fitted(hurrNB.gamlss)\nlines(xvar, log(hatNB+1), col=\"blue\", lty=2)\nwith(hurric, text(log(BaseDam2014)[56], log(deaths+1)[56], \"Audrey\", pos=3),\n     cex=0.72)\n## Show fit from power transform model\nlines(xvar, hatPower, col=\"blue\", lty=1)\n## Show 68.8\\% and 40.1\\% fits from regression smooths\nlines(hat68.8 ~ xvar, lty=2, col='red')\nlines(hat40.9 ~ xvar, lty=1, col='red')\nlegend(\"topleft\", col=rep(c('blue','red'),c(2,2)), lty=rep(2:1,2), cex=0.8,\n       y.intersp=0.75, legend=c(\"Negative binomial fit\",\"Power transform fit\",\n                                \"68.8% quantile\", \"40.9% quantile\"), bty=\"n\")\nmtext(side=3, \"A: Deaths vs damage\", line=0.5, cex=1.15, adj=0)\n## Quantile-quantile plot -- negative binomial model\nqqnorm(zres, main=\"\", fg=\"gray\", cex=0.5,\n  col=adjustcolor(\"black\",alpha.f=0.65)); qqline(zres, col=2)\nmtext(side=3, \"B: Q-Q plot\", line=0.5, cex=1.15, adj=0)\n\n\n## a) Fitted and empirical centiles from hurrNB.gamlss\npc &lt;- t(centiles.split(hurrNB.gamlss, xvar=log(hurric$BaseDam2014)[-56],\n   cent=c(5,10,25,50,75,90,95), xcut.points=log(c(150, 1500)),\n   plot=FALSE))\nrownames(pc) &lt;- c(\"up to 150M\", \"150M to 1500M\", \"above 1500M\")\nround(pc,2)\n\n\nhurrP.gamlss &lt;- gamlss(car::yjPower(deaths, -0.2) ~ log(BaseDam2014), data=hurric)\n\n\n## Fitted and empirical centiles from hurrP.gamlss\npc &lt;- t(centiles.split(hurrP.gamlss, xvar=log(hurric$BaseDam2014),\ncent=c(5,10,25,50,75,90,95),\nxcut.points=log(c(150, 1500)), plot=FALSE))\nrownames(pc) &lt;- c(\"up to 150M\", \"150M to 1500M\", \"above 1500M\")\nround(pc,2)\n\n\n\n\n\nSection 5.5 Fitting smooths\n\nSubsection 5.5.1: Handedness of first-class cricketers in the UK\n\ntab &lt;- with(DAAG::cricketer, table(left,dead))\ncolnames(tab) &lt;- c('live','dead')\ntab &lt;- cbind(addmargins(tab, margin=2), prop.table(tab, margin=1))\ntab\n\n\nlibrary(mgcv)\nlibrary(latticeExtra)\nDAAG::cricketer |&gt; dplyr::count(year, left, name=\"Freq\") -&gt; handYear\nnames(handYear)[2] &lt;- \"hand\"\nbyYear &lt;- tidyr::pivot_wider(handYear, names_from='hand', values_from=\"Freq\")\nhand.gam &lt;- gam(cbind(left,right) ~ s(year), data=byYear, family=binomial)\nconst &lt;- attr(predict(hand.gam, type='terms'), \"constant\")\n  ## `const` is the mean on the scale of the linear predictor\nplot(hand.gam, shift=const, trans=function(x)exp(x)/(1+exp(x)), ylim=c(.05,.4),\n     xlab=\"\", ylab=\"Proportion lefhanded\", rug=FALSE, fg=\"gray\",\n     main=list(\"Proportion lefthanded, with 2SE limits\",font=1,cex=1.2))\n  ## Add `const`, then apply inverse link function.\n  ## Plots estimated proportions (i.e., on the scale of the response)\nwith(byYear, points(year, I(left/(left+right)), cex=0.8, col=\"gray50\"))\nleftrt.gam &lt;- gam(Freq ~ hand + s(year, by=factor(hand)), data=handYear,\n                  family=poisson)\nleftrt.pred &lt;- predict(leftrt.gam, se=T, type='response')\nhandYear &lt;- cbind(handYear, as.data.frame(leftrt.pred))\ncol2 &lt;- DAAG::DAAGtheme(color=T)$superpose.symbol$col[c(2,2,1)]\ngph.key &lt;- list(space=\"top\", columns=3, lines=list(lty=c(1,2,1), lwd=2, col=col2),\n                text=list(c(\"left\",expression(4.4%*%\"left\"),\"right\")), cex=1.2)\ngph &lt;- xyplot(leftrt.pred$fit ~ year, groups=hand, ylab=list(\"Number born\", cex=1.2),\n              type=\"l\", xlab=\"\", data=handYear, key=gph.key, col=col2[c(3,1)], lwd=2)\ngph1 &lt;- xyplot(Freq~year, groups=hand, data=handYear, col=col2[c(3,1)])\ngph2 &lt;- xyplot(I(4.4*fit) ~ year, data=subset(handYear, hand==\"left\"),\n               type=\"l\", lty=2, lwd=2, col=col2[2])\nupdate(gph+as.layer(gph1)+as.layer(gph2), par.settings=DAAG::DAAGtheme(color=TRUE),\n       scales=list(cex=1.2))\n\n\n\n\nSection 5.6 Additional notes on generalized linear models\n\nSubsection 5.6.1: Residuals, and estimating the dispersion\n\nOther choices of link function for binomial models\n\n\nQuasi models — estimating the dispersion\n\n\n\nSubsection 5.6.2: Standard errors and \\(z\\)- or \\(t\\)-statistics for binomial models\n\nfac &lt;- factor(LETTERS[1:4])\np &lt;- c(73, 30, 11, 2)/500\nn &lt;- rep(500,4)\nround(signif(coef(summary(glm(p ~ fac, family=binomial, weights=n))), 6), 6)\n\n\np &lt;- c(0.001,0.002,(1:99)/100,0.998,0.999)\nfor(i in 1:3){\nlink &lt;- c(\"logit\", \"probit\", \"cloglog\")[i]\nfun &lt;- make.link(link)$linkfun\nx &lt;- fun(p)\nu &lt;- glm(p ~ x, family=binomial(link=link), weights=rep(1000,103))\nif  (i==1)\nplot(x, hatvalues(u), type=\"l\", ylab=\"Leverage\", xaxt=\"n\", fg='gray',\nyaxt=\"n\",\nylim=c(0, 0.0425), yaxs=\"i\", xlab=\"Fitted proportion\") else {\nphat &lt;- predict(u, type=\"response\")\nlines(log(phat/(1-phat)), hatvalues(u), type=\"l\",\ncol=c(\"black\",\"black\",\"gray\")[i], lwd=0.75,\nlty=c(1,2,1)[i])\n}\n}\npos=c(0.001,0.002, 0.005, 0.01,0.02,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.98,0.99, 0.995, 0.998, 0.999)\nsub1 &lt;- seq(from=1,to=17, by=2)\nsub3 &lt;- seq(from=2,to=16, by=2)\naxis(1, at=log(pos/(1-pos))[sub1], labels=paste(pos)[sub1],\ncex.axis=0.7, lwd=0, lwd.ticks=1)\naxis(3, at=log(pos/(1-pos))[sub3], labels=paste(pos)[sub3],\ncex.axis=0.7, lwd=0, lwd.ticks=1)\naxis(2, at=c(0,.01,.02,.03), cex.axis=.7, lwd=0, lwd.ticks=1)\nlegend(\"topleft\", lty=c(1,2,1),\nlegend=c(\"logit link\", \"probit link\", \"cloglog link\"),\ncol=c(\"black\",\"black\",\"gray\"), bty=\"n\", cex=0.8)\n\n\n\n\nSection 5.7 Models with an ordered categorical or categorical response\n\nlibrary(VGAM)\ninhaler &lt;-  data.frame(freq=c(99,76,41,55,2,13),\n  choice=rep(c(\"inh1\",\"inh2\"), 3),\n  ease=ordered(rep(c(\"easy\",\"re-read\",\"unclear\"), rep(2,3))))\ninhaler1.vglm &lt;-  vglm(ease ~ 1, weights=freq, data=inhaler,\n  cumulative(link=\"logitlink\"), subset=inhaler$choice==\"inh1\")\ninhaler2.vglm &lt;-  vglm(ease ~ 1, weights=freq, data=inhaler,\n  cumulative(link=\"logitlink\"), subset=inhaler$choice==\"inh2\")\n\n\n## Inhaler 1\nround(coef(summary(inhaler1.vglm)),3)\n## Inhaler 2\nround(coef(summary(inhaler2.vglm)),3)\n\n\ninhaler.vglm &lt;-  vglm(ease ~ choice, weights=freq, data=inhaler,\ncumulative(link=\"logitlink\", parallel=FALSE))\nround(coef(summary(inhaler.vglm)),3)\n\n\ninhalerP.vglm &lt;-  vglm(ease ~ choice, weights=freq, data=inhaler,\ncumulative(link=\"logitlink\", parallel=TRUE))\nround(coef(summary(inhalerP.vglm)),3)\n\n\npred &lt;- predict(inhalerP.vglm, se.fit=TRUE, newdata=inhaler[1:2,])\ncolnames(pred$se.fit) &lt;- paste(\"SE\", colnames(pred$se.fit))\nfitvals &lt;- with(pred, cbind(fitted.values, se.fit))\ncolnames(fitvals) &lt;- gsub('link', '', colnames(fitvals))\nround(fitvals, 2)\n\n\nd &lt;- deviance(inhalerP.vglm) - deviance(inhaler.vglm)\n## Refer to chi-squared distribution with 1 degree of freedom\nc(Difference=d, \"p-Value\"=pchisq(3.416, df=1, lower.tail=FALSE))\n\n\nSubsection 5.7.2: Loglinear Models\n\n\n\nSection 5.8 Survival analysis\n\nsuppressMessages(library(survival))\n\n\ndf &lt;- data.frame(x0 = c(1, 5, 1, 2, 14, 10, 12, 19)*30,\nx1 = c(46, 58, 85, 67, 17, 85, 18, 42)*30,\nfail = c(1, 0, 0, 1, 1, 0, 0, 1))\nplot(c(0, 2610), c(0.65, 8.15), type = \"n\",\nxlab = \"Days from beginning of study\",\nylab = \"Subject number\", axes = F)\n##  mtext(side = 1, line = 2.5, \"Days from beginning of study\", adj = 0.5)\nm &lt;- dim(df)[1]\npar(las=2)\naxis(2, at = (1:m), labels = paste((m:1)), lwd=0, lwd.ticks=1)\npar(las=1)\nabline(v = 600, lty = 4, col=\"gray40\")\nabline(v = 2550)\nmtext(side = 3, line = 0.5, at = c(600, 2550),\ntext = c(\"\\nEnd of recruitment\",\n\"\\nEnd of study\"), cex = 0.9)\nlines(rep((0:8) * 300, rep(3, 9)), rep(c(-0.4, -0.2, NA), 9),\nxpd = T)\nmtext(side = 1, line = 1.0, at = (0:8) * 300,\ntext = paste((0:8) * 300), adj = 0.5)\nchw &lt;- par()$cxy[1]\nxx &lt;- as.vector(t(cbind(df[, 1], df[, 2] - 0.25 * chw,\nrep(NA, m))))\nyy &lt;- as.vector(t(cbind(matrix(rep(m:1, 2), ncol = 2),\nrep(NA, m))))\nlines(as.numeric(xx), as.numeric(yy))\npoints(df[, 1], m:1, pch = 16)\ntext(df[, 1]-0.25*chw, m:1, paste(df[,1]), pos=1, cex=0.75)\nfail &lt;- as.logical(df$fail)\npoints(df[fail, 2], (m:1)[fail], pch = 15)\npoints(df[!fail, 2], (m:1)[!fail], pch = 0)\ntext(df[, 2]+0.25*chw, m:1, paste(df[,2]), pos=1, cex=0.75)\npar(xpd=TRUE)\nlegend(0, 11.5, pch = 16, legend = \"Entry\", y.intersp=0.15)\nlegend(1230, 11.5, pch = c(15, 0),\nlegend = c(\"Dead\", \"Censored\"), ncol=2, y.intersp=0.15)\n\n\nSubsection 5.8.1: Analysis of the Aids2 data\n\nstr(MASS::Aids2, vec.len=2)\n\n\nbloodAids &lt;- subset(MASS::Aids2, T.categ==\"blood\")\nbloodAids$days &lt;- bloodAids$death-bloodAids$diag\nbloodAids$dead &lt;- as.integer(bloodAids$status==\"D\")\n\n\nbloodAids &lt;- subset(MASS::Aids2, T.categ==\"blood\")\nbloodAids$days &lt;- bloodAids$death-bloodAids$diag\nbloodAids$dead &lt;- as.integer(bloodAids$status==\"D\")\nplot(survfit(Surv(days, dead) ~ sex, data=bloodAids),\n     col=c(2,4), conf.int=TRUE, lty=1, fg=\"gray\",\n     xlab=\"Days from diagnosis\", ylab=\"Survival probability\")\nlegend(\"top\", legend=levels(bloodAids$sex), lty=c(1,1),\n       col=c(2,4), horiz=TRUE, bty=\"n\")\n\n\n## Pattern of censoring for male homosexuals\nhsaids &lt;- subset(MASS::Aids2, sex==\"M\" & T.categ==\"hs\")\nhsaids$days &lt;- hsaids$death-hsaids$diag\nhsaids$dead &lt;- as.integer(hsaids$status==\"D\")\ntable(hsaids$status,hsaids$death==11504)\n\n\nhsaids &lt;- subset(MASS::Aids2, sex==\"M\" & T.categ==\"hs\")\nhsaids$days &lt;- hsaids$death-hsaids$diag\nhsaids$dead &lt;- as.integer(hsaids$status==\"D\")\nhsaids.surv &lt;- survfit(Surv(days, dead) ~ 1, data=hsaids)\nplot(hsaids.surv, col=\"gray\", conf.int=F, tcl=-0.4, fg=\"gray\")\npar(new=TRUE)\nplot(hsaids.surv,col=1, conf.int=F,mark.time=F, fg=\"gray\",\nxlab=\"Days from diagnosis\", ylab=\"Estimated survival probabality\")\nchw &lt;- par()$cxy[1]\nchh &lt;- par()$cxy[2]\nsurv &lt;- hsaids.surv$surv\nxval &lt;- c(200,700,1400,1900)\nhat &lt;- approx(hsaids.surv$time, surv, xout=xval)$y\nfor(i in 1:2) arrows(xval[i], hat[i], 0, hat[i],\nlength=0.05, col=\"gray\")\nlines(rep(xval[1],2), hat[1:2], col=\"gray\")\n##    lines(rep(xval[3],2), hat[3:4], col=\"gray\")\n## Offset triangle 1\nchw &lt;- par()$cxy[1]\nlines(xval[c(1,2,1,1)]+650, hat[c(2,2,1,2)]+0.2,col=\"gray40\")\nxy1 &lt;- c(mean(xval[c(1,1,2)]), mean(hat[c(1,2,2)]))\narrows(xy1[1], xy1[2], xy1[1]+650, xy1[2]+0.2, col=\"gray40\", length=0.1)\ntext(xval[1]-0.1*chw+650, hat[1]+0.2,\npaste(round(hat[1],2)), col=\"gray20\",cex=0.75, adj=1)\ntext(xval[1]+650-0.1*chw, hat[2]+0.2,\npaste(round(hat[2],2)), col=\"gray20\",cex=0.75, adj=1)\ntext(mean(xval[1:2])+650, hat[2]+0.2-0.5*chh,\npaste(round(diff(xval[1:2]))), col=\"gray20\", cex=0.75)\ntext(xval[1]+650-0.5*chw, mean(hat[1:2]+0.2), paste(round(hat[1]-hat[2],3)),\nsrt=90, adj=0.5, col=\"gray20\", cex=0.75)\n\n\n\nSubsection 5.8.4: Hazard rates\n\n\nSubsection 5.8.5: The Cox proportional hazards model\n\nbloodAids.coxph &lt;- coxph(Surv(days, dead) ~ sex, data=bloodAids)\nprint(summary(bloodAids.coxph), digits=6)\n\n\n## Add `age` as explanatory variable\nbloodAids.coxph1 &lt;- coxph(Surv(days, dead) ~ sex+age, data=bloodAids)\n\n\nbloodAids &lt;- subset(MASS::Aids2,T.categ==\"blood\")\nbloodAids &lt;- within(bloodAids, {days &lt;- death-diag\ndead &lt;- as.integer(status==\"D\")})\nbloodAids.coxph &lt;- coxph(Surv(days, dead) ~ sex, data = bloodAids)\nplot(cox.zph(bloodAids.coxph), cex=0.75, bty=\"n\")\nbox(col=\"gray\")\n\n\ncox.zph(bloodAids.coxph)\n\n\ncricketer &lt;- DAAG::cricketer\nkia4.coxph &lt;- coxph(Surv(life, kia) ~ left/poly(year,4),\n                    data = cricketer, model=T)\nkia6.coxph &lt;- update(kia4.coxph, . ~ left/poly(year,6),\n                     data = cricketer, model=T)\n# Type `plot(cox.zph(kia6.coxph)` to plot the two graphs\n# Perhaps check also `AIC(kia4.coxph, kia6.coxph)`\ncox.zph(kia6.coxph)\n\n\nplot(cox.zph(kia6.coxph), cex=0.75, bty=\"n\")\nbox(col=\"gray\")\n\n\n\n\nSection 5.9: Transformations for proportions and counts\n\n\nSection 5.10: Further reading\n\n\nExercises (5.11)\n5.1\n\ninhibition &lt;- rbind(\nconc =c(0.1,0.5, 1,10,20,30,50,70,80,100,150),\nno  = c(7,  1, 10, 9, 2, 9, 13, 1, 1,  4,  3),\nyes = c(0,  0, 3, 4, 0, 6, 7, 0, 0,  1,  7)\n)\ncolnames(inhibition) &lt;- rep(\"\", ncol(inhibition))\ninhibition\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch5.R\")\n}",
    "crumbs": [
      "Book",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Generalized linear models and survival analysis</span>"
    ]
  },
  {
    "objectID": "ch9.html",
    "href": "ch9.html",
    "title": "6  Chapter 9: Multivariate data exploration and discrimination",
    "section": "",
    "text": "Packages required (plus any dependencies)\nPackages used are: DAAG MASS RColorBrewer teigen BiocManager DAAGbio hddplot lmtest splines cobalt mice datasets car micemd oz randomForest ggplot2 latticeExtra mvtnorm teigen limma hddplot mgcv MatchIt sandwich gridExtra DAAGbio mlbench (exercise).\nAdditionally, knitr and Hmisc are required in order to process the Rmd source file.\n\nHmisc::knitrSet(basename=\"mva\", lang='markdown', fig.path=\"figs/g\", w=7, h=7)\noldopt &lt;- options(digits=4, formatR.arrow=FALSE, width=70, scipen=999)\nlibrary(knitr)\nopts_chunk[['set']](cache.path='cache-', out.width=\"80%\", fig.align=\"center\", \n                    fig.show='hold', ps=10, strip.white = TRUE,\n                    comment=NA, width=70, tidy.opts = list(replace.assign=FALSE))\n\n\n\nSection 9.1: Multivariate exploratory data analysis\n\n## Make the lattice package and the possum dataset available\nlibrary(latticeExtra)\npossum &lt;- DAAG::possum\n\n\nSubsection 9.1.1: Scatterplot matrices\n\n## Colors distinguish sexes; symbols distinguish sites\nsitenames &lt;- row.names(DAAG::possumsites)[c(1,2,4:6,3,7)]\nkey &lt;- list(points = list(pch=0:6), text=list(sitenames),\n            columns=4, between=1, between.columns=2)\ncolr &lt;- c(\"red\",\"blue\")\nvnames &lt;- c(\"tail\\nlength\",\"foot\\nlength\", \"conch\\nlength\")\ngphA &lt;- with(possum, splom(~ possum[, 9:11], pch=(0:6)[site], col=colr[sex],\n            xlab=\"\",  varnames=vnames, key=key, axis.line.tck=0.6))\ngphB &lt;- with(possum, cloud(earconch~taill+footlgth, data=possum, \n  col=colr[sex], key=key, pch = (0:6)[site], \n  zlab=list(\"earconch\", rot=90), zoom=0.925))\nupdate(c(\"A: Scatterplot matrix\"=gphA, \"B: Cloud plot\"=gphB),\n       between=list(x=1))\n\n\n\nSubsection 9.1.2: Principal components analysis\n\nPreliminary data scrutiny\n\n## Ratios of largest to smallest values: possum[, 6:14] (DAAG)\npossum &lt;- DAAG::possum\nsapply(na.omit(possum[, 6:14]), function(x)round(max(x)/min(x),2))\n\n\n## Principal components calculations: possum[, 6:14] (DAAG)\nhere &lt;- complete.cases(possum[, 6:14])\npossum.prc &lt;- prcomp(log(possum[here, 6:14]))\nscores &lt;- cbind(predict(possum.prc), possum[here, c('sex', 'site')])\n\n\n## For parset, key and colr; see code for Fig 9.1\npchr &lt;- c(3,4,0,8,2,10,1)\nparset &lt;- list(fontsize=list(text=10, points=6), cex=0.75, pch=pchr, alpha=0.8)\nkey &lt;- modifyList(key, list(columns=1, space=\"right\"))\ngph &lt;- with(scores, xyplot(PC2 ~ PC1, aspect=\"iso\", key = key,\n            col = colr[sex], pch = (0:6)[site]))\nupdate(gph, scales=list(tck=0.5), par.settings=parset,\n       xlab=\"1st Principal Component\", ylab=\"2nd Principal Component\")\n\n\nprint(summary(possum.prc),digits=2)\ncat(\"\\nRotations (otherwise called Loadings)\\n\")\nprint(possum.prc$rotation, digits=2)\n## By default, blanks are shown for loadings &lt; 0.1 in magnitude\n\n\n\nThe stability of the principal components plot\n\nsuppressPackageStartupMessages(library(ggplot2))\ntheme_set(theme_gray(base_size=8))\n## Bootstrap principal components calculations: possum (DAAG)\n## Sample from rows where there are no missing values\nrowsfrom &lt;- (1:nrow(possum))[complete.cases(possum[, 6:14])]\nlogpossum6to14 &lt;- log(possum[rowsfrom, 6:14])\nsexPop &lt;- possum[rowsfrom, c(\"sex\",\"Pop\")]\nn &lt;- length(rowsfrom); ntimes &lt;- 3\nbootscores &lt;- data.frame(scores1=numeric(ntimes*n), scores2=numeric(ntimes*n))\nfor (i in 1:ntimes){\n  samprows &lt;- sample(1:n, n, replace=TRUE)\n  bootscores[n*(i-1)+(1:n), 1:2] &lt;-\n  prcomp(logpossum6to14[samprows, ])$x[, 1:2]\n}\nbootscores[, c(\"sex\",\"Pop\")] &lt;- sexPop[samprows, ]\nbootscores$sampleID &lt;- factor(rep(1:ntimes, rep(n,ntimes)))\ngph &lt;- quickplot(x=scores1, y=scores2, colour=sex, size=I(1.0),\n  asp=1, shape=Pop, facets=.~sampleID, data=bootscores) + \n  scale_shape_discrete(solid=F)\ngph + scale_colour_manual(values=c(\"m\"=\"blue\",\"f\"=\"red\"))  +\n  xlab(\"First Principal Component\") + ylab(\"Second Principal Component\")\n\n\n\n\nSubsection 9.1.3: Multi-dimensional scaling\n\nDistance measures\n\n\nOrdination\n\n## Code that will display individual graphs\nd.possum &lt;- dist(possum[,6:14])  # Euclidean distance matrix\nMASS::sammon(d.possum, k=2, trace=FALSE)$points |&gt; as.data.frame() |&gt;\n  setNames(paste0(\"ord\",1:2)) |&gt; cbind(Pop=DAAG::possum$Pop) -&gt; sammon.possum\nMASS::isoMDS(d.possum, k=2, trace=FALSE) |&gt; as.data.frame() |&gt; \n  setNames(paste0(\"ord\",1:2)) |&gt; cbind(Pop=DAAG::possum$Pop) -&gt; mds.possum\ngph1 &lt;- xyplot(ord2~ord1, groups=Pop, aspect=\"iso\", data=sammon.possum)\ngph2 &lt;- xyplot(ord2~ord1, groups=Pop, aspect=\"iso\", data=mds.possum)\nupdate(c(gph1, gph2, layout=c(2,1)), \n       par.settings=simpleTheme(pch=c(1,3)),\n       between=list(x=0.5), auto.key=list(columns=2),\n       strip=strip.custom(factor.levels=c(\"A: Sammon\",\"B: ISOmds\")))\n\n\n\nBinary data\n\n\n\n\nSection 9.2: Principal component scores in regression\n\n## Principal components: data frame socsupport (DAAG)\nsocsupport &lt;- DAAG::socsupport\nss.pr1 &lt;- prcomp(as.matrix(na.omit(socsupport[, 9:19])), retx=TRUE, scale=TRUE)\n\n\noldpar &lt;- par(fg='gray40',col.axis='gray20',lwd=0.5,col.lab='gray20')\npairs(ss.pr1$x[, 1:3], col='gray40', gap=0.2)\npar(oldpar)\n\n\nsummary(sort(ss.pr1$rotation[,1]))\n## Note the very large maximum value\nwhich.max(ss.pr1$x[,1])\n## Try also boxplot(ss.pr1$x[,1])\n## ss.pr1$x[\"36\",1]  ## Check that this returns 42\n\n\nuse &lt;- complete.cases(socsupport[, 9:19])\nuse[36] &lt;- FALSE\nss.pr &lt;- prcomp(as.matrix(socsupport[use, 9:19]))\n\n\n## Output from summary()\nprint(summary(ss.pr), digits=1)  # Compare contributions\n\n\ncomp &lt;- as.data.frame(ss.pr$x[,1:6])\nss.lm &lt;- lm(socsupport[use, \"BDI\"] ~ ., data=comp)\nsignif(round(coef(summary(ss.lm)),5), digits=3)\n\n\nprint(ss.pr$rotation[, 1], digits=2)\n\n\n## Plot BDI against first principal component score\ngph &lt;- xyplot(BDI ~ ss.pr$x[ ,1], groups=gender, data=socsupport[use,],\npar.settings=simpleTheme(pch=1:2), auto.key=list(columns=2))\nbw9 &lt;- list(pch=c(1,3), list(text=9, points=5))\nupdate(gph, scales=list(tck=0.5), par.settings=bw9,\nxlab =\"1st principal component\")\n\n\n\nSection 9.3: Cluster analysis\n\nSubsection 9.3.1: Hierarchical Clustering\n\nlibrary(mvtnorm)\nmakeClust &lt;- function(n=6, d1=4, d2=4, sigs=c(1, 1, 1, 1), seed=NULL){\n  if(!is.null(seed))set.seed(seed)\n  g1 &lt;- rmvnorm(n, mean = c(-d1,d2), sigma=sigs[1]*diag(2))\n  g2 &lt;- rmvnorm(n, mean = c(d1,d2), sigma=sigs[2]*diag(2))\n  g3 &lt;- rmvnorm(n, mean = c(-d1,-d2), sigma=sigs[3]*diag(2))\n  g4 &lt;- rmvnorm(n, mean = c(d1,-d2), sigma=sigs[4]*diag(2))\n  rbind(g1,g2,g3,g4)\n}\n\n\n## Code for the plots\ndatA &lt;- makeClust(seed=35151)\ndatB &lt;- makeClust(d2=16, seed=35151)\ndatC &lt;- makeClust(d1=2,d2=2, seed=35151)\nplot(datA, xlab=\"X1\", ylab=\"X2\", fg=\"gray\")  \ntitle(main=\"A: 4blobsA\", adj=0, line=0.5, font.main=1)\n## Repeat previous two lines for datB and datC\nplot(datB, xlab=\"X1\", ylab=\"X2\", fg=\"gray\")  \ntitle(main=\"B: 4blobsB\", adj=0, line=0.5, font.main=1)\nplot(datC, xlab=\"X1\", ylab=\"X2\", fg=\"gray\")  \ntitle(main=\"C: 4blobsC\", adj=0, line=0.5, font.main=1)\n\n\n## Possible alternative\nconfig &lt;- c('Equidistant blobs', 'Pulled vertically', 'Closer centers')\ndat123 &lt;- cbind(as.data.frame(rbind(datA, datB, datC)), \n                              gp=factor(rep(1:3, rep(6*4,3)), labels=config))\nxyplot(V2 ~ V1 | gp, data=dat123, scales=list(relation='free'),\n       strip=strip.custom(factor.levels=config), between=list(x=0.5),\n       par.settings=DAAG::DAAGtheme(color=F))\n\n\n## Code for single linkage plots: `?plot.hclust` gives help for the plot method\nclusres_sing &lt;- hclust(dist(datA), method=\"single\")\npar(fig=c(0,0.75,0,1))\nplot(clusres_sing, sub=\"\", xlab=\"\", ylab=\"Distance joined\", adj=0.5, \n     main=\"\", fg=\"gray\")\nmtext('A: Single linkage cluster dendrogram, for 4blobsA layout', side=3, adj=0, \n      font=1, line=1, cex=1.15)\npar(fig=c(0.72,1,0,1), new=TRUE)\nmembs &lt;- cutree(clusres_sing, 4)\ncol4= RColorBrewer::brewer.pal(4,'Set1')\nplot(datA, xlab=\"X1\", ylab=\"X2\", col=col4[membs], fg='gray', pch=membs+1)\nmtext('B: 4blobsA, by color', side=3, adj=1.0, font=1, cex=1.15, line=1)\n## To see plots from 'average' and 'complete' linkage methods,do:\n# plot(hclust(dist(datB), method=\"average\"))\n# plot(hclust(dist(datC), method=\"complete\"))\n\n\n## Dendrograms from data where blobs were pulled vertically\n## Follow each use of `hclust()` with a `plot()` command\nsclusres_sing &lt;- hclust(dist(datB), method=\"single\")\nplot(sclusres_sing, sub=\"\", xlab=\"\", ylab=\"Distance Joined\", main=\"\")\ntitle(main='A: Single linkage, blobs pulled vertically (4blobsB)', \n      adj=0, font.main=1)\nsclusres_sing_s &lt;- hclust(dist(scale(datA)), method=\"single\")\nplot(sclusres_sing_s, sub=\"\", xlab=\"\", ylab=\"Distance Joined\", main=\"\")\ntitle(main='B: Single linkage, (4blobsB, rescaled to variance 1)', \n      adj=0, font.main=1)\n# sclusres_avg_s &lt;- hclust(dist(scale(datB)), method=\"average\")\n# #plot(sclusres_avg_s, sub=\"\", xlab=\"\", ylab=\"\")\n# sclusres_comp_s &lt;- hclust(dist(scale(datB)), method=\"complete\")\n# #plot(sclusres_comp_s, sub=\"\", xlab=\"\", ylab=\"\")\n\n\n## Code. Follow each use of `hclust()` with a `plot()` command\nclusres_sing2 &lt;- hclust(dist(datC), method=\"single\")\nplot(clusres_sing2, sub=\"\", xlab=\"\", ylab=\"\", cex=1.25, cex.main=1.65,\n     main=\"A: Single linkage, closer clusters (4blobsC)\", adj=0, font.main=1)\nclusres_avg2 &lt;- hclust(dist(datC), method=\"average\")\nplot(clusres_avg2, sub=\"\", xlab=\"\", ylab=\"\", cex=1.25, cex.main=1.65,\n     main=\"B: Average linkage, closer clusters (4blobsC)\", adj=0, font.main=1)\nclusres_comp2 &lt;- hclust(dist(datC), method=\"complete\")\nplot(clusres_comp2, sub=\"\", xlab=\"\", ylab=\"\", cex=1.25, cex.main=1.65, \n     main=\"C: Complete linkage, closer clusters (4blobsC)\", adj=0, font.main=1)\n\n\n\nSubsection 9.3.2: \\(k\\)-Means Clustering\n\nset.seed(35151)\nkdat &lt;- makeClust(n=100, d1=5, d2=5, sigs=c(.5, .5, 6, 6))\nplot(kdat, xlab=\"X1\", ylab=\"X2\", fg=\"gray\")\nkmres &lt;- kmeans(kdat, 4, nstart=30)\nplot(kdat, col=rainbow(4)[kmres$cluster], pch=kmres$cluster+1, \n     xlab=\"X1\", ylab=\"X2\", fg=\"gray\")\n\n\nComments on \\(k\\)-means and hierarchical clustering\n\n\n\nSubsection 9.3.3: Mixture model-based clustering\n\n## Code\nplotMix2 &lt;- function(taus=c(.5, .5), means=c(10,15), sds=c(3,1), xlims=c(0,20)){\n  curve(taus[1]*dnorm(x, mean=means[1], sd=sds[1]) + \n          taus[2]*dnorm(x, mean=means[2], sd=sds[2]), \n        from=xlims[1], to=xlims[2], ylab=\"Density\", fg=\"gray\")\n  curve(taus[1]*dnorm(x, mean=means[1], sd=sds[1]), \n        from=xlims[1], to=xlims[2], col=\"red\", lty=2, add=TRUE, fg=\"gray\")\n  curve(taus[2]*dnorm(x, mean=means[2], sd=sds[2]), \n        from=xlims[1], to=xlims[2], col=\"blue\", lty=3, add=TRUE, fg=\"gray\")\n}\nplotMix2(taus=c(.2, .8))\nplotMix2(taus=c(.5, .5))\nplotMix2(taus=c(.9, .1))\n\n\nlibrary(teigen)\npossml &lt;- na.omit(DAAG::possum[,c(3,9:11)])\nset.seed(513451)\ngaus_fit &lt;- teigen(possml[,2:4], models=\"UUUU\", gauss=TRUE, verbose=FALSE, \n                    scale=FALSE)\n\n\n## BIC values are plotted against number of groups\ngaus_fit$allbic\nplot(gaus_fit$allbic, type=\"b\", ylab=\"\", xlab=\"Number of Groups\", fg=\"gray\")\nmtext(side=2, line=3.5, \"BIC\", las=0)\naxis(1, at=1:9, fg=\"gray\")\n\n\ntable(possml$Pop, gaus_fit$classification)\n\n\npar(fig=c(0, 0.5, 0.5, 1))\nplot(gaus_fit, what=\"contour\", xmarg=1, ymarg=2, draw.legend=FALSE, fg=\"gray\")\n## See ?teigen::plot.teigen for details of the plot command used here.\npar(fig=c(0, 0.5, 0, 0.5), new=TRUE)\nplot(gaus_fit, what=\"contour\", xmarg=1, ymarg=3, draw.legend=FALSE, fg=\"gray\")\npar(fig=c(0.5, 1, 0, 0.5), new=TRUE)\nplot(gaus_fit, what=\"contour\", xmarg=2, ymarg=3, draw.legend=FALSE, fg=\"gray\")\npar(fig=c(0,1,0,1))\n\n\nIssues of high parametrization and scaling\n\n\n\nSubsection 9.3.4: Relationship between \\(k\\)-means and mixture models\n\n\n\nSection 9.4: Discriminant analysis\n\nSubsection 9.4.1: Example – plant architecture\n\nleafshape17 &lt;- DAAG::leafshape17\nplot(bladelen ~ bladewid, data=leafshape17, pch=c(1,3)[arch+1])\n## For panel B, specify log=\"xy\" in the call to plot()\n\n\nLogistic regression, versus linear discriminant analysis\n\n\n\nSubsection 9.4.2: Logistic regression\n\n## Fit logistic regression model\nleafshape17 &lt;- DAAG::leafshape17\nleaf17.glm &lt;- glm(arch ~ logwid + loglen, family=binomial(link=logit),\ndata=leafshape17)\nprint(DAAG::sumry(leaf17.glm)$coef, digits=2)\n\n\nPredictive accuracy\n\nset.seed(29)\nleaf17.cv &lt;- DAAG::CVbinary(leaf17.glm)\ntCV &lt;- table(DAAG::leafshape17[[\"arch\"]], round(leaf17.cv$cvhat))\nrownames(tCV) &lt;- colnames(tCV) &lt;- c(\"0=Plagiotropic\",\"1=Orthotropic\")\ncbind(tCV, \"Proportion correct\"=c(tCV[1,1], tCV[2,2])/(tCV[,1]+tCV[,2]))\n\n\nround(unlist(leaf17.cv[c(\"acc.training\",\"acc.cv\")]),3)\n\n\n\n\nSubsection 9.4.3: Linear discriminant analysis\n\nsuppressPackageStartupMessages(library(MASS))\n## Discriminant analysis; data frame leafshape17 (DAAG)\nleaf17.lda &lt;- lda(arch ~ logwid+loglen, data=DAAG::leafshape17)\nprint(leaf17.lda)\n\n\nAssessments of predictive accuracy\n\nset.seed(29)\nleaf17cv.lda &lt;- lda(arch ~ logwid+loglen, data=leafshape17, CV=TRUE)\n## the list element 'class' gives the predicted class\n## The list element 'posterior' holds posterior probabilities\ntab &lt;- table(leafshape17$arch, leaf17cv.lda$class)\nrownames(tab) &lt;- colnames(tab) &lt;- c(\"0=Plagiotropic\",\"1=Orthotropic\")\ncbind(tab, \"Proportion correct\"=c(tCV[1,1], tCV[2,2])/(tCV[,1]+tCV[,2]))\ncbind(tab, c(tab[1,1], class.acc=tab[2,2])/(tab[,1]+tab[,2]))\ncat(\"Overall proportion correct =\", sum(tab[row(tab)==col(tab)])/sum(tab), \"\\n\")\n\n\n\nThe function qda(), and other alternatives to lda()\n\n\n\nSubsection 9.4.4: An example with more than two groups\n\n## Linear discriminant calculations for possum data\npossum &lt;- DAAG::possum\npossum.lda &lt;- lda(site ~ hdlngth + skullw + totlngth + taill + footlgth +\n                  earconch + eye + chest + belly, data=na.omit(possum))\n# na.omit() omits any rows that have one or more missing values\n\n\nplot(possum.lda, dimen=3, col=1:9)\n# Scatterplot matrix - scores on 1st 3 canonical variates\n# See `?plot.lda` for details of the generic lda plot function\n\n\n## Linear discriminant calculations for possum data\nprint(possum.lda, digits=3)\n\n\n\n\nSection 9.5: *High-dimensional data — RNA-Seq gene expression\n\nSetup for installing and using Bioconductor packages\n\n## For latest details, see: https://www.bioconductor.org/install/\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install()\nBiocManager::install('limma','multtest')\n\n\n\n*Brief note on mRNA technical issues\n\n\nSubsection 9.5.1: Data and design matrix setup\n\ncounts &lt;- DAAGbio::plantStressCounts\ncolSums(counts)\n\n\n## Require at least 3 counts per million that are &gt; 1\nkeep &lt;- rowSums(counts)&gt;=3\ncounts &lt;- counts[keep,]\n\n\ntreatment &lt;- factor(rep(c(\"CTL\", \"L\", \"D\"), rep(3,3)))\ndesign &lt;- model.matrix(~0+treatment)\ncolnames(design) &lt;- levels(treatment)\n\n\nA two-dimensional representation\n\nlibrary(limma)\nv &lt;- voom(counts, design, plot=TRUE)\n\n\npar(oma=c(0,0,1,0))\nlibrary(limma)\nv &lt;- voom(counts, design, plot=TRUE)\nfirstchar &lt;- substring(colnames(counts),1,1)\nplotMDS(counts, labels=paste0(firstchar, rep(1:3,3)), cex=0.8)\nbox(col=\"gray\")\nmtext(side=3, line=0.4, adj=0, \"MDS summary plot\")\nmtext(side=3, line=-0.25, adj=0.105, \"A\", outer=TRUE)\nmtext(side=3, line=-0.25, adj=0.605, \"B\", outer=TRUE)\n\n\n\nFitting the model\n\nfit &lt;- lmFit(v, design)\n\n\ncontrs &lt;- c(\"D-CTL\", \"L-CTL\", \"L-D\")\ncontr.matrix &lt;- makeContrasts(contrasts=contrs,\nlevels=levels(treatment))\nfit2 &lt;- contrasts.fit(fit, contr.matrix)\nefit2 &lt;- eBayes(fit2)\n\n\n\n\nSubsection 9.5.2: From \\(p\\)-values to false discovery rate (FDR)\n\n## First contrast only; Drought-CTL\nprint(round(topTable(efit2, coef=1, number=4),15), digits=3)\n\n\nround(sort(p.adjust(p=efit2$p.value[,1], method=\"BH\"))[1:4], 15) # Not run\n\n\nround(topTable(efit2, number=4), 15)\n\n\nhead(decideTests(fit2),5)\n\n\nsummary(decideTests(fit2))\n## Try also\n## summary(decideTests(fit2, p.value=0.001))\n\n\n\n\nSection 9.6: High dimensional data from expression arrays\n\nSubsection 9.6.1: Molecular classification of cancer — an older technology\n\nBreakdown of ALL B-type data, with one observation excluded\n\nlibrary(hddplot)\ndata(golubInfo)\nwith(golubInfo, table(cancer, tissue.mf))\n\n\n## Identify allB samples that are BM:f or BM:m or PB:m\nsubsetB &lt;- with(golubInfo,\ncancer==\"allB\" & tissue.mf%in%c(\"BM:f\",\"BM:m\",\"PB:m\"))\n## Separate off the relevant columns of the matrix Golub\n## NB: variables (rows) by cases (columns)\nGolubB &lt;- with(golubInfo, Golub[, subsetB])\n## Form vector that identifies these as BM:f or BM:m or PB:m\ntissue.mfB &lt;- with(golubInfo, tissue.mf[subsetB, drop=TRUE])\n## Change the level names to leave out the colons\nlevels(tissue.mfB) &lt;- list(\"b_f\"=\"BM:f\", \"b_m\"=\"BM:m\", \"PBm\"=\"PB:m\")\n\n\n\n\nSubsection 9.6.2: Classifications and associated graphs\n\nPreliminary data manipulation\n\n## Display distributions for the first 20 observations\nboxplot(data.frame(GolubB[, 1:20]))  # First 20 columns (observations)\n## Random selection of 20 rows (features)\nboxplot(data.frame(GolubB[sample(1:7129, 20), ]))\n\n\n\nFlawed graphs\n\ncolr &lt;- c(\"red\",\"blue\",\"gray40\", \"magenta\")\ntissue.mf &lt;- golubInfo[, \"tissue.mf\"]\ncancer &lt;- golubInfo[, \"cancer\"]\nG.PBf &lt;- Golub[, tissue.mf==\"PB:f\" & cancer==\"allB\", drop=FALSE]\nset.seed(41)\nrGolubB &lt;- matrix(rnorm(prod(dim(GolubB))), nrow=dim(GolubB)[1])\nrownames(rGolubB) &lt;- rownames(Golub)\nrG.PBf &lt;- matrix(rnorm(prod(dim(G.PBf))), nrow=dim(G.PBf)[1])\nplot2 &lt;- function(x = GolubB, cl=tissue.mfB, x.omit=Golub.PBf, cl.omit=\"PBf\", \n                  ncol = length(cl), nfeatures=12, device = \"\", seed = 37,\n                  pretext=\"\", colr=1:3, levnames = NULL,\n                  ylab=\"2nd discriminant function\"){\n  cl &lt;- factor(cl)\n  if(!is.null(levnames))levels(cl) &lt;- levnames\n  ord15 &lt;- orderFeatures(x, cl=cl)[1:nfeatures]\n  dfB &lt;- t(x[ord15, ])\n  dfB.lda &lt;-  lda(dfB, grouping=cl)\n  scores &lt;- predict(dfB.lda, dimen=2)$x\n  df.PBf &lt;- data.frame(t(x.omit[ord15, drop=FALSE]))\n  colnames(df.PBf) &lt;- colnames(dfB)\n  scores.other &lt;- predict(dfB.lda, newdata=df.PBf)$x\n  scoreplot(list(scores=scores, cl=cl, other=scores.other, cl.other=cl.omit,                       nfeatures=nfeatures), prefix.title=pretext, adj.title=0, \n                 fg=\"gray\", params=list(other=list(pch=4, cex=1.5)),\n            xlab=\"1st discriminant function\", ylab=ylab)\n}\nplot2(x = GolubB, cl = tissue.mfB, x.omit=G.PBf, cl.omit=\"PBf\",\n      nfeatures=15, device = \"\", seed = 37, ylab=\"2nd discriminant function\",\n      colr=colr, pretext=\"A: ALL B-cell:\")\nplot2(x = rGolubB, cl = tissue.mfB, x.omit=rG.PBf, cl.omit=\"Gp 4\", \n     device = \"\", seed = 37, colr=colr, levnames = c(\"Gp 1\", \"Gp 2\", \"Gp 3\"),\n     pretext=\"B: Random data:\", ylab=\"\")\n\n\n## Uses orderFeatures() (hddplot); see below\nord15 &lt;- orderFeatures(GolubB, cl=tissue.mfB)[1:15]\n\n\n## Panel A: Take 1st 15 features & transpose to observations by features\ndfB15 &lt;- data.frame(t(GolubB[ord15, ]))\ndfB15.lda &lt;-  MASS::lda(dfB15, grouping=tissue.mfB)\nscores &lt;- predict(dfB15.lda, dimen=2)$x\n## Scores for the single PB:f observation\nchooseCols &lt;- with(golubInfo, tissue.mf==\"PB:f\"& cancer==\"allB\")\ndf.PBf &lt;- data.frame(t(Golub[ord15, chooseCols, drop=FALSE]))\nscores.PBf &lt;- predict(dfB15.lda, newdata=df.PBf, dimen=2)$x\n## Warning! The plot that now follows may be misleading!\n## Use hddplot::scoreplot()\nscoreplot(list(scores=scores, cl=tissue.mfB, other=scores.PBf, cl.other=\"PB:f\"),\n          fg=\"gray\")\n\n\n## Panel B: Repeat plot, now with random normal data\nsimscores &lt;- simulateScores(nrow=7129, cl=rep(1:3, c(19,10,2)),\ncl.other=4, nfeatures=15, seed=41)\n# Returns list elements: scores, cl, scores.other & cl.other\nscoreplot(simscores)\n\n\n\n\nSubsection 9.6.3: The mean-variance relationship\n\npar(oma=c(0,0,1,0))\ndesignG &lt;- model.matrix(~0+tissue.mfB)\ncolnames(designG) &lt;- levels(tissue.mfB)\nvG &lt;- vooma(GolubB, designG, plot=TRUE)         # Panel A\nplotMDS(vG, pch=unclass(tissue.mfB), cex=0.8)   # Panel B\nleglabs &lt;- c(\"BM:female\",\"BM:male\",\"PB:female\")\nlegend(x=\"bottomright\", bty=\"n\", legend=leglabs, pch=1:3)\nmtext(side=3, line=0.4, adj=0, \"MDS summary plot\")\nmtext(side=3, line=-0.275, adj=0.085, \"A\", outer=TRUE)\nmtext(side=3, line=-0.275, adj=0.585, \"B\", outer=TRUE)\n\n\nCross-validation to determine the optimum number of features\n\n\nCross-validation for a range of choices of number of features\n\n##  Cross-validation to determine the optimum number of features\n## 10-fold (x4). Warning messages are omitted.\n## Accuracy measure will be: tissue.mfB.cv$acc.cv\ntissue.mfB.cv &lt;- cvdisc(GolubB, cl=tissue.mfB, nfeatures=1:23,\nnfold=c(10,4), print.progress=FALSE)\n## Defective measures will be in acc.resub (resubstitution)\n## and acc.sel1 (select features prior to cross-validation)\ntissue.mfB.badcv &lt;- defectiveCVdisc(GolubB, cl=tissue.mfB,\nfoldids=tissue.mfB.cv$folds, nfeatures=1:23)\n##\n## Calculations for random normal data:\nset.seed(43)\nrGolubB &lt;- matrix(rnorm(prod(dim(GolubB))), nrow=nrow(GolubB))\nrtissue.mfB.cv &lt;- cvdisc(rGolubB, cl=tissue.mfB, nfeatures=1:23,\nnfold=c(10,4), print.progress=FALSE)\nrtissue.mfB.badcv &lt;- defectiveCVdisc(rGolubB, cl=tissue.mfB,\nnfeatures=1:23,\nfoldids=rtissue.mfB.cv$folds)\n\n\n\nWhich features?\n\ngenelist &lt;- matrix(tissue.mfB.cv$genelist[1:3, ,], nrow=3)\ntab &lt;- table(genelist, row(genelist))\nord &lt;- order(tab[,1], tab[,2], tab[,3], decreasing=TRUE)\ntab[ord,]\n\n\n\n\nSubsection 9.6.4: Graphs derived from the cross-validation process\n\n## Uses tissue.mfB.acc from above\ntissue.mfB.scores &lt;-\ncvscores(cvlist = tissue.mfB.cv, nfeatures = 3, cl.other = NULL)\nscoreplot(scorelist = tissue.mfB.scores, cl.circle=NULL,\nprefix=\"B-cell subset -\", fg='gray')\n\n\nThe key role of cross-validation\n\n\n\nSubsection 9.6.5: Estimating contrasts, and calculating False Discovery Rates\n\nfitG &lt;- lmFit(vG, designG)\ncontrs &lt;- c(\"b_f-b_m\", \"b_f-PBm\", \"b_m-PBm\")\ncontr.matrix &lt;- makeContrasts(contrasts=contrs,\nlevels=levels(tissue.mfB))\nfit2 &lt;- contrasts.fit(fitG, contr.matrix)\nfit2 &lt;- eBayes(fit2)\n\n\nFrom \\(p\\)-values to false discovery rate (FDR)\n\nprint(topTable(fit2, number=5), digits=2)\n\n\nsummary(decideTests(fit2))\n## Try also\n## summary(decideTests(fit2, p.value=0.001))\n\n\n\nDistributional extremes\n\n\n\n\nSection 9.7: Causal inference from observational data — balance and matching\n\nSubsection 9.7.1: Tools for the task\n\nlibrary(DAAG)\n## Columns 4:7 are factors; columns 9:10 (re75 & re78) are continuous\npropmat &lt;- matrix(0, ncol=6, nrow=8)\ndimnames(propmat) &lt;- list(c(\"psid1\", \"psid2\", \"psid3\", \"cps1\", \"cps2\", \"cps3\",\n\"nsw-ctl\", \"nsw-trt\"), names(nswdemo)[c(4:7, 9:10)])\nfor(k in 1:8){\n  dframe &lt;- switch(k, psid1, psid2, psid3, cps1, cps2, cps3,\n  subset(nswdemo, trt==0), subset(nswdemo, trt==1))\n  propmat[k,] &lt;- c(sapply(dframe[,4:7], function(x){\n                   z &lt;- table(x); z[2]/sum(z)}),\n                   sapply(dframe[,9:10], function(x)sum(x&gt;0)/sum(!is.na(x))))\n}\n\n\nPGtheme &lt;- DAAG::DAAGtheme(color=TRUE)\nlibrary(DAAG)\nif(!require(grid))return(\"Package 'grid' is not installed -- cannot proceed\")\ndsetnames &lt;- c(\"nsw-ctl\", \"nsw-trt\", \"psid1\", \"psid2\", \"psid3\",\n               \"cps1\", \"cps2\", \"cps3\")\ncolrs &lt;- c(\"gray\",\"black\", PGtheme$superpose.line$col[1:3])\nlty &lt;- c(1,2,1,1,1)\nlwd &lt;- c(1,0.75,0.75,0.75,0.75)\ndenplot &lt;-\n  function(sel=c(1:2,6:8), yvar=\"re75\", offset=30, ylim=c(0,1.75),\n    from=NULL, at=c(.5,1,1.5), labels=paste(at), bw=\"nrd0\",\n    ylab=\"Density\", takelog=TRUE, col.axis=\"black\"){\n      nzre &lt;- unlist(lapply(list(subset(nswdemo, trt==0),\n                                 subset(nswdemo, trt==1),\n                                 psid1, psid2, psid3, cps1, cps2, cps3)[sel],\n                            function(x){z &lt;- x[,yvar]; z[z&gt;0]}))\nnum &lt;- unlist(lapply(list(subset(nswdemo, trt==0), subset(nswdemo, trt==1),\n                          psid1, psid2, psid3, cps1, cps2, cps3),\n                     function(x){z &lt;- x[,yvar]; sum(z&gt;0)}))\nxy &lt;- data.frame(nzre=nzre, fac = factor(rep(dsetnames[sel], num[sel]),\n                 levels=dsetnames[sel]))\nif(takelog) {\ny &lt;- log(xy$nzre+offset)\nxlab &lt;- paste(\"log(\", yvar, \"+\", offset, \")\", sep=\"\")} else \n  {\n  y &lt;- xy$nzre\n  xlab &lt;- yvar\n}\ndensityplot(~ y, groups=fac, data=xy, bw=bw, from=from,\n  scales=list(y=list(at=at, labels=labels, col=col.axis), tck=0.25),\n  plot.points=FALSE, col=colrs[1:5], lwd=lwd, lty=lty, \n  key=list(x=0.01, y=0.99, text=list(dsetnames[sel[3:5]]), col=colrs[3:5],\n           cex=0.75, lines=list(lwd=rep(1.5,3)), between=1),\n  par.settings=list(col=colrs, lty=lty, cex=0.75, lwd=lwd, \n                    fontsize=list(text=9, points=5)),\n  fg=\"gray\", ylim=ylim, ylab=ylab, xlab=xlab)\n}\n## Plot base graph; overlay with lattice graphs on same page\npar(fig=c(0,1,0,1), mar=c(0,0,0,0))\nplot(0:1,0:1, axes=FALSE, type=\"n\", bty=\"n\", xlab=\"\", ylab=\"\")\nlegend(x=\"top\",legend=dsetnames[1:2], lty=1:2, lwd=c(1,0.75),\n       col=colrs[1:2], bty=\"n\", ncol=2, yjust=0.75)\nprint(denplot(), position=c(0, 0, 0.32, 0.505), newpage=FALSE)\nprint(denplot(1:5, ylab=\" \", col.axis=\"white\"),\n      position=c(0.21, 0, .53, 0.505), newpage=FALSE)\nprint(denplot(ylab=\" \", yvar=\"re78\", col.axis=\"white\"),\n      position=c(0.47, 0, 0.79, 0.505), newpage=FALSE)\nprint(denplot(1:5, ylab=\" \", yvar=\"re78\", col.axis=\"white\"),\n      position=c(0.68, 0, 1, 0.505), newpage=FALSE)\n## Age\nprint(denplot(yvar=\"age\", takelog=FALSE, ylim=c(0,0.1), from=16,\n      at=c(.02,.04,.06,.08), labels=c(\".02\",\".04\",\".06\",\".08\")),\n      position=c(0, 0.475, 0.32, .98), newpage=FALSE)\nprint(denplot(1:5, yvar=\"age\", takelog=FALSE, ylim=c(0,0.1), from=16,\n      at=c(.02,.04,.06,.08), labels=c(\".02\",\".04\",\".06\",\".08\"),\n      ylab=\" \", col.axis=\"white\"),\n      position=c(0.21, 0.475, .53, .98), newpage=FALSE)\n## educ\nprint(denplot(1:5, yvar=\"educ\", takelog=FALSE, ylim=c(0,0.5), bw=0.5,\n      at=c(.1,.2,.3,.4), ylab=\" \"),\n      position=c(0.47, 0.475, .79, .98), newpage=FALSE)\nprint(denplot(yvar=\"educ\", takelog=FALSE, ylim=c(0,0.75), bw=0.5,\n      at=c(.1,.2,.3,.4), ylab=\" \", col.axis=\"white\"),\n      position=c(0.68, 0.475, 1, .98), newpage=FALSE)\n\n\naddControl &lt;-\nfunction(control, offset=30){\n  nam &lt;- deparse(substitute(control))\n  if(nam==\"nswdemo\")nsw0 &lt;- nswdemo else\n    nsw0 &lt;- rbind(control, subset(DAAG::nswdemo, trt==1))\n  nsw0$z75 &lt;- factor(nsw0$re75==0, labels=c(\"0\",\"&gt;0\"))\n  nsw0$ethnicid &lt;- factor(with(nsw0, ifelse(black==1, \"black\",\n    ifelse(hisp==1, \"hisp\", \"other\"))), levels=c(\"other\",\"black\",\"hisp\"))\nnsw0 &lt;- nsw0[, -match(c(\"black\",\"hisp\"), names(nsw0))]\nnsw0\n}\n\n\n## Create dataset that will be used for later analyses\nnsw &lt;- addControl(psid1)\nnsw &lt;- within(nsw, {re75log &lt;- log(re75+30);\n                    re78log &lt;- log(re78+30);\n                    trt &lt;- factor(trt, labels=c(\"Control\",\"Treat\"))})\n## A treated values only dataset will be required below\ntrtdat &lt;- subset(nsw, trt==\"Treat\")\ntrtdat$pres74 &lt;- factor(!is.na(trtdat$re74), labels=c(\"&lt;NA&gt;\",\"pres\"))\ntable(trtdat$pres74)\n\n\nwith(trtdat, table(pres74,z75))\n\n\n\nSubsection 9.7.2: Regression comparisons\n\nRegression calculations\n\nnsw.gam &lt;- gam(log(re78+30)~ trt + ethnicid + z75 + nodeg + s(age) +\n               s(educ) + log(re75+30), data=nsw)\n\n\n\n\nSubsection 9.7.3: The use of scores to replace covariates\n\n\nSubsection 9.7.4: Two-dimensional representation using randomForest proximities\n\nsuppressPackageStartupMessages(library(randomForest))\nform &lt;- trt ~ age + educ + ethnicid + marr + nodeg + z75 + re75log\nnsw.rf &lt;- randomForest(form, data=nsw, sampsize=c(297,297))\np.rf &lt;- predict(nsw.rf,type=\"prob\")[,2]\nsc.rf &lt;- log((p.rf+0.001)/(1-p.rf+0.001))\n\n\nomitn &lt;- match(c(\"PropScore\",\"weights\",\"subclass\"), names(dat2RF), nomatch=0)\nmatchISO.rf &lt;-matchit(trt ~ age + educ + ethnicid + marr + nodeg + z75 +\n                      re75log, ratio=1, data=dat2RF[,-omitn], distance=isoScores[,1])\n## summary(match.rf,un=F,improvement=F)\n## summary(match.rf, un=F, interactions=T, improvement=F)$sum.matched[,1:4]\n## In the first place, look only at the first 4 columns\n\n\ndat1RF &lt;- match.data(matchISO.rf, distance=\"PropScore\")\ndat1RF.lm &lt;- lm(re78log ~ trt, data = dat1RF, weights = weights)\nlibrary(sandwich)   # Allows use of `vcovCL()` from the `sandwich` package\nlmtest::coeftest(dat1RF.lm, vcov. = vcovCL, cluster = ~subclass)\n## Check for increase in number with non-zero earnings\ndat1RF.glm &lt;- glm(I(re78&gt;0) ~ trt, data = dat1RF, weights = weights, \n                  family=binomial)\nlmtest::coeftest(dat1RF.glm, vcov. = vcovCL, cluster = ~subclass)\n\n\nDerivation and investigation of scores\n\nlibrary(mgcv)\nformG &lt;- trt ~ ethnicid + marr+ z75 + s(age) + s(educ) + s(re75log)\nnsw.gam &lt;- gam(formG, family=binomial(link=\"logit\"), data=nsw)\npred &lt;- predict(nsw.gam, type='response')\ntable(nsw$trt, round(pred))\n## Alternative\nlibrary(splines)  ## Fit normal cubic splines using splines::ns()\nformNS &lt;- trt ~ ethnicid + marr+ z75 + ns(age,2) +\nns(educ) + ns(re75log,3)\nnsw.glm &lt;- glm(formNS, family=binomial(link=\"logit\"), data=nsw)\npred &lt;- predict(nsw.glm, type='response')\ntable(nsw$trt, round(pred))\ncbind(AIC(nsw.glm,nsw.gam), BIC(nsw.glm, nsw.gam))\n\n\n## Include factor by factor and variable interactions with ethnicid\n## and marr (Result not shown)\nformGx &lt;- trt ~ (ethnicid+marr+z75)^2 + s(age, by=ethnicid)+\n                s(educ, by=ethnicid) + s(re75log,by=ethnicid)+\n                s(age, by=marr)+ s(educ, by=marr) + s(re75log,by=marr)\nnswx.gam &lt;- gam(formula = formGx, data = nsw, family=binomial(link = \"logit\"))\npredx &lt;- predict(nswx.gam, type='response')\ntable(nsw$trt, round(predx))\nAIC(nsw.glm,nsw.gam,nswx.gam)\n\n\nlibrary(MatchIt)\n## Use data frame that omits re74. Otherwise matchit() will generate NAs\n## where they occur in re74, even though re74 is not in the model formula.\nnswG &lt;- nsw[, c(\"trt\",\"age\",\"educ\",\"ethnicid\", \"marr\",\"nodeg\",\"z75\",\n                \"re75log\",\"re78log\",\"re78\")]\nformG &lt;- trt ~ ethnicid + marr+ z75 + s(age) + s(educ) + s(re75log)\nmatch.gam &lt;- matchit(formula = formG, data = nswG, method = \"nearest\",\n                     distance = \"gam\", link = \"logit\", reestimate=TRUE)\ndatG &lt;- match.data(match.gam, distance=\"PropScore\")\n## Summary information\nmatch.gam\n## summary(match.gam,un=F,improvement=F)\n## summary(match.gam, un=F, interactions=T, improvement=F)$sum.matched[,1:4]\n## In the first place, look only at the first 4 columns\n\n\nsuppressPackageStartupMessages(library(gridExtra))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(cobalt))\ngg1&lt;- cobalt::love.plot(match.gam, position=\"bottom\", grid=TRUE,\n                        star.char=\"\",stars='raw') +\n  ggtitle(\"A: Differences from balance\") +\n  theme(plot.title = element_text(hjust=0, vjust=0.5, size=11),\n        plot.margin=unit(c(9,15,0,9), 'pt'))\nsub &lt;- match(with(subset(datG, trt==\"Control\"),subclass),\n             with(subset(datG, trt==\"Treat\"),subclass))\ndatGpaired &lt;- cbind(subset(datG, trt==\"Treat\"),\n                    with(subset(datG, trt==\"Control\")[sub,],\n                    cbind(\"Cre78log\"=re78log,\"CPropScore\"=PropScore)))\ngg2 &lt;- ggplot(datGpaired)+\n  geom_point(aes(PropScore,I(re78log-Cre78log)), size=1)+\n  geom_smooth(aes(PropScore,I(re78log-Cre78log)), method = \"gam\", \n              formula = y ~ s(x, bs = \"cs\")) +\n  xlab(\"Propensity score for treated\")+\n  ylab(\"Treatment vs control differences\") +\n  ggtitle(\"B: Treatment vs control differences\") +\n  theme(plot.title = element_text(hjust=0, vjust=0.5, size=11),\n        plot.margin=unit(c(9,9,0,15), 'pt'))\ngrid.arrange(gg1, gg2, ncol=2) \n\n\nlibrary(sandwich)\ndatG.lm &lt;- lm(re78log ~ trt, data = datG, weights = weights)\n## With 1:1 matching, the weights argument is not really needed\n## Print first two coefficients only. \nlmtest::coeftest(datG.lm, vcov. = vcovCL, cluster = ~subclass)[1:2,]\n## Check number whose income was greater than 0\ndatG.glm &lt;- glm(I(re78&gt;0) ~ trt, data = datG, weights = weights, family=binomial)\nlmtest::coeftest(datG.glm, vcov. = vcovCL, cluster = ~subclass)[1:2,]\n\n\n\nAlternative matching approaches\n\n\n\nSubsection 9.7.5: Coarsened exact matching\n\nform &lt;- trt ~ age + educ + ethnicid + marr + nodeg + z75 + re75log\nmatch5.cem &lt;- matchit(formula=form, data=nswG, method=\"cem\", cutpoints=5)\ndatcem5 &lt;- match.data(match5.cem)\nmatch6.cem &lt;- matchit(formula=form, data=nswG, method=\"cem\", cutpoints=6)\ndatcem6 &lt;- match.data(match6.cem)\n## Show the effect of adding another cutpoint\nmatch5.cem\nmatch6.cem\n\n\nlibrary(sandwich)\ndatcem5.lm &lt;- lm(re78log ~ trt, data = datcem5, weights = weights)\n## The function vcovHC() provides cluster robust standard errors\nlmtest::coeftest(datcem5.lm, vcov. = vcovHC)\n## Estimate treatment effect on number with some earnings:\ndatcem6.glm &lt;- glm(I(re78&gt;0) ~ trt, data = datcem6, weights = weights,\nfamily=binomial)\nlmtest::coeftest(datcem6.glm, vcov. = vcovHC)\n\n\n\n\nSection 9.8: Multiple imputation\n\nsuppressPackageStartupMessages(library(mice))\nBoys &lt;- with(subset(mice::boys, age&gt;=9), \n             data.frame(age=age, loghgt=log(hgt), logbmi=log(bmi), loghc=log(hc)))\n(Pattern &lt;- md.pattern(Boys, plot=F))\n\n\nset.seed(31)       # Set to reproduce result shown\nPatternB &lt;- rbind(Pattern[-c(1,nrow(Pattern)), -ncol(Pattern)],\n             c(0,1,1,1), c(0,1,0,0), c(0,0,1,0))\nboys &lt;- rbind(ic(Boys), \n              ampute(cc(Boys), pattern=PatternB, freq=c(.3,.15,.15,.2,.1,.1), \n                     prop=0.75)$amp)\nmd.pattern(boys, plot=FALSE)\n\n\nset.seed(17)       # Set to reproduce result shown\nout &lt;- capture.output(        # Evaluate; send screen output to text string\n  boys.mids &lt;- mice(boys, method='pmm', m=8)  )\nimpDFs &lt;- complete(boys.mids, action='all')   # Returns a list of m=8 dataframes\n## Average over imputed dataframes (use for exploratory purposes only)\nimpArray &lt;- sapply(impDFs, function(x)as.matrix(x), simplify='array')\nboysAv &lt;- as.data.frame(apply(impArray, 1:2, mean))  \n\n\nfits &lt;- with(boys.mids, lm(logbmi~age+loghgt))\npool.coef &lt;- summary(pool(fits))  # Include in table below\n\n\n## 2) Regression that leaves out rows with NAs\nomitNArows.coef &lt;- coef(summary(lm(logbmi~age+loghgt, data=boys)))\n## 3) Regression fit to average over data frames after imputation\nboysAv.coef &lt;- coef(summary(lm(logbmi~age+loghgt, data=boysAv)))\n## 4) Fit to original data, with 36 rows had missing data\nOrig.coef &lt;- coef(summary(lm(logbmi ~ age+loghgt, data=Boys)))\n\n\nctab &lt;- cbind(summary(pool(fits))[,2:3], omitNArows.coef[,1:2], boysAv.coef[,1:2], \n      Orig.coef[,1:2])\ntab &lt;- setNames(cbind(ctab[,c(1,3,5,7)], ctab[,c(2,4,6,8)]),\n                paste0(rep(c('Est','SE'), c(4,4)), rep(1:4, 2)))\nround(tab,3)\n\n\nTime series cross-sectional data – an example\n\nairquality &lt;- datasets::airquality\nairq &lt;- cbind(airquality[, 1:4], day=1:nrow(airquality))\n  # 'day' (starting May 1) replaces columns 'Month' & 'Day')\n## Replace `Ozone` with `rt4ozone`:\nairq &lt;- cbind(rt4ozone=airq$Ozone^0.25, airq[,-1])\n\n\n## Generate the scatterplot matrix, now with `rt4ozone` replacing `Ozone`\nsmoothPars &lt;- list(col.smooth='red', lty.smooth=2, spread=0)\ncar::spm(airq, cex.labels=1.2, regLine=FALSE, col='blue', \n         oma=c(1.95,3,4, 3), gap=.25, smooth=smoothPars)\n\n\nairq.imp &lt;- mice(airq, m=20, print=FALSE)\n  ## 20 imputations shows up issues of concern very clearly\n\n\n## Code for figure\nout &lt;- micemd::overimpute(airq.imp)\n\n\n\nSome further points\n\n\n\nSection 9.9: Further reading\n\nData with more variables than observations\n\n\nCausal inference\n\n\nMultiple imputation\n\n\n\nSection 9.10: Exercises\n9.3\n\nlibrary(DAAG)\noz::oz(sections=c(3:5, 11:16))\nnames(possumsites)[1:2] &lt;- c(\"long\", \"lat\")\nwith(possumsites, {\npoints(long, lat);\ntext(long, lat, row.names(possumsites), pos=c(2,4,2,2,4,2,2))\n})\n\n9.7\n\ndata(wine, package='gclus')\nmat &lt;- with(wine, \n  round(1-cor(cbind(Alcohol, Malic, Magnesium, Phenols, Flavanoids)),2))\ncolnames(mat) &lt;- rownames(mat) &lt;- 1:5\nprint(mat)\n\n9.9a\n\n`confusion` &lt;-\nfunction(actual, predicted, digits=4){\n  tab &lt;- table(actual, predicted)\n  confuse &lt;- apply(tab, 1, function(x)x/sum(x))\n  print(round(confuse, digits))\n  acc &lt;- sum(tab[row(tab)==col(tab)])/sum(tab)\n  invisible(print(c(\"Overall accuracy\" = round(acc,digits))))\n}\ndata(Vehicle, package=\"mlbench\")\nlhat &lt;- MASS::lda(Class ~ ., data=Vehicle, CV=TRUE)$class\nqhat &lt;- MASS::qda(Class ~ ., data=Vehicle, CV=TRUE)$class\nDAAG::confusion(Vehicle$Class, lhat)\nDAAG::confusion(Vehicle$Class, qhat)\nrandomForest::randomForest(Class ~ ., data=Vehicle, CV=TRUE)\n\n9.9c\n\nVehicle.lda &lt;- MASS::lda(Class ~ ., data=Vehicle)\ntwoD &lt;- predict(Vehicle.lda)$x\nggplot2::quickplot(twoD[,1], twoD[,2], color=Vehicle$Class,\n                   geom=c(\"point\",\"density2d\"))\n\n9.10\n\nlibrary(ape); library(MASS)\nlibrary(DAAGbio)\nprimates.dna &lt;- as.DNAbin(primateDNA)\nprimates.dist &lt;- dist.dna(primates.dna, model=\"K80\")\nprimates.cmd &lt;- cmdscale(primates.dist)\neqscplot(primates.cmd)\nrtleft &lt;- c(4,2,4,2)[unclass(cut(primates.cmd[,1], breaks=4))]\ntext(primates.cmd, labels=row.names(primates.cmd), pos=rtleft)\n\n\nd &lt;- dist(primates.cmd)\nsum((d-primates.dist)^2)/sum(primates.dist^2)\n\n9.11\n\nlibrary(DAAG)\npacific.dist &lt;- dist(x = as.matrix(rockArt[-c(47,54,60,63,92),28:641]), \n                     method = \"binary\")\nsum(pacific.dist==1)/length(pacific.dist)\nplot(density(pacific.dist, to = 1))\n## Check that all columns have at least one distance &lt; 1\nsymmat &lt;- as.matrix(pacific.dist)\ntable(apply(symmat, 2, function(x) sum(x&lt;1)))\npacific.cmd &lt;- cmdscale(pacific.dist)\npacific.sam &lt;- sammon(pacific.dist)\n\n9.15\n\nWine &lt;- setNames(cbind(stack(wine, select=2:14), rep(wine[,-1], 13)),\n                 c(\"value\", \"measure\", \"Class\"))\nbwplot(measure ~ value, data=Wine)\n\n\nwine.pr &lt;- prcomp(wine[,-1], scale=TRUE)\nround(wine.pr$sdev,2)\nt(round(wine.pr$rotation[,1:2],2))\nscores &lt;- as.data.frame(cbind(predict(wine.pr), Class=wine[,1]))\nxyplot(PC2 ~ PC1, groups=Class, data=scores, aspect='iso', \n       par.settings=simpleTheme(pch=16), auto.key=list(columns=3))\n\n\nlibrary(MASS)\nwine.lda &lt;- lda(Class ~ ., data=wine)\nwineCV.lda &lt;- lda(Class ~ ., data=wine, CV=T)\nt(round(wine.lda$scaling,2))\ntab &lt;- table(wine$Class, wineCV.lda$class, \n             dnn=c('Actual', 'Predicted'))   \ntab\nsetNames(round(1-sum(diag(tab))/sum(tab),4), \"CV error rate\")\nscores &lt;- as.data.frame(cbind(predict(wine.lda)$x, Class=wine[,1]))\nxyplot(LD2 ~ LD1, groups=Class, data=scores, aspect='iso', \n       par.settings=simpleTheme(pch=16), auto.key=list(columns=3))\n\n\nwine$Class &lt;- factor(Wine$Class)\nwine.rf &lt;- randomForest(x=wine[,-1], y=wine$Class)\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/ch9.R\")\n}",
    "crumbs": [
      "Book",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 9: Multivariate data exploration and discrimination</span>"
    ]
  },
  {
    "objectID": "Appendix.html",
    "href": "Appendix.html",
    "title": "7  Appendix A: The R System – A Brief Overview",
    "section": "",
    "text": "Packages required (plus any dependencies)\nDAAG dplyr tidyr tibble MASS gplots plotrix latticeExtra RColorBrewer\nAdditionally, knitr and Hmisc are required in order to process the qmd source file.",
    "crumbs": [
      "Book",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix A: The R System – A Brief Overview</span>"
    ]
  },
  {
    "objectID": "Appendix.html#the-r-system-a-brief-overview",
    "href": "Appendix.html#the-r-system-a-brief-overview",
    "title": "7  Appendix A: The R System – A Brief Overview",
    "section": "The R System – A Brief Overview",
    "text": "The R System – A Brief Overview\n\nSection 10.1: Getting started with R\n\nSubsection 10.1.1: Learn by typing code at the command line\n\n&gt; ## Arithmetic calculations.  See the help page `?Arithmetic` {-}\n&gt; 2*3+10            # The symbol `*` denotes 'multiply'\n\n\n&gt; ## Use the `c()` function to join results into a numeric vector\n&gt; c(sqrt(10), 2*3+10, sqrt(10), 2^3)  # 2^3 is 2 to the power of 3\n&gt; ## R knows about pi\n&gt; 2*pi*6378         # Approximate circumference of earth at equator (km)\n\n\n?help              # Get information on the use of `help()`\n?sqrt              # Or, type help(sqrt)\n?Arithmetic        # See, in similar vein ?Syntax\n?'&lt;'               # `?Comparison` finds the same help page\n\n\n## Two commands on one line; Use ';' as separator\n2*3*4+10; sqrt(10)        ## Try also `cat(2*3*4+10, sqrt(10), sep='n')\n## Convert CO2 carbon emissions from tonnes of carbon to tonnes of CO2 \n3.664*c(.53, 2.56, 9.62)  ## Data are for 1900, 1960 & 2020\n\n\n## Use `cat()` to print several items, with control of formatting\ncat(2*3*4+10, sqrt(10), '\\n')\n\n\nAssignment\n\n## Convert from amounts of carbon to amounts of CO2 (billions of tonnes)\n## and assign result to a named object\nfossilCO2vals &lt;- c(.53, 2.56, 9.62)*3.664  # Amounts in 1900, 1960, and 2020\n  # Equivalently `fossilCO2vals &lt;- c(.53, 2.56, 9.62)*rep(3.664,3)`\n## To assign and print, enclose in round brackets\n(fossilCO2vals &lt;- c(.53, 2.56, 9.62)*3.664)\n\n\n3.664*c(.53,2.56, 9.62) -&gt; fossilCO2vals\n\n\n\nEntry of data at the command line, a graphics formula, and a graph\n\nYear &lt;- c(1900, 1920, 1940, 1960, 1980, 2000, 2020)\nCO2 &lt;- c(.53,.96,1.32,2.56,5.32,6.95,9.62)*3.664\n## Now plot Carbon Dioxide emissions as a function of Year\nplot(CO2 ~ Year, pch=16, fg=\"gray\")\n\n\n\nGrouping vectors togeher into data frames\n\nCO2byYear &lt;- data.frame(year=Year, co2gas=CO2)\nCO2byYear         # Display the contents of the data frame.\nrm(Year, CO2)     # Optionally, remove `Year` and `Carbon` from the workspace\nplot(co2gas ~ year, data=CO2byYear, pch=16)  \n\n\nsqrt(10)           # Number of digits is determined by current seting\noptions(digits=2)  # Change until further notice,\nsqrt(10)\n\n\n\nWide-ranging information access and searches\n\n\n\n\nSection 10.2: R data structures\n\nSubsection 10.2.1: Vectors, dates, and arrays\n\nvehicles &lt;- c(\"Compact\", \"Large\", \"Midsize\", \"Small\", \"Sporty\", \"Van\")\nc(T, F, F, F, T, T, F)   # A logical vector, assuming F=FALSE and T=TRUE\n\n\n## Character vector\nmammals &lt;- c(\"Rat\", \"Pig\", \"Rat\", \"Mouse\", \"Pig\")\n## Logical vector\nrodent &lt;- c(\"TRUE\", \"FALSE\", \"TRUE\", \"FALSE\", \"TRUE\", \"FALSE\")\n## From character vector `mammals`, create factor\nmfac &lt;- factor(mammals)\nlevels(mfac)  \ntable(mfac)\n\n\nDates\n\nday1 &lt;- as.Date(c(\"2022-01-01\", \"2022-02-01\", \"2022-03-01\"))\nas.numeric(day1)   # Days since 1 January 1970\nday1[3] - day1[2]\n\n\n\nThe use of square brackets to extract subsets of vectors\n\n## Specify the indices of the elements that are to be extracted\nx &lt;- c(3, 11, 8, 15, 12,18)\nx[c(1,4:6)]        # Elements in positions 1, 4, 5, and 6\n## Use negative indices to identify elements for omission\nx[-c(2,3)]         # Positive and negative indices cannot be mixed\n## Specify a vector of logical values. \nx &gt; 10             # This generates a vector of logical values\nx[x &gt; 10]\n\n\nbodywt &lt;- c(Cow=465, Goat=28, Donkey=187, Pig=192)\nbodywt[c(\"Goat\", \"Pig\")]\n\n\n\nMatrices and arrays\n\narr123 &lt;- array(1:24, dim=c(2,4,3))\n## This prints as three 2 by 4 matrices.  Print just the first of the three.\narr123[, 2, 1]     # Column 2 and index 1 of 3rd dimension\nattributes(arr123)\n\n\n\n\nSubsection 10.2.2: Factors\n\ngender &lt;- c(rep(\"male\",691), rep(\"female\",692))\ngender &lt;- factor(gender)  # From character vector, create factor\nlevels(gender)            # Notice that `female` comes first\n\n\nGender &lt;- factor(gender, levels=c(\"male\", \"female\"))\n\n\nmf1 &lt;- factor(rep(c('male','female'),c(2,3)), labels=c(\"f\", \"m\"))\n## The following has the same result\nmf2 &lt;- factor(rep(c('male','female'), c(2,3)))\nlevels(mf2) &lt;- c(\"f\",\"m\")   # Assign new levels\nif(all(mf1==mf2))print(mf1)\n\n\nsum(gender==\"male\")\n\n\ntable(chickwts$feed)  # feed is a factor\nsource &lt;- chickwts$feed  \nlevels(source) &lt;- c(\"milk\",\"plant\",\"plant\",\"meat\",\"plant\",\"plant\")\ntable(source)\n\n\nOrdered factors\n\nstress &lt;- rep(c(\"low\",\"medium\",\"high\"), 2)\nord.stress &lt;- ordered(stress, levels=c(\"low\", \"medium\", \"high\"))\nord.stress\nord.stress &gt;= \"medium\"\n\n\n\nMissing values in values of factors\n\n\n\nSubsection 10.2.3: Operations with data frames\n\nCars93sum &lt;- DAAG::Cars93.summary  # Create copy in workspace\nCars93sum\n\n\nCars93sum[4:6, 2:3]   # Extract rows 4 to 6 and columns 2 and 3\nCars93sum[6:4, ]      # Extract rows in the order 6, 5, 4\nCars93sum[, 2:3]      # Extract columns 2 and 3\n## Or, use negative integers to specify rows and/or columns to be omitted\nCars93sum[-(1:3), -c(1,4)]  # In each case, numbers must be all +ve or all -ve\n## Specify row and/or column names\nCars93sum[c(\"Small\",\"Sporty\",\"Van\"), c(\"Max.passengers\",\"No.of.cars\")]\n\n\nData frames vs matrices\n\nnames(Cars93sum)[3] &lt;- \"numCars\"\nnames(Cars9sum) &lt;- c(\"minPass\",\"maxPass\",\"numCars\",\"code\")\n\n\n\nUsing a data frame as a database – with() and within()\n\n## trees (datasets) has data on Black Cherry Trees\nwith(trees, round(c(mean(Girth), median(Girth), sd(Girth)),1))\n\n\nwith(DAAG::pair65,       # stretch of rubber bands\n  {lenchange = heated-ambient\n   c(mean(lenchange), median(lenchange))\n})\n\n\n## Add variables `mph` and `gradient` to `DAAG::nihills`\nnihr &lt;- within(DAAG::nihills, {mph &lt;- dist/time; gradient &lt;- climb/dist})\n\n\n\nExtracting rows from data frames\n\nunlist(Cars93sum[1, ])\n\n\n\n\nSubsection 10.2.4: Data manipulation functions used in earlier chapters\n\n## For columns of `DAAG::jobs`, show the range of values\nsapply(DAAG::jobs, range)\n## Split egg lengths by species, calculate mean, sd, and number for each\nwith(DAAG::cuckoos, sapply(split(length,species), \n                           function(x)c(av=mean(x), sd=sd(x), nobs=length(x))))\n\n\napply(UCBAdmissions, 3, function(x)(x[1,2]/(x[1,2]+x[2,2]))*100) # Females\napply(UCBAdmissions, 3, function(x)(x[1,1]/(x[1,1]+x[2,1]))*100) # Males\n\n\nUCBAdmissions[, , 1]\n\n\nDAAG::cricketer |&gt; dplyr::count(year, left, name=\"Freq\") -&gt; handYear\nnames(handYear)[2] &lt;- \"hand\"\nbyYear &lt;- tidyr::pivot_wider(handYear, names_from='hand', values_from=\"Freq\")\n\n\n\nSubsection 10.2.5: Writing data to a file, and reading data from a file\n\nCO2byYear &lt;- data.frame(year = seq(from=1900, to=2020, by=20),\n                        co2gas = c(1.94, 3.52, 4.84, 9.38, 19.49, 25.46, 35.25))\nwrite.table(CO2byYear, file='gas.txt')    # Write data frame to file\nCO2byYear &lt;- read.table(file=\"gas.txt\")   # Read data back in\nwrite.csv(CO2byYear, file='gas.csv')                  # Write data frame\nCO2byYear &lt;- read.csv(file=\"gas.csv\", row.names=1)    # Read data back in\n\n\nData input from the RStudio menu — data frames vs tibbles\n\n\n\nSubsection 10.2.6: Issues for working with data frames and tibbles\n\nExtraction of columns from data frames and tibbles\n\nsites &lt;- DAAG::possumsites    # sites is then a data frame\nsites[,3]                     # returns a vector\nsites[,3, drop=FALSE]         # returns a 1-column data frame\n\n\ndplyr::as_tibble(sites)[,3]   # returns a 1-column tibble\ndplyr::as_tibble(sites)[[3]]  # returns a vector\nsites[[3]]                    # returns a vector\n\n\n\nConversion between data frames and tibbles\n\nattributes(DAAG::possumsites)[['row.names']]\n\n\npossumSites &lt;- tibble::as_tibble(DAAG::possumsites, rownames=\"Site\")\npossumSites\n\n\n\n\nSubsection 10.2.7: Lists\n\n## Summary statistics for 31 felled black cherry tree\n## Median (middle value), range, number, units\nhtstats &lt;- list(med=76, range=c(low=63,high=87), n=31, units=\"ft\")\nhtstats[1:2]       # Show first two list elements only\n\n\n## The following are alternative ways to extract the second list element\nhtstats[2]          # First list element (Can replace `2` by 'range')\nhtstats[2][1]       # A subset of a list is a list \n\n\nhtstats[[2]]; htstats$range; htstats[[\"range\"]]\n\n\nunlist(htstats[2])  # Contents of second list element, with composite names\nunlist(htstats[2], use.names=F)   # Elements have no names\n\n\ntstats &lt;- with(MASS::shoes, t.test(B, A, paired=TRUE))\nnames(tstats)        ## Names of list elements. See `?t.test` for details.\ntstats[1]            ## Type tstats[1] to see the first list element\n## Compact listing of contents list elements 1 to 5, which are all numeric\nunlist(tstats[1:5])  ## With `unlist(tstats)` all elements become character \n\n\n\n\nSection 10.3: Functions and operators\n\nSubsection 10.3.1: Common useful built-in functions\n\n## Data indices\nlength()       # number of elements in a vector or a list\norder()        # x[order(x)] sorts x (by default, NAs are last)\nwhich()        # which indices of a logical vector are `TRUE`\nwhich.max()    # locates (first) maximum (NB, also: `which.min()`)\n\n\n## Data manipulation\nc()            # join together (`concatenate`) elements or vectors or lists\ndiff()         # vector of first differences\nsort()         # sort elements into order, by default omitting NAs\nrev()          # reverse the order of vector elements\nt()            # transpose matrix or data frame \n               # (a data frame is first coerced to a matrixwith() \nwith()         # do computation using columns of specified data frame\n\n\n## Data summary\nmean()         # mean of the elements of a vector\nmedian()       # median of the elements of a vector\nrange()        # minimum and maximum value elements of vector\nunique()       # form the vector of distinct values\n## List function arguments\nargs()         # information on the arguments to a function\n\n\n## Obtain details\nhead()         # display first few rows (by default 6) of object\nls()           # list names of objects in the workspace\n\n\n## Print multiple objects\ncat()          # prints multiple objects, one after the other\n\n\n## Functions that return TRUE or FALSE?\nall()          # returns TRUE if all values are TRUE\nany()          # returns TRUE if any values are TRUE\nis.factor()    # returns TRUE if the argument is a factor\nis.na()        # returns TRUE if the argument is an NA\n               # NB also is.logical(), etc.\n\n\nseq(from =1, by=2, length.out=3)  # Unabbeviated arguments\nseq(from =1, by=2, length=3)      # Abbreviate `length.out` to `length`\n\n\n\nSubsection 10.3.2: User-written functions\n\ndistance &lt;- c(148,182,173,166,109,141,166)\nmean.and.sd(distance)\n\n\n## Execute the function with the  default argument:\nmean.and.sd()\n\n\n## Thus, to return the mean, SD and name of the input vector\n## replace c(mean=av, SD=sdev) by\nlist(mean=av, SD=sdev, dataset = deparse(substitute(x)))\n\n\n\nSubsection 10.3.3: Generic functions, and the class of an object\n\n\nSubsection 10.3.4: Pipes — a “Do this, then this, . . .” syntax\n\nmean(rnorm(20, sd=2))\n20 |&gt; rnorm(sd=2) |&gt; mean()\n\n\nlogmammals &lt;- MASS::mammals |&gt; log() |&gt; setNames(c(\"logbody\",\"logbrain\"))\n## Alternatively, use the ability to reverse the assignment operator.\nMASS::mammals |&gt; log() |&gt; setNames(c(\"logbody\",\"logbrain\")) -&gt; logmammals \n  ## This last is more in the spirit of pipes.\n\n\nMASS::mammals |&gt;\n  log() |&gt;\n  setNames(c(\"logbody\",\"logbrain\")) |&gt;\n  (\\(d)lm(logbrain ~ logbody, data=d))() |&gt;\n  coef()\n\n\n\nSubsection 10.3.5: Operators\n\n## Multiple of divisor that leaves smallest non-negative remainder\nc(\"Multiple of divisor\" = 24 %/% 9, \"Remainder after division\" = 6)\n\n\n\n\nSection 10.4: Calculations with matrices, arrays, lists, and data frames\n\nCalculations in parallel across all elements of a vector\n\nx &lt;- 1:6\nlog(x)                 # Natural logarithm of 1, 2, ... 6\nlog(x, base=10)        # Common logarithm (base 10)\nlog(64, base=c(2,10))  # Apply different bases to one number\nlog(matrix(1:6, nrow=2), base=2)  # Take logarithms of all matrix elements\n\n\n\nPatterned data\n\nseq(from=5, to=22, by=3)  # The first value is 5.\nrep(c(2,3,5), 4)          #  Repeat the sequence (2, 3, 5) four times over\nrep(c(\"female\", \"male\"), c(2,3))    # Use syntax with a character vector\n\n\n\nSubsection 10.4.1: Missing values\n\nnbranch &lt;- subset(DAAG::rainforest, species==\"Acacia mabellae\")$branch\nnbranch            # Number of small branches (2cm or less)\n\n\nmean(nbranch, na.rm=TRUE)\n\n\nnbranch == NA      # This always equals `NA`\n\n\nis.na(nbranch)    # Use to check for NAs\n\n\nnbranch[is.na(nbranch)] &lt;- -999\n  # `mean(nbranch)` will then be a nonsense value\n\n\nNAs in modeling functions\n\noptions()$na.action # Version 3.2.2, following startup\n\n\n\nCounting and identifying NAs – the use of table()\n\nwith(DAAG::nswdemo, table(trt, re74&gt;0, useNA=\"ifany\"))\n\n\n\nInfinities and NaNs\n\nsummary(DAAG::primates)\n\n\nprimates &lt;- DAAG::primates\n\n\ngplots::plotCI()    # `plotCI() function in package `gplots`\nplotrix::plotCI()   # `plotCI() function in package `plotrix`\n\n\nsessionInfo()[['basePkgs']]\n\n\n## List just the workspace and the first eight packages on the search list:\nsearch()[1:9]\n\n\ndata(package=\"datasets\") \n\n\n\n\n\nSection 10.5: Brief notes on R graphics packages and functions\n\nSubsection 10.5.1: Lattice graphics — a step beyond base graphics\n\ngrog &lt;- DAAG::grog\nchr &lt;- with(grog, match(Country, c('Australia', 'NewZealand')))  \n  # Australia: 1; matches 1st element of c('Australia', 'NewZealand')\n  # NewZealand: 2; matches 2nd element\nplot(Beer ~ Year, data=grog, ylim=c(0, max(Beer)*1.1), pch = chr)\nwith(grog, points(Wine ~ Year, pch=chr, col='red'))\nlegend(\"bottomright\", legend=c(\"Australia\", \"New Zealand\"), pch=1:2)\ntitle(main=\"Beer consumption (l, pure alcohol)\", line=1)\n\n\nlibrary(latticeExtra)    ## Loads both lattice and the add-on latticeExtra\ngph &lt;- xyplot(Beer+Wine ~ Year, groups=Country, data=grog)\nupdate(gph, par.settings=simpleTheme(pch=19), auto.key=list(columns=2))\n\n\n## Or, condition on `Country`\nxyplot(Beer+Wine+Spirit ~ Year | Country, data=grog,\n       par.settings=simpleTheme(pch=19), auto.key=list(columns=3))\n\n\ntinting &lt;- DAAG::tinting\nxyplot(csoa~it | tint*target, groups=agegp, data=tinting, auto.key=list(columns=2))\n\n\ncuckoos &lt;- DAAG::cuckoos\nav &lt;- with(cuckoos, aggregate(length, list(species=species), FUN=mean))\ngph &lt;- dotplot(species ~ length, data=cuckoos, alpha=0.4) +\n  as.layer(dotplot(species ~ x, pch=3, cex=1.4, col=\"black\", data=av))\nupdate(gph, xlab=\"Length of egg (mm)\")\n\n\n## Alternatives, using `layer()` or `as.layer()`\navg &lt;- with(cuckoos, data.frame(nspec=1:nlevels(species), \n                             av=sapply(split(length,species),mean)))\ndotplot(species ~ length, data=cuckoos) + \n  layer(lpoints(nspec~av, pch=3, cex=1.25, col=\"black\"), data=avg)\n\n\ndotplot(species ~ length, data=cuckoos) + \n  as.layer(dotplot(nspec~av, data=avg, pch=3, cex=1.25, col=\"black\"))\n\n\n## Specify panel function\ndotplot(species ~ length, data=cuckoos, \n  panel=function(x,y,...){panel.dotplot(x, y, pch=1, col=\"gray40\")\n    avg &lt;- data.frame(nspec=1:nlevels(y), av=sapply(split(x,y),mean))\n    with(avg, lpoints(nspec~av, pch=3, cex=1.25, col=\"black\")) })\n\n\nCombining separately created graphics objects\n\ncuckoos &lt;- DAAG::cuckoos\n## Panel A: Dotplot without species means added\ngphA &lt;- dotplot(species ~ length, data=cuckoos) \n## Panel B: Box and whisker plot\ngphB &lt;- bwplot(species ~ length, data=cuckoos)\nupdate(c(\"A: Dotplot\"=gphA, \"B: Boxplot\"=gphB), between=list(x=0.4),\n       xlab=\"Length of egg (mm)\", layout=c(2,1))\n  ## `latticeExtra::c()` joins compatible plots together. \n  ## `layout=c(2,1)` : join horizontally; `layout=c(1,2)` : join vertically\n\n\n\n\nSubsection 10.5.2: Dynamic graphics – the rgl package\n\nvignette('plot3D', package='plot3D')\n\n\n\n\nSection 10.6: Plotting characters, symbols, line types and colors\n\nycol &lt;- -2.1 - (0:9) * 2.1\nftype &lt;- c(\"plain\", \"bold\", \"italic\", \"bold italic\", \"symbol\")\nyline &lt;- 4.2\nypmax &lt;- 20\nfarleft &lt;- -7.8\nplot(c(-4, 31), c(4.25, ypmax), type = \"n\", xlab = \"\", ylab = \"\",\naxes = F)\nchh &lt;- par()$cxy[2]\ntext(0:25, rep(ypmax + 0.8 * chh, 26), paste(0:25), srt = 90,\ncex = 0.75, xpd = T)\ntext(-1.5, ypmax + 0.8 * chh, \"pch = \", cex = 0.75, xpd = T)\npoints(0:25, rep(ypmax, 26), pch = 0:25, lwd=0.8)\nletterfont &lt;- function(ypos = ypmax, font = 2) {\npar(font = font)\ntext(-1.35, ypos, \"64-76\", cex = 0.75, adj = 1, xpd = TRUE)\ntext(19 - 1.35, ypos, \"96-108\", cex = 0.75, adj = 1)\npoints(c(0:12), rep(ypos, 13), pch = 64:76)\npoints(19:31, rep(ypos, 13), pch = 96:108)\ntext(farleft, ypos, paste(font), xpd = T)\ntext(farleft, ypos - 0.5, ftype[font], cex = 0.75)\n}\nplotfont &lt;- function(xpos = 0:31, ypos = ypmax, font = 1,\nsel32 = 2:4, showfont = TRUE) {\npar(font = font)\ni &lt;- 0\nfor (j in sel32) {\ni &lt;- i + 1\nmaxval &lt;- j * 32 - 1\nif(j==4)maxval &lt;- maxval-1\ntext(-1.35, ypos - i + 1, paste((j - 1) * 32, \"-\",\nmaxval, sep = \"\"), cex = 0.75, adj = 1, xpd = TRUE)\nif(j!=4)\npoints(xpos, rep(ypos - i + 1, 32), pch = (j - 1) *\n32 + (0:31))\nelse\npoints(xpos[-32], rep(ypos - i + 1, 31), pch = (j - 1) *\n32 + (0:30))\n}\nlines(rep(-1.05, 2), c(ypos - length(sel32) + 1, ypos) +\nc(-0.4, 0.4), xpd = T, col = \"grey40\")\nif (showfont) {\ntext(farleft, ypos, paste(\"font =\", font, \" \"), xpd = T)\ntext(farleft, ypos - 0.5, ftype[font], cex = 0.75,\nxpd = T)\n}\n}\nplotfont(ypos = ypmax - 1.5, font = 1, sel32 = 2:4)\nfor (j in 2:4) letterfont(ypos = ypmax - 2.1 - 1.4 * j, font = j)\nplotfont(ypos = ypmax - 9.1, font = 5, sel32 = 3)\nplotfont(xpos = c(-0.5, 1:31), ypos = ypmax - 10.1, font = 5,\nsel32 = 4, showfont = FALSE)\npar(font = 1)\nltypes &lt;- c(\"blank\", \"solid\", \"dashed\", \"dotted\", \"dotdash\",\n\"longdash\", \"twodash\")\nlcode &lt;- c(\"\", \"\", \"44\", \"13\", \"1343\", \"73\", \"2262\")\nfor (i in 0:6) {\nlines(c(4, 31), c(yline + 4.5 - 0.8 * i, yline + 4.5 -\n0.8 * i), lty = i, lwd = 2, xpd = T)\nif (i == 0)\nnumchar &lt;- paste(\"lty =\", i, \" \")\nelse numchar &lt;- i\ntext(farleft, yline + 4.5 - 0.8 * i, numchar, xpd = TRUE)\ntext(farleft + 3.5, yline + 4.5 - 0.8 * i, ltypes[i +\n1], cex = 0.85, xpd = TRUE)\ntext(farleft + 7.5, yline + 4.5 - 0.8 * i, lcode[i +\n1], cex = 0.85, xpd = TRUE)\n}\n\n\nycol &lt;- -2.1 - (0:9) * 2.1\nftype &lt;- c(\"plain\", \"bold\", \"italic\", \"bold italic\", \"symbol\")\nyline &lt;- 4.2\nypmax &lt;- 20\nfarleft &lt;- -7.8\nplot(c(-4, 31), c(4.25, ypmax), type = \"n\", xlab = \"\", ylab = \"\",\naxes = F)\nchh &lt;- par()$cxy[2]\ntext(0:25, rep(ypmax + 0.8 * chh, 26), paste(0:25), srt = 90,\ncex = 0.75, xpd = T)\ntext(-1.5, ypmax + 0.8 * chh, \"pch = \", cex = 0.75, xpd = T)\npoints(0:25, rep(ypmax, 26), pch = 0:25, lwd=0.8)\nletterfont &lt;- function(ypos = ypmax, font = 2) {\npar(font = font)\ntext(-1.35, ypos, \"64-76\", cex = 0.75, adj = 1, xpd = TRUE)\ntext(19 - 1.35, ypos, \"96-108\", cex = 0.75, adj = 1)\npoints(c(0:12), rep(ypos, 13), pch = 64:76)\npoints(19:31, rep(ypos, 13), pch = 96:108)\ntext(farleft, ypos, paste(font), xpd = T)\ntext(farleft, ypos - 0.5, ftype[font], cex = 0.75)\n}\nplotfont &lt;- function(xpos = 0:31, ypos = ypmax, font = 1,\nsel32 = 2:4, showfont = TRUE) {\npar(font = font)\ni &lt;- 0\nfor (j in sel32) {\ni &lt;- i + 1\nmaxval &lt;- j * 32 - 1\nif(j==4)maxval &lt;- maxval-1\ntext(-1.35, ypos - i + 1, paste((j - 1) * 32, \"-\",\nmaxval, sep = \"\"), cex = 0.75, adj = 1, xpd = TRUE)\nif(j!=4)\npoints(xpos, rep(ypos - i + 1, 32), pch = (j - 1) *\n32 + (0:31))\nelse\npoints(xpos[-32], rep(ypos - i + 1, 31), pch = (j - 1) *\n32 + (0:30))\n}\nlines(rep(-1.05, 2), c(ypos - length(sel32) + 1, ypos) +\nc(-0.4, 0.4), xpd = T, col = \"grey40\")\nif (showfont) {\ntext(farleft, ypos, paste(\"font =\", font, \" \"), xpd = T)\ntext(farleft, ypos - 0.5, ftype[font], cex = 0.75,\nxpd = T)\n}\n}\nplotfont(ypos = ypmax - 1.5, font = 1, sel32 = 2:4)\nfor (j in 2:4) letterfont(ypos = ypmax - 2.1 - 1.4 * j, font = j)\nplotfont(ypos = ypmax - 9.1, font = 5, sel32 = 3)\nplotfont(xpos = c(-0.5, 1:31), ypos = ypmax - 10.1, font = 5,\nsel32 = 4, showfont = FALSE)\npar(font = 1)\nltypes &lt;- c(\"blank\", \"solid\", \"dashed\", \"dotted\", \"dotdash\",\n\"longdash\", \"twodash\")\nlcode &lt;- c(\"\", \"\", \"44\", \"13\", \"1343\", \"73\", \"2262\")\nfor (i in 0:6) {\nlines(c(4, 31), c(yline + 4.5 - 0.8 * i, yline + 4.5 -\n0.8 * i), lty = i, lwd = 2, xpd = T)\nif (i == 0)\nnumchar &lt;- paste(\"lty =\", i, \" \")\nelse numchar &lt;- i\ntext(farleft, yline + 4.5 - 0.8 * i, numchar, xpd = TRUE)\ntext(farleft + 3.5, yline + 4.5 - 0.8 * i, ltypes[i +\n1], cex = 0.85, xpd = TRUE)\ntext(farleft + 7.5, yline + 4.5 - 0.8 * i, lcode[i +\n1], cex = 0.85, xpd = TRUE)\n}\n\n\nFont families\n\n\nColors\n\nlibrary(RColorBrewer)\npalette(brewer.pal(12, \"Set3\"))\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/Appendix.R\")\n}",
    "crumbs": [
      "Book",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix A: The R System – A Brief Overview</span>"
    ]
  }
]